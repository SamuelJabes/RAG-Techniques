{"docstore/data": {"7db136dc-ee57-4a77-9846-8065af8ba6a6": {"__data__": {"id_": "7db136dc-ee57-4a77-9846-8065af8ba6a6", "embedding": null, "metadata": {"page_label": "1", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e841420-0ab3-4331-8b62-49fd7d387b45", "node_type": "4", "metadata": {"page_label": "1", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "9d3aa91ccb7566d963d33339b13fdf0bb272dc4e4ca27b5f9e6ab90abd4af4de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00391c6e-7a4e-4478-a9d1-b282ad7ee62c", "node_type": "1", "metadata": {}, "hash": "6c1d553a9b458459d6538c5dc8d65797bb0459b4593e577413a3607f8a0f2bd8", "class_name": "RelatedNodeInfo"}}, "text": "Deep Reinforcement Learning based\nRecommendation with Explicit User-Item\nInteractions Modeling\nFeng Liu\u2217, Ruiming Tang\u2020, Xutao Li\u2217, Weinan Zhang\u2021Yunming Ye\u2217, Haokun Chen\u2021, Huifeng Guo\u2020and Yuzhou Zhang\u2020\n\u2217Shenzhen Key Laboratory of Internet Information Collaboration\nShenzhen Graduate School, Harbin Institute of Technology, Shenzhen, 518055, China\nEmail: fengliu@stu.hit.edu.cn, lixutao@hit.edu.cn, yeyunming@hit.edu.cn\n\u2020Noah\u2019s Ark Lab, Huawei, China\nEmail: tangruiming, huifeng.guo, zhangyuzhou3@huawei.com\n\u2021Shanghai Jiao Tong University, Shanghai, China\nEmail: wnzhang@sjtu.edu.cn, chenhaokun@sjtu.edu.cn\nAbstract \u2014Recommendation is crucial in both academia and\nindustry, and various techniques are proposed such as content-\nbased collaborative \ufb01ltering, matrix factorization, logistic re-\ngression, factorization machines, neural networks and multi-\narmed bandits. However, most of the previous studies suffer\nfrom two limitations: (1) considering the recommendation as\na static procedure and ignoring the dynamic interactive nature\nbetween users and the recommender systems; (2) focusing on the\nimmediate feedback of recommended items and neglecting the\nlong-term rewards. To address the two limitations, in this paper\nwe propose a novel recommendation framework based on deep\nreinforcement learning, called DRR. The DRR framework treats\nrecommendation as a sequential decision making procedure and\nadopts an \u201cActor-Critic\u201d reinforcement learning scheme to model\nthe interactions between the users and recommender systems,\nwhich can consider both the dynamic adaptation and long-\nterm rewards. Further more, a state representation module is\nincorporated into DRR, which can explicitly capture the interac-\ntions between items and users. Three instantiation structures are\ndeveloped. Extensive experiments on four real-world datasets are\nconducted under both the of\ufb02ine and online evaluation settings.\nThe experimental results demonstrate the proposed DRR method\nindeed outperforms the state-of-the-art competitors.\nIndex Terms \u2014Recommendation, Deep Reinforcement Learn-\ning, User-Item Interactions\nI. I NTRODUCTION\nThanks to the increasing online services, such as online\nshopping, online news and online social networks, it becomes\nquite convenient to acquire items (goods, books, videos,\nnews, etc.) via Internet or mobile devices. Albeit the great\nconvenience, the overwhelming number of items in the sys-\ntems also pose a signi\ufb01cant challenge for users, to \ufb01nd the\nitems that match their interests. Recommendation is a widely\nused solution and various families of techniques have been\nproposed, such as content-based collaborative \ufb01ltering [1],\nmatrix factorization based methods [2]\u2013[5], logistic regression,\nfactorization machines and its variants [6]\u2013[8], deep learning\nmodels [9]\u2013[12] and multi-armed bandits [13]\u2013[17]. However,\nsuch mentioned studies suffer from two serious limitations.Firstly , most of them consider the recommendation proce-\ndure as a static process, i.e., they assume the user\u2019s underlying\npreference keeps unchanged. However, it is very common\nthat a user\u2019s preference is dynamic w.r.t. time, i.e., a user\u2019s\npreference on previous items will affect her choice on the next\nitems. Hence, it would be more reasonable to model the recom-\nmendation as a sequential decision making process. We will\nshow some evidence observed in publicly available datasets\n(MovieLens and Yahoo! Music) to support our opinion. In the\ntwo datasets, the sequential behaviors of users are recorded and\nwe are interested in what would happen if a user consecutively\nreceives satis\ufb01ed or unsatis\ufb01ed recommendations. Though the\ndatasets do not record any recommendation procedure, we\ncan simulate this according to the users\u2019 ratings, namely,\nconsecutive rating \u201cpositive\u201d (\u201cnegative\u201d) simulates that a user\nconsecutively receives satis\ufb01ed (unsatis\ufb01ed) recommendations.\nAs presented in Figure 1, we observe that a user tends\nto gives a higher (lower) rating if she has consecutively\nreceived more satis\ufb01ed (unsatis\ufb01ed) items, as shown by the\ngreen (red) line, where the blue dot line denotes the average\nrating for reference.", "start_char_idx": 0, "end_char_idx": 4131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00391c6e-7a4e-4478-a9d1-b282ad7ee62c": {"__data__": {"id_": "00391c6e-7a4e-4478-a9d1-b282ad7ee62c", "embedding": null, "metadata": {"page_label": "1", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e841420-0ab3-4331-8b62-49fd7d387b45", "node_type": "4", "metadata": {"page_label": "1", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "9d3aa91ccb7566d963d33339b13fdf0bb272dc4e4ca27b5f9e6ab90abd4af4de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7db136dc-ee57-4a77-9846-8065af8ba6a6", "node_type": "1", "metadata": {"page_label": "1", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "c8f1c4c1930094a370b441a463cb674a4213eadc5833d6149a253f7ddff1e5ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16abbe40-c1b8-4a71-a5c8-275ec5ea1bf7", "node_type": "1", "metadata": {}, "hash": "f8169d607beecb64a256c97072e69eea6bcaab2f93e5209626300c8732b8b649", "class_name": "RelatedNodeInfo"}}, "text": "Music) to support our opinion. In the\ntwo datasets, the sequential behaviors of users are recorded and\nwe are interested in what would happen if a user consecutively\nreceives satis\ufb01ed or unsatis\ufb01ed recommendations. Though the\ndatasets do not record any recommendation procedure, we\ncan simulate this according to the users\u2019 ratings, namely,\nconsecutive rating \u201cpositive\u201d (\u201cnegative\u201d) simulates that a user\nconsecutively receives satis\ufb01ed (unsatis\ufb01ed) recommendations.\nAs presented in Figure 1, we observe that a user tends\nto gives a higher (lower) rating if she has consecutively\nreceived more satis\ufb01ed (unsatis\ufb01ed) items, as shown by the\ngreen (red) line, where the blue dot line denotes the average\nrating for reference. This suggests that a user will be more\npleasant (unpleasant) if she consecutively receives more sat-\nis\ufb01ed (unsatis\ufb01ed) recommendations and therefore she tends\nto give a higher (lower) rating to the current recommendation.\nHence, the user\u2019s dynamic preference suggests that a good\nrecommendation should be modeled as a sequential decision\nmaking process.\nSecondly , the aforementioned studies are trained by max-\nimizing the immediate rewards of recommendations, which\nmerely concentrates on whether the recommended items are\nclicked or consumed, but ignores the long-term contributions\nthat the items can make. However, the items with small imme-\ndiate rewards but large long-term bene\ufb01ts are also crucial [18].\nWe take an example in News recommendation [19] to explain\nthis. As a user requests for news to read, two possible pieces of\nnews may lead to the same immediate reward, i.e., the user willarXiv:1810.12027v3  [cs.IR]  29 Oct 2019", "start_char_idx": 3408, "end_char_idx": 5072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16abbe40-c1b8-4a71-a5c8-275ec5ea1bf7": {"__data__": {"id_": "16abbe40-c1b8-4a71-a5c8-275ec5ea1bf7", "embedding": null, "metadata": {"page_label": "2", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd7e3a64-b63b-43d1-8db0-a8b2ab7cdb12", "node_type": "4", "metadata": {"page_label": "2", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "b31b6311ab83c8df71e0ee055568d4752b569f44a0c7eacf0f4883c11c2256c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00391c6e-7a4e-4478-a9d1-b282ad7ee62c", "node_type": "1", "metadata": {"page_label": "1", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "714e21722fe4ee24f29be5d24bf63cb47377e397382a24ca7518b2d23c2a24df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9a454c6-c2fb-4659-bc49-66b6c3c39676", "node_type": "1", "metadata": {}, "hash": "3a74f2cfa0bc9dfa1e3de1deb7877786922a8386cd6254392c4a54489ebec78f", "class_name": "RelatedNodeInfo"}}, "text": "Fig. 1. Analysis on sequential patterns on user\u2019s behavior in MovieLens and\nYahoo!Music datasets\nclick and read the two pieces of news with equal probability,\nwhere one is about a thunderstorm alert and the other is about\na basketball player Kobe Bryant. In this example, after reading\nthe news about thunderstorm, the user probably is not willing\nto read news about this issue anymore; while on the other hand,\nthe user will possibly read more about NBA or basketball\nafter reading the news about Kobe. The fact suggests that\nrecommending the news about Kobe will introduce more long-\nterm rewards. Hence, when recommending items to users, both\nthe immediate and long-term rewards should be taken into\nconsideration.\nRecently, Reinforcement Learning (RL) [20], which has\nshown great potential in various challenging scenarios that\nrequire both dynamic modeling and long term planning, such\nas game playing [21], [22], real-time ads bidding [23], [24],\nneural network structure searching [25], [26], is introduced in\nrecommender systems [18], [19], [27]\u2013[33].\nIn the early stage, model-based RL techniques are proposed\nto model recommendation procedure, such as POMDP [18]\nand Q-learning [27]. However, these methods are inapplicable\nto complicated recommendation scenarios when the number\nof candidate items is large, because a time-consuming dy-\nnamic programming step is required to update the model.\nLater, model-free RL techniques are utilized in recommender\nsystems, from both academia and industry. Such techniques\ncan be divided into two categories: value-based [19], [29] and\npolicy-based [28], [32], [33]. Value-based approaches compute\nQ-values of all available actions for a given state and the\none with the maximum Q-value is selected as the best action.\nDue to the evaluation on overall actions, the approaches may\nbecome very inef\ufb01cient if the action space is too large. As\nfor the policy-based approaches, this type of studies generate\na continuous parameter vector as the representation of an\naction [28], [32], [33], which can be utilized in generating the\nrecommendation and updating the Q-value evaluator. Thanks\nto the continuous representations, the inef\ufb01ciency drawbacks\ncan be overcome. However, these studies [28], [32], [33] still\nhave one common limitation: the user state is learnt via a\nconventional fully connected neural network, which does not\nexplicitly and carefully model the interactions between users\nand items.\nIn this paper, to break the limitations stated above, we\npropose a d eep r einforcement learning based r ecommendation\nframework with explicit user-item interactions modeling(DRR). The \u201cActor-Critic\u201d type framework DRR is incor-\nporated with a state representation module, which explicitly\nmodels the complex dynamic user-item interactions to pursuit\nbetter recommendation performance. Speci\ufb01cally, the embed-\ndings of users and items from the historical interactions are fed\ninto a carefully designed multi-layer network, which explicitly\nmodels the interactions between users and items, to produce\na continuous state representation of the user in terms of her\nunderlying sequential behaviors. This network is named as\nthe state representation module, which plays two important\nroles in our framework. On the one hand, it is utilized to\ngenerate an ranking action to calculate the recommendation\nscores for ranking. On the other hand, the state representation\ntogether with the generated action is the input of the Critic\nnetwork, which aims to estimate the Q-value, i.e., the quality\nof the action in the current state. Based on the evaluation, the\nActor (policy) network can be updated. We note that both the\nActor and Critic networks are carefully designed by modeling\nthe interactions between users and items explicitly. Extensive\nexperiments on four real-world datasets demonstrate that the\nproposed method yields superior performance than the state-\nof-the-art methods. The main contributions of this paper can\nbe summarized as follows:\n\u2022We propose a deep reinforcement learning based rec-\nommendation framework DRR. Unlike the conventional\nstudies, DRR adopts an \u201cActor-Critic\u201d structure and treats\nthe recommendation as a sequential decision making\nprocess, which takes both the immediate and long-term\nrewards into consideration.\n\u2022Under the DRR framework, three different network struc-\ntures are proposed, which can explicitly model the inter-\nactions between users and items.\n\u2022Extensive experiments are carried out on four real-world\ndatasets, and the results demonstrate the proposed meth-\nods indeed outperforms the state-of-the-art competitors.\nThe rest of this paper is organized as follows.", "start_char_idx": 0, "end_char_idx": 4656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9a454c6-c2fb-4659-bc49-66b6c3c39676": {"__data__": {"id_": "d9a454c6-c2fb-4659-bc49-66b6c3c39676", "embedding": null, "metadata": {"page_label": "2", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd7e3a64-b63b-43d1-8db0-a8b2ab7cdb12", "node_type": "4", "metadata": {"page_label": "2", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "b31b6311ab83c8df71e0ee055568d4752b569f44a0c7eacf0f4883c11c2256c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16abbe40-c1b8-4a71-a5c8-275ec5ea1bf7", "node_type": "1", "metadata": {"page_label": "2", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "e30d01d56e39cc698e7d86cca0c7d1b0f8748f6ac8a4e47dd2b99917d81d35d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b00d6fa4-a8db-46fb-bfdc-50dd0cb2cc06", "node_type": "1", "metadata": {}, "hash": "cd0b0600a63bd4981d4c71aea66d19ddfafde4d9a484c4a0ed424471c78135cc", "class_name": "RelatedNodeInfo"}}, "text": "Extensive\nexperiments on four real-world datasets demonstrate that the\nproposed method yields superior performance than the state-\nof-the-art methods. The main contributions of this paper can\nbe summarized as follows:\n\u2022We propose a deep reinforcement learning based rec-\nommendation framework DRR. Unlike the conventional\nstudies, DRR adopts an \u201cActor-Critic\u201d structure and treats\nthe recommendation as a sequential decision making\nprocess, which takes both the immediate and long-term\nrewards into consideration.\n\u2022Under the DRR framework, three different network struc-\ntures are proposed, which can explicitly model the inter-\nactions between users and items.\n\u2022Extensive experiments are carried out on four real-world\ndatasets, and the results demonstrate the proposed meth-\nods indeed outperforms the state-of-the-art competitors.\nThe rest of this paper is organized as follows. Related work\nand background are presented in Section II. The preliminary\nknowledge is presented in Section III. The proposed methods\nare introduced in Section IV . Experimental details and results\nare discussed in Section V . Finally, we conclude this paper\nand discuss some future work in Section VI.\nII. R ELATED WORK\nA. Non-RL based Recommendation Techniques\nVarious kinds of recommendation techniques are proposed\nin the past a few decades to improve the performance of\nrecommender systems, including content-based \ufb01ltering [1],\nmatrix factorization based methods [2]\u2013[5], logistic regression,\nfactorization machines and its variants [6]\u2013[8], and until\nrecently deep learning models [9]\u2013[12].\nAt the beginning of this century, content-based \ufb01ltering [1]\nis proposed to recommend items by considering the content\nsimilarity between items. Later, collaborative \ufb01ltering (CF) is\nput forward and extensively studied. The rationale behind CF\nis that the users with similar behaviors tend to prefer the same", "start_char_idx": 3775, "end_char_idx": 5662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b00d6fa4-a8db-46fb-bfdc-50dd0cb2cc06": {"__data__": {"id_": "b00d6fa4-a8db-46fb-bfdc-50dd0cb2cc06", "embedding": null, "metadata": {"page_label": "3", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7cdc9ccc-7d69-4451-b304-6c9c58e7c82f", "node_type": "4", "metadata": {"page_label": "3", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "630d026addd73a1d8417f00c6366067e705c2a558c6b47bd049ea30d6207cd60", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9a454c6-c2fb-4659-bc49-66b6c3c39676", "node_type": "1", "metadata": {"page_label": "2", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "2265db8607ef113144dca3779962de10b3be4958db2e4aa590a45822791db644", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e5c0f3e-0971-47d5-8992-763233432c7a", "node_type": "1", "metadata": {}, "hash": "85b4651e09b64a3c3b77ef455dcbb1501b47f964d85670eb1195f803ad614222", "class_name": "RelatedNodeInfo"}}, "text": "items, and the items consumed by similar users tend to have\nthe same rating. However, conventional CF based methods\ntend to suffer from the data scarcity, because the similarity\ncalculated from sparse data can be very unreliable. Matrix\nfactorization (MF), as an advanced CF technique, plays an\nimportant role in recommender systems. MF models [2]\u2013[5]\ncharacterize both items and users by vectors in the same space,\nwhich are inferred from the observed user-item interactions.\nRegarding the recommendation as a binary classi\ufb01cation prob-\nlem, logistic regression and its variants [6] are also utilized\nin recommender systems. However, logistic regression based\nmodels are hard to generalize to the feature interactions that\nnever or rarely appear in the training data. Factorization\nmachines [7] model pairwise feature interactions as inner\nproduct of latent vectors between features and show promising\nresults. As an extension to FM, Field-aware FM (FFM [8])\nenables each feature to have multiple latent vectors to interact\nwith different \ufb01elds. Recently, deep learning models [9]\u2013[12]\nare applied to model the complicated feature interactions for\nrecommendation.\nAs a distinguished direction, contextual multi-armed bandits\nare also utilized to model the interactive nature of recom-\nmender systems [13]\u2013[17]. Li et al. apply Thompson Sampling\n(TS) and Upper Con\ufb01dent Bound (UCB) to balance the trade-\noff between exploration and exploitation in [13] and [14],\nrespectively. The authors of [16] propose a dynamic context\ndrift model to address the time varying problem. To integrate\nthe latent vectors of items and users with some exploration,\nthe authors of [15], [17] combine matrix factorization with\nmulti-armed bandits.\nHowever, all these methods suffer from two limitations.\nFirst, they consider the recommendation procedure as a static\nprocess, i.e., they assume the underlying user\u2019s preference\nkeeps static and they aim to learn the user\u2019s preference as\nprecise as possible. Second, they are learned to maximize the\nimmediate rewards of recommendations, but ignore the long-\nterm bene\ufb01ts that the recommendations can make.\nB. RL based Recommendation Techniques\nAs model-based RL techniques [18], [27] are inapplicable\nin recommendation scenario due to their high time complex-\nity, most researchers turn to model-free RL techniques. The\nmodel-free RL techniques can be divide into two categories:\npolicy-based and value-based.\nPolicy-based approaches [28], [32], [33] aim to generate\na policy, of which the input is a state, and the output is\nan action. These works apply deterministic policies, which\ngenerates an action directly. Dulac-Arnold et al. [33] resolves\nthe large action space problem by modeling the state in a\ncontinuous item embedding space and selecting the items via\na neighborhood method. However, as the underlying algorithm\nis essentially a continuous-action algorithm, its performance\nmay be cursed by the gap between the continuous and discrete\naction spaces. In [28], [32], the policy network outputs a\ncontinuous action representation, and the recommendation is\ngenerated by ranking the items with their scores, which arecomputed by a pre-de\ufb01ned function with the action representa-\ntion and the item embeddings as input. However, one common\nlimitation of the studies is that they do not carefully learn the\nstate representation.\nFor value-based approaches [19], [29], the action with max-\nimum Q-value over all the possible actions is selected as the\nbest action. Zhao et al. [29] take both user\u2019s positive feedback\nand negative feedback into consideration when modeling user\nstate. Dueling Q-network is utilized in [19], to model Q-\nvalue of a state-action pair. Moreover, a minor update with\nexploration by dueling bandit gradient descent is proposed.\nHowever, such value-based approaches need to evaluate the\nQ-values of all the actions under a speci\ufb01c state, which is\nvery inef\ufb01cient when the number of actions is large.\nTo make RL based recommendation techniques suitable for\nlarge-scale scenario, in this paper, we propose the DRR frame-\nwork which carefully and explicitly model the interactions\nbetween users and items to learn the state representation.\nIII. P RELIMINARIES\nThe essential underlying model of reinforcement learning\nis Markov Decision Process (MDP).", "start_char_idx": 0, "end_char_idx": 4308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e5c0f3e-0971-47d5-8992-763233432c7a": {"__data__": {"id_": "8e5c0f3e-0971-47d5-8992-763233432c7a", "embedding": null, "metadata": {"page_label": "3", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7cdc9ccc-7d69-4451-b304-6c9c58e7c82f", "node_type": "4", "metadata": {"page_label": "3", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "630d026addd73a1d8417f00c6366067e705c2a558c6b47bd049ea30d6207cd60", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b00d6fa4-a8db-46fb-bfdc-50dd0cb2cc06", "node_type": "1", "metadata": {"page_label": "3", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "8166aaf6db86d098916292ca68ede3df9bd0b5b74e827694c2657d77d9b0e2dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2acf71c7-adad-4ee8-bce3-6f291a083a97", "node_type": "1", "metadata": {}, "hash": "d23a87a242edacada3e5abc237f9ab2312f414039b0ee61c81af7479e0a9217d", "class_name": "RelatedNodeInfo"}}, "text": "Zhao et al. [29] take both user\u2019s positive feedback\nand negative feedback into consideration when modeling user\nstate. Dueling Q-network is utilized in [19], to model Q-\nvalue of a state-action pair. Moreover, a minor update with\nexploration by dueling bandit gradient descent is proposed.\nHowever, such value-based approaches need to evaluate the\nQ-values of all the actions under a speci\ufb01c state, which is\nvery inef\ufb01cient when the number of actions is large.\nTo make RL based recommendation techniques suitable for\nlarge-scale scenario, in this paper, we propose the DRR frame-\nwork which carefully and explicitly model the interactions\nbetween users and items to learn the state representation.\nIII. P RELIMINARIES\nThe essential underlying model of reinforcement learning\nis Markov Decision Process (MDP). An MDP is de\ufb01ned as\n(S,A,P,R,\u03b3).Sis the state space and Ais the action\nspace.P:S\u00d7A\u00d7S \u21a6\u2192 [0,1]is the state transition\nfunction.R:S\u00d7A\u00d7S \u21a6\u2192 Ris the reward function. \u03b3\nis the discount rate. The objective of an agent in an MDP\nis to \ufb01nd an optimal policy ( \u03c0\u03b8:S\u00d7A \u21a6\u2192 [0,1]) which\nmaximizes the expected cumulative rewards from any state\ns\u2208 S , i.e.,V\u2217(s) = max \u03c0\u03b8E\u03c0\u03b8{\u2211\u221e\nk=0\u03b3krt+k|st=s},\nor maximizes equivalently the expected cumulative rewards\nfrom any state-action pair s\u2208 S,a\u2208 A , i.e.,Q\u2217(s,a) =\nmax\u03c0\u03b8E\u03c0\u03b8{\u2211\u221e\nk=0\u03b3krt+k|st=s,at=a}. Here E\u03c0\u03b8is the\nexpectation under policy \u03c0\u03b8,tis the current timestep and rt+k\nis the immediate reward at a future timestep t+k.\nWe model the recommendation procedure as a sequential\ndecision making problem, in which the recommender (i.e.,\nagent) interacts with users (i.e., environment) to suggest a list\nof items sequentially over the timesteps, by maximizing the\ncumulative rewards of the whole recommendation procedure.\nMore speci\ufb01cally, the recommendation procedure is modeled\nby an MDP, as follows.\n\u2022StatesS.A statesis the representation of user\u2019s positive\ninteraction history with recommender, as well as her\ndemographic information (if it exists in the datasets).\n\u2022ActionsA.An actionais a continuous parameter vector\ndenoted as a\u2208R1\u00d7k. Each item it\u2208R1\u00d7k1has a\nranking score, which is de\ufb01ned as the inner product of\nthe action and the item embedding, i.e., ita\u22a4. Then the\ntop ranked ones will be recommended.\n\u2022TransitionsP.The state is modeled as the representa-\ntion of user\u2019s positive interaction history. Hence, once\nthe user\u2019s feedback is collected, the state transition is\ndetermined.\n1itis the embedding of item i, which can be generated by MF or V AE.", "start_char_idx": 3500, "end_char_idx": 6000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2acf71c7-adad-4ee8-bce3-6f291a083a97": {"__data__": {"id_": "2acf71c7-adad-4ee8-bce3-6f291a083a97", "embedding": null, "metadata": {"page_label": "4", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b95f758-0a02-4385-9648-ed9e5000ab71", "node_type": "4", "metadata": {"page_label": "4", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "1deb4f222739b0e78e16c4a4be62120148d86c62f9c62362387f52fd9e1c9e44", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e5c0f3e-0971-47d5-8992-763233432c7a", "node_type": "1", "metadata": {"page_label": "3", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "4c547c0198c891e1c8a21bcb29b9be646e760e6cf5a5959cb379aaa5c2b89a97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2ef63db-7d27-4212-8388-d42ae717fff9", "node_type": "1", "metadata": {}, "hash": "0f7b6391da92e9b3fb5e3b4ee71bed2fd3074c6c7c1196d18918829c0417a83c", "class_name": "RelatedNodeInfo"}}, "text": "\u2022RewardR.Given the recommendation based on the\nactionaand the user state s, the user will provide her\nfeedback, i.e., click, not click, or rating, etc. The recom-\nmender receives immediate reward R(s,a)according to\nthe user\u2019s feedback.\n\u2022Discount rate \u03b3.\u03b3\u2208[0,1]is a factor measuring the\npresent value of long-term rewards. In the case of \u03b3= 0,\nthe recommender considers only immediate rewards but\nlong-term rewards are ignored. On the other hand, when\n\u03b3= 1, the recommender treats immediate rewards and\nlong-term rewards as equally important.\nFigure 2 illustrates the recommender-user interactions in\nMDP formulation. Considering the current user state and\nimmediate reward to the previous action, the recommender\ntakes an action. Note that in our model, an action corre-\nsponds to neither recommending an item nor recommending\na list of items. Instead, an action is a continuous parameter\nvector. Taking such an action, the parameter vector is used\nto determine the ranking scores of all the candidate items,\nby performing inner product with item embeddings. All the\ncandidate items are ranked according to the computed scores\nand Top-N items are recommended to the user. Taking the\nrecommendation from the recommender, the user provides her\nfeedback to the recommender and the user state is updated\naccordingly. The recommender receives rewards according\nto the user\u2019s feedback. Without loss of generalization, a\nrecommendation procedure is a Ttimestep2trajectory as\n(s0,a0,r0,s1,a1,r1,...,sT\u22121,aT\u22121,rT\u22121,sT).\nRecommender (Agent)\nUsers (Environment)state!\"!\"=$(&\")reward(\"(\"=)(!\",+\")!\",-(\",-action+\"+\"=./(!\")\nFig. 2. Recommender-User interactions in MDP\nIV. T HEPROPOSED DRR F RAMEWORK\nAs aforementioned in Section 1, conventional recommenda-\ntion techniques suffer from either a lack of sequential model-\ning or ignoring the long-term rewards, or both. To address\nthe drawbacks, we propose a deep reinforcement learning\nbased recommendation framework (DRR) based on the Actor-\nCritic learning scheme. Also, different from some recent RL\nstudies, we carefully and explicitly build a state representation\nmodule to model the interactions between the users and items.\nNext, we will \ufb01rst elaborate the Actor network, Critic network\nand the state representation module respectively, which are\n2If a recommendation episode terminates in less than T timesteps, then the\nlength of the episode is the actual value.essentially the three key ingredients in our framework; then\nthe training and evaluation procedures will be presented to\nshow how to learn and use the DRR framework.\nSReLUReLUaReLUconcatQ(s,a)State RepresentationModuleItemsSReLUReLUaTanhItem Space.RankingRecommendationActorCriticLegend.Item or userScalar productFC layerXElement-wise productWeight 1Item weightAverage poolinglayerScalar \nFig. 3. DRR Framework\nA. Three Key Ingredients in DRR\n1) The Actor network: The Actor network, also called the\npolicy network, is depicted on the left part of Figure 3. For\na given user, the network accounts for generating an action\nabased on her state s. Let us explain the network from the\ninput to the output part. In DRR, the user state, denoted by\nthe embeddings of her nlatest positively interacted items,\nis regarded as the input. Then the embeddings are fed into\na state representation module (which will be introduced in\ndetails later) to produce a summarized representation sfor the\nuser. For instance, at timestep t, the state can be de\ufb01ned in\nEq. (1):\nst=f(Ht) (1)\nwheref(\u00b7)stands for the state representation module, Ht=\n{i1,...,in}denotes the embeddings of the latest positive inter-\naction history, and it\u2208R1\u00d7kis ak-dimensional vector. When\nthe recommender agent recommends an item it, if the user\nprovides positive feedback, then in the next timestep, the state\nis updated to st+1=f(Ht+1), whereHt+1={i2,...,in,it};\notherwise,Ht+1=Ht.", "start_char_idx": 0, "end_char_idx": 3846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2ef63db-7d27-4212-8388-d42ae717fff9": {"__data__": {"id_": "a2ef63db-7d27-4212-8388-d42ae717fff9", "embedding": null, "metadata": {"page_label": "4", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b95f758-0a02-4385-9648-ed9e5000ab71", "node_type": "4", "metadata": {"page_label": "4", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "1deb4f222739b0e78e16c4a4be62120148d86c62f9c62362387f52fd9e1c9e44", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2acf71c7-adad-4ee8-bce3-6f291a083a97", "node_type": "1", "metadata": {"page_label": "4", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "cd3b1b3fce52d138d53f1498d0073e69c509090d293c2bf7f66667af7cd55c9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be2a97f7-8c32-4d37-9383-eb2ae00559ea", "node_type": "1", "metadata": {}, "hash": "1d42df03e22565f73d25b73adbc4b868e93e25d349684901373eb39c4b075fd6", "class_name": "RelatedNodeInfo"}}, "text": "Then the embeddings are fed into\na state representation module (which will be introduced in\ndetails later) to produce a summarized representation sfor the\nuser. For instance, at timestep t, the state can be de\ufb01ned in\nEq. (1):\nst=f(Ht) (1)\nwheref(\u00b7)stands for the state representation module, Ht=\n{i1,...,in}denotes the embeddings of the latest positive inter-\naction history, and it\u2208R1\u00d7kis ak-dimensional vector. When\nthe recommender agent recommends an item it, if the user\nprovides positive feedback, then in the next timestep, the state\nis updated to st+1=f(Ht+1), whereHt+1={i2,...,in,it};\notherwise,Ht+1=Ht. The reasons to de\ufb01ne the state in such\na manner are two folds: (i) a superior recommender system\nshould cater to the users\u2019 taste, i.e., what items the users like;\n(ii) the latest records represent the users\u2019 recent interests more\nprecisely.\nFinally, by two ReLU layers and one Tanh layer, the state\nrepresentation sis transformed into an action a=\u03c0\u03b8(s)as\nthe output of the Actor network. Particularly, the action a\nis de\ufb01ned as a ranking function represented by a continuous\nparameter vector a\u2208R1\u00d7k. By using the action, the ranking\nscore of the item itis de\ufb01ned as:\nscoret=ita\u22a4(2)\nThen, the top ranked item (w.r.t. the ranking scores) is rec-\nommended to the user. Note that, the widely used \u03b5-greedy\nexploration technique is adopted here.", "start_char_idx": 3234, "end_char_idx": 4588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be2a97f7-8c32-4d37-9383-eb2ae00559ea": {"__data__": {"id_": "be2a97f7-8c32-4d37-9383-eb2ae00559ea", "embedding": null, "metadata": {"page_label": "5", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4db6341-4067-472a-b3f4-3e28cda74469", "node_type": "4", "metadata": {"page_label": "5", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "b46005c10d546e52bdf5bef67a3a554fe75c9b1a5c3c931dd78dcd17ec604993", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2ef63db-7d27-4212-8388-d42ae717fff9", "node_type": "1", "metadata": {"page_label": "4", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "dd35a8a840e07c4555eef625d9f85b49bb7d7e790f7f73df4c469554a2345f7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba9c9a35-5657-4d59-bfc3-33559d06b4bb", "node_type": "1", "metadata": {}, "hash": "be6ab64b3c75dde26ff31c0f2844a52f90e226aef2c5757cb4788a43d06a31da", "class_name": "RelatedNodeInfo"}}, "text": "2) The Critic network: The Critic part in DRR, shown as\nthe middle part of Figure 3, is a Deep Q-Network [21], which\nleverages a deep neural network parameterized as Q\u03c9(s,a)\nto approximate the true state-action value function Q\u03c0(s,a),\nnamely, the Q-value function. The Q-value function re\ufb02ects the\nmerits of the action policy generated by the Actor network.\nSpeci\ufb01cally, the input of the Critic network is the user state\nsgenerated by the user state representation module and the\nactionagenerated by the policy network, and the output is\nthe Q-value, which is a scalar. According to the Q-value, the\nparameters of the Actor network are updated in the direction of\nimproving the performance of action a, i.e., boosting Q\u03c9(s,a).\nBased on the deterministic policy gradient theorem [34], we\ncan update the Actor by the sampled policy gradient shown\nin Eq.(3):\n\u2207\u03b8J(\u03c0\u03b8)\u22481\nN\u2211\nt\u2207aQ\u03c9(s,a)|s=st,a=\u03c0\u03b8(st)\u2207\u03b8\u03c0\u03b8(s)|s=st\n(3)\nwhereJ(\u03c0\u03b8)is the expectation of all possible Q-values that\nfollow the policy \u03c0\u03b8. Here the mini-batch strategy is utilized\nandNdenotes the batch size. Moreover, the Critic network\nis updated accordingly by the temporal-difference learning\napproach [20], i.e., minimizing the mean squared error shown\nin Eq.(4):\nL=1\nN\u2211\ni(yi\u2212Q\u03c9(si,ai))2(4)\nwhereyi=ri+\u03b3Q\u03c9\u2032(si+1,\u03c0\u03b8\u2032(si+1)). The target net-\nwork [35] technique is also adopted in DRR framework, where\n\u03c9\u2032and\u03b8\u2032is the parameters of the target Critic and Actor\nnetwork.\n3) The State Representation Module: As noted above, the\nstate representation module plays an important role in both\nthe Actor network and Critic network. Hence, it is very\ncrucial to design a good structure to model the state. In [10],\n[11], it has been shown that modeling the feature interactions\nexplicitly can boost the performance of a recommendation\nsystem. Inspired by the studies, we propose to design the state\nrepresentation module by explicitly modeling the interactions\nbetween the users and items. Speci\ufb01cally, we develop three\nstructures, which will be elaborated next.\nItemsS\u2026Concat& flattenXXX\nFig. 4. DRR-p Structure\n\u2022DRR-p . Inspired by [10], [11], we propose a product\nbased neural network for the state representation module,which is depicted in Figure 43. The structure is named\nas DRR-p, which utilizes a product operator to capture\nthe pairwise local dependency between items. We can\nsee that the structure clones the representations of the n\nitems fromH={i1,...,in}. In addition, it computes the\npairwise interactions between the nitems, by using the\nelement-wise product operator. As a result, n(n\u22121)/2\nnew features vectors are yielded, which will be concate-\nnated with the cloned vectors as the state representation.\nWe note that in the element-wise product part, a weight is\nalso learned for each item to show its importance. Hence,\nin DRR-p the state representation module can be formally\nstated as follows:\ns= [H,{pa,b|a,b= 1,...,n}] (5)\npa,b=waia\u2297wbib (6)\nwhere\u2297denotes the element-wise product, wais a\nscalar indicating the importance of item ia, andpa,bis\nak-dimensional vector which models the interactions\nbetween item iaandib. The dimensionality of sis\nk(n+n(n\u22121)/2).\n\u2022DRR-u . Though DRR-p can model the pairwise local\ndependency between items, the user-item interactions are\nneglected. To remedy this, we design another structure in\nFigure 5, which is referred as DRR-u. In DRR-u, we\ncan see that the user embedding is also incorporated.\nIn addition to the local dependency between items, the\npairwise interactions of user-item are also taken into\naccount.", "start_char_idx": 0, "end_char_idx": 3514, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba9c9a35-5657-4d59-bfc3-33559d06b4bb": {"__data__": {"id_": "ba9c9a35-5657-4d59-bfc3-33559d06b4bb", "embedding": null, "metadata": {"page_label": "5", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4db6341-4067-472a-b3f4-3e28cda74469", "node_type": "4", "metadata": {"page_label": "5", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "b46005c10d546e52bdf5bef67a3a554fe75c9b1a5c3c931dd78dcd17ec604993", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be2a97f7-8c32-4d37-9383-eb2ae00559ea", "node_type": "1", "metadata": {"page_label": "5", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "380953ecc31e8e332419aefcdf5ba62ddc38b23b08d1822443617558faff3ed1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60a1a0df-5e79-4431-875d-19e9013e0af2", "node_type": "1", "metadata": {}, "hash": "506ba60c56c1710c6904ba05f59687d666bd7fb842b2ca59590ef186c7cf94d2", "class_name": "RelatedNodeInfo"}}, "text": "The dimensionality of sis\nk(n+n(n\u22121)/2).\n\u2022DRR-u . Though DRR-p can model the pairwise local\ndependency between items, the user-item interactions are\nneglected. To remedy this, we design another structure in\nFigure 5, which is referred as DRR-u. In DRR-u, we\ncan see that the user embedding is also incorporated.\nIn addition to the local dependency between items, the\npairwise interactions of user-item are also taken into\naccount. Formally, the state representation module can\nbe expressed as:\ns= [{u\u2297waia|a= 1,...,n},{pa,b|a,b= 1,...,n}](7)\nThe dimensionality of sis alsok(n+n(n\u22121)/2).\nItemsS\u2026Concat& flattenXXXuserXXXXX\nFig. 5. DRR-u Structure\n\u2022DRR-ave . In DRR-p and DRR-u structures, the inter-\nactions between users and items can be exploited and\nmodeled. For the two structures, it is not dif\ufb01cult to \ufb01nd\nthat the positions of items in Hmatters, e.g., the state\nrepresentations of H1={ia,ib,ic}andH2={ic,ib,ia}\n3The legend in Figure 4, 5 and 6 is the same to Figure 3", "start_char_idx": 3084, "end_char_idx": 4057, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60a1a0df-5e79-4431-875d-19e9013e0af2": {"__data__": {"id_": "60a1a0df-5e79-4431-875d-19e9013e0af2", "embedding": null, "metadata": {"page_label": "6", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "309b599c-d2a9-4b9f-84f5-d0a2f2b5d590", "node_type": "4", "metadata": {"page_label": "6", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "da5963a06a59eb61902b58252fe01e5405329d09051ac64d993914551e5a6714", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba9c9a35-5657-4d59-bfc3-33559d06b4bb", "node_type": "1", "metadata": {"page_label": "5", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "456bf675a4e25311b8fa02c196c7b60de5e33afc2f6e23e5510684ea0197beb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97862008-8561-43b2-8aff-5ae8e8157387", "node_type": "1", "metadata": {}, "hash": "fb71fed15d4f4695d0483a6bc8eae510ed6e561249d56cfd205d452bd3ab1226", "class_name": "RelatedNodeInfo"}}, "text": "are different. When His large, we expect the positions\nof items really matter, because Hdenotes a long-term\nsequence; whereas memorizing the positions of items\nmay lead to over\ufb01tting if the sequence His a short-term\none. Hence, we design another structure by eliminating\nthe position effects, which is depicted in Figure 6. As an\naverage pooling layer is adopted, we call the structure\nDRR-ave. We can see from Figure 6 that the embeddings\nof items inHare \ufb01rst transformed by a weighted average\npooling layer. Then, the resulting vector is leveraged to\nmodel the interactions with the input user. Finally, the\nembedding of the user, the interaction vector, and the\naverage pooling result of items are concatenate into a\nvector to denote the state representation. Formally, the\nDRR-ave structure can be expressed as:\ns= [u,u\u2297{g(ia)|a= 1,...,n},{g(ia)|a= 1,...,n}]\n(8)\ng(ia) =ave(waia)|a= 1,...,n (9)\nHereg(\u00b7)indicates the weighted average pooling layer.\nThe dimensionality of sin DRR-ave is 3k.\nItemsSConcat& flattenXuser\nFig. 6. DRR-ave Structure\nB. Training Procedure of the DRR Framework\nNext, we introduce how to train the DRR framework. We\n\ufb01rst present the overall idea and then discuss the detailed\nalgorithm. As aforementioned, DRR utilizes the users\u2019 inter-\naction history with the recommender agent as training data.\nDuring the procedure, the recommender takes an action at\nfollowing the current recommendation policy \u03c0\u03b8(st)after\nobserving the user (environment) state st, then it obtains the\nfeedback (reward) rtfrom the user, and the user state is\nupdated tost+1. According to the feedback, the recommender\nupdates its recommendation policy. In this work, we utilize\ndeep deterministic policy gradient (DDPG) [35] algorithm to\ntrain the proposed DRR framework, as detailed in Algorithm\n1.\nSpeci\ufb01cally, in timestep t, the training procedure mainly\nincludes two phases, i.e., transition generation (lines 7-12)\nand model updating (lines 13-17). For the \ufb01rst stage, the\nrecommender observes the current state stthat is calculated\nby the proposed state representation module, then generates\nan actionat=\u03c0\u03b8(st)according to the current policy \u03c0\u03b8with\u03b5-greedy exploration, and recommends an item itaccording to\nthe actionatby Eq. (2) (lines 8-9). Subsequently, the reward\nrtcan be calculated based on the feedback of the user to\nthe recommended item it, and the user state is updated (lines\n10-11). Finally, the recommender agent stores the transition\n(st,at,rt,st+1)into the replay buffer D(line 12).\nIn the second stage, the model updating, the recommender\nsamples a minibatch of Ntransitions with widely used pri-\noritized experience replay [36] sampling technique (line 13),\nwhich is essentially an importance sampling strategy. Then, the\nrecommender updates the parameters of the Actor network and\nCritic network according to Eq. (3) and Eq. (4) respectively\n(line 14-16). Finally, the recommender updates the target\nnetworks\u2019 parameters with the soft replace strategy.\nAlgorithm 1: Training Algorithm of DRR Framework\ninput : Actor learning rate \u03b7a, Critic learning rate \u03b7c,\ndiscount factor \u03b3, batch size N, state window\nsizenand reward function R\n1Randomly initialize the Actor \u03c0\u03b8and the Critic Q\u03c9with\nparameters\u03b8and\u03c9\n2Initialize the target network \u03c0\u2032andQ\u2032with weights\n\u03b8\u2032\u2190\u03b8and\u03c9\u2032\u2190\u03c9\n3Initialize replay buffer D\n4forsession = 1, M do\n5 Observe the initial state s0according to the of\ufb02ine\nlog\n6 for t = 1, T do\n7 Observe current state st=f(Ht), where\nHt={i1,...,in}\n8 Find action at=\u03c0\u03b8(st)according to the current\npolicy with \u03b5-greedy exploration\n9 Recommended item itaccording to action atby\nEq.", "start_char_idx": 0, "end_char_idx": 3601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97862008-8561-43b2-8aff-5ae8e8157387": {"__data__": {"id_": "97862008-8561-43b2-8aff-5ae8e8157387", "embedding": null, "metadata": {"page_label": "6", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "309b599c-d2a9-4b9f-84f5-d0a2f2b5d590", "node_type": "4", "metadata": {"page_label": "6", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "da5963a06a59eb61902b58252fe01e5405329d09051ac64d993914551e5a6714", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60a1a0df-5e79-4431-875d-19e9013e0af2", "node_type": "1", "metadata": {"page_label": "6", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "6844d372a5f9a02f236aafd1969b1983d585221ce5a4634500cbd8b6c7620de6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16110f8e-4c4b-40f8-88e9-805e312cee13", "node_type": "1", "metadata": {}, "hash": "74bca74b17fee3641c9d6ee3bf3dec9f0b337c4d1b8ba0a3f95cfc0513705b24", "class_name": "RelatedNodeInfo"}}, "text": "Algorithm 1: Training Algorithm of DRR Framework\ninput : Actor learning rate \u03b7a, Critic learning rate \u03b7c,\ndiscount factor \u03b3, batch size N, state window\nsizenand reward function R\n1Randomly initialize the Actor \u03c0\u03b8and the Critic Q\u03c9with\nparameters\u03b8and\u03c9\n2Initialize the target network \u03c0\u2032andQ\u2032with weights\n\u03b8\u2032\u2190\u03b8and\u03c9\u2032\u2190\u03c9\n3Initialize replay buffer D\n4forsession = 1, M do\n5 Observe the initial state s0according to the of\ufb02ine\nlog\n6 for t = 1, T do\n7 Observe current state st=f(Ht), where\nHt={i1,...,in}\n8 Find action at=\u03c0\u03b8(st)according to the current\npolicy with \u03b5-greedy exploration\n9 Recommended item itaccording to action atby\nEq. (2)\n10 Calculate reward rt=R(st,at)based on the\nfeedback of the user\n11 Observe new state st+1=f(Ht+1), where\nHt+1={i2,...,in,it}ifrtis positive,\notherwise,Ht+1=Ht\n12 Store transition (st,at,rt,st+1)inD\n13 Sample a minibatch of Ntransitions\n(si,ai,ri,si+1)inDwith prioritized experience\nreplay sampling technique\n14 Setyi=ri+\u03b3Q\u03c9\u2032(si+1,\u03c0\u03b8\u2032(si+1))\n15 Update the Critic network by minimizing the\nloss:L=1\nN\u2211\ni(yi\u2212Q\u03c9(si,ai))2\n16 Update the Actor network using the sampled\npolicy gradient:\n\u2207\u03b8J(\u03c0\u03b8)\u2248\n1\nN\u2211\nt\u2207aQ\u03c9(s,a)|s=st,a=\u03c0\u03b8(st)\u2207\u03b8\u03c0\u03b8(s)|s=st\n17 Update the target networks:\n\u03b8\u2032\u2190\u03c4\u03b8+ (1\u2212\u03c4)\u03b8\u2032\n\u03c9\u2032\u2190\u03c4\u03c9+ (1\u2212\u03c4)\u03c9\u2032\n18return\u03b8and\u03c9", "start_char_idx": 2977, "end_char_idx": 4212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16110f8e-4c4b-40f8-88e9-805e312cee13": {"__data__": {"id_": "16110f8e-4c4b-40f8-88e9-805e312cee13", "embedding": null, "metadata": {"page_label": "7", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e808915-a91a-401f-9426-85e142367b6d", "node_type": "4", "metadata": {"page_label": "7", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "eea81e1e9f2e88478336284cb0136bd67a24e386705be163a1370649765bfe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97862008-8561-43b2-8aff-5ae8e8157387", "node_type": "1", "metadata": {"page_label": "6", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "3f18143361742d3b62b52838797bcf494ed9132ca1eff6c979607a41bdba3f70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d59eff3-58a2-44e1-9bd5-a7d079a903f2", "node_type": "1", "metadata": {}, "hash": "7b1f81fb01c1db6763bf95ed8b8b90aaa1887d4ebec8fd8f62ac883aded59ae3", "class_name": "RelatedNodeInfo"}}, "text": "C. Evaluation\nIn this subsection, we discuss how to evaluate the models\nwith a environment simulator. The most straightforward way to\nevaluate the RL based models is to conduct online experiments\non recommender systems where the recommender directly\ninteracts with users. However, the underlying commercial risk\nand the costly deployment on the platform make it impracti-\ncal. Therefore, throughout the testing phase, we conduct the\nevaluation of the proposed models on public of\ufb02ine datasets\nand propose two ways to evaluate the models, which are the\nof\ufb02ine evaluation and the online evaluation.\n1) Of\ufb02ine evaluation: Intuitively, the of\ufb02ine evaluation of\nthe trained models is to test the recommendation performance\nwith the learned policy, which is described in Algorithm 2.\nSpeci\ufb01cally, for a given session Sj, the recommender only\nrecommends the items that appear in this session, denoted as\nI(Sj), rather than the ones in the whole item space. The reason\nis that we only have the ground truth feedback for the items in\nthe session in the recoreded of\ufb02ine log. For each timestep, the\nrecommender agent takes an action ataccording to the learned\npolicy\u03c0\u03b8, and recommends an item it\u2208I(Sj)based on the\nactionatby Eq. (2) (lines 4-5). After that, the recommender\nobserves the reward rt=R(st,at)according to the feedback\nof the recommended item itby Eq. (10) (lines 5-6). Then the\nuser state is updated to st+1and the recommended item itis\nremoved from the candidate set I(Sj)(lines 7-8). The of\ufb02ine\nevaluation procedure can be treated as a rerank procedure of\nthe candidate set by iteratively selecting an item w.r.t. the\naction generated by the Actor network in DRR framework.\nMoreover, the model parameters are not updated in the of\ufb02ine\nevaluation.\nAlgorithm 2: Of\ufb02ine Evaluation Algorithm of DRR\nFramework\ninput : state window size nand reward function R\n1Observe the initial state s0and item setIaccording to\nthe of\ufb02ine log\n2for t = 1, T do\n3 Observe current state st={i1,...,in}\n4 Execute action at=\u03c0\u03b8(st)according to the current\npolicy\n5 Observe the recommended item itaccording to\nactionatby Eq. (2)\n6 Get rewardrt=R(st,at)from the feedback located\nin the users\u2019 log by Eq. (10)\n7 Update to a new state st+1=f(Ht+1), where\nHt+1={i2,...,in,it}ifrtis positive, otherwise,\nHt+1=Ht\n8 removeitfromI\n2) Online evaluation with environment simulator: As afore-\nmentioned that it is risky and costly to directly deploy the\nRL based models on recommender systems. Therefore, we\nconduct online evaluation with an environment simulator. In\nthis paper, we pretrain a PMF [37] model as the environmentsimulator, i.e., to predict an item\u2019s feedback that the user\nnever rates before. The online evaluation procedure follows\nAlgorithm 1, i.e., the parameters continuously update dur-\ning the online evaluation stage. Its major difference from\nAlgorithm 1 is that the feedback of a recommended item\nis observed by the environment simulator. Moreover, before\neach recommendation session starting in the simulated online\nevaluation, we reset the parameters back to \u03b8and\u03c9which is\nthe policy learned in the training stage for a fair comparison.\nV. E XPERIMENT\nA. Datasets and Evaluation Metrics\nWe adopt the following publicly available datasets from the\nreal world to conduct the experiments:\n\u2022MovieLens (100k)4. A benchmark dataset comprises\nof 0.1 million ratings from users to the recommended\nmovies on MovieLens website.\n\u2022Yahoo! Music (R3)5. This dataset contains over 0.36\nmillion ratings of songs collected from two different\nsources. The \ufb01rst source consists of ratings provided\nby users during normal interactions with Yahoo! Music\nservices. The second source consists of ratings of ran-\ndomly selected songs collected during an online survey\nby Yahoo! Research. We normalize the ratings to discrete\nvalues from 1 to 5.\n\u2022MovieLens (1M)6. A benchmark dataset includes of 1\nmillion ratings from the MovieLens website.\n\u2022Jester (2)7.", "start_char_idx": 0, "end_char_idx": 3924, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d59eff3-58a2-44e1-9bd5-a7d079a903f2": {"__data__": {"id_": "3d59eff3-58a2-44e1-9bd5-a7d079a903f2", "embedding": null, "metadata": {"page_label": "7", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e808915-a91a-401f-9426-85e142367b6d", "node_type": "4", "metadata": {"page_label": "7", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "eea81e1e9f2e88478336284cb0136bd67a24e386705be163a1370649765bfe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16110f8e-4c4b-40f8-88e9-805e312cee13", "node_type": "1", "metadata": {"page_label": "7", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "8c45e097a0c80aa1488c7c0558d5909827049c7dd835aecccf3f9bbed47f39a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5149e052-6bcf-42a1-9f91-89547425aa39", "node_type": "1", "metadata": {}, "hash": "e55ce25540629bd848b2824b847f83e7ac931d75de0dd5f77da030ce7ef19bbc", "class_name": "RelatedNodeInfo"}}, "text": "V. E XPERIMENT\nA. Datasets and Evaluation Metrics\nWe adopt the following publicly available datasets from the\nreal world to conduct the experiments:\n\u2022MovieLens (100k)4. A benchmark dataset comprises\nof 0.1 million ratings from users to the recommended\nmovies on MovieLens website.\n\u2022Yahoo! Music (R3)5. This dataset contains over 0.36\nmillion ratings of songs collected from two different\nsources. The \ufb01rst source consists of ratings provided\nby users during normal interactions with Yahoo! Music\nservices. The second source consists of ratings of ran-\ndomly selected songs collected during an online survey\nby Yahoo! Research. We normalize the ratings to discrete\nvalues from 1 to 5.\n\u2022MovieLens (1M)6. A benchmark dataset includes of 1\nmillion ratings from the MovieLens website.\n\u2022Jester (2)7. This dataset contains over 1.7 million real-\nvalue ratings (-10.0 to +10.0) over jokes in an online joke\nrecommender system.\nNote that except for Jester, the ratings in the other datasets\nare discrete values from 1 to 5, and the statistic information\nof the datasets is given in Table I. The MovieLens (100k) and\nMovieLens (1M) are abbreviated as ML (100k) and ML (1M)\nrespectively.\nTABLE I\nSTATISTIC INFORMATION OF THE DATASETS\nML (100k) Yahoo! Music ML (1M) Jester\n# user 943 15,400 6,040 63,978\n# item 1,682 1,000 3,952 150\n# ratings 100,000 365,740 1,000,209 1,761,439\nWe conduct both of\ufb02ine and simulated online evaluation\non these four datasets. For the of\ufb02ine evaluation, we utilize\nPrecision@k and NDCG@k as the metrics to measure the\nperformance of the proposed models. For the simulated online\nevaluation, we leverage the total accumulated rewards as the\nmetric.\n4https://grouplens.org/datasets/movielens/100k/\n5https://webscope.sandbox.yahoo.com/\n6https://grouplens.org/datasets/movielens/1m/\n7http://eigentaste.berkeley.edu/dataset/", "start_char_idx": 3131, "end_char_idx": 4969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5149e052-6bcf-42a1-9f91-89547425aa39": {"__data__": {"id_": "5149e052-6bcf-42a1-9f91-89547425aa39", "embedding": null, "metadata": {"page_label": "8", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55895482-15b6-4f3d-9c92-928d46da84f8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "e84c9aff7482e965fae1a7e9ee76ae572f56e0f0e86d48fd7af11e9e85a0ba18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d59eff3-58a2-44e1-9bd5-a7d079a903f2", "node_type": "1", "metadata": {"page_label": "7", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "71cef1a97a94c79c4be26654b3667025aa3fc61fa86bed28f29d2581f3cb8419", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1728b9a4-3f46-452a-b1fe-0016b3240982", "node_type": "1", "metadata": {}, "hash": "1d2bbdc6da96c9e84eee6aa0f86726debb603e70c4a484b282cd0a8e75353e4d", "class_name": "RelatedNodeInfo"}}, "text": "B. Compared Methods\nWe compare the proposed methods with some representative\nbaseline methods. For the of\ufb02ine evaluation, we compare to\nconventional methods including Popularity, PMF [37] and\nSVD++ [38], and a RL based method DRR-n. Moreover, the\nonline evaluation baselines contain the state-of-the-art multi-\narmed bandits methods LinUCB [39] and HLinUCB [40] and\nthe DRR-n as well.\n\u2022Popularity recommends the most popular item, i.e., the\nitem with the highest average rating or the items with\nlargest number of positive ratings8from current available\nitems to the users at each timestep.\n\u2022PMF makes a matrix decomposition as SVD, while it\nonly takes into account the non zero elements.\n\u2022SVD++ mixes strengths of the latent model as well as\nthe neighborhood model.\n\u2022LinUCB selects an arm (item) according to the estimated\nupper con\ufb01dence bound of the potential reward.\n\u2022HLinUCB further learns hidden features for each arm to\nmodel the potential reward.\n\u2022DRR-n simply utilizes the concatenation of the item\nembeddings to represent user state, which is widely\nused in previous studies. Although it is under the DRR\nframework, we treat this method as a baseline to assess\nthe effectiveness of our proposed state representation\nmodule.\nC. Experimental Settings\nFor each dataset, we choose 80% of the interactions in\neach user session as the training set, and leave the rest as the\ntesting set. Moreover, for MovieLens (100k), Yahoo! Music\nand MovieLens (1M), the positive ratings are 4and5, while for\nJester, the positive ones are those higher than 0. The number\nof latest positively rated items n, which is empirically set to 5.\nWe perform PMF to pretrain the 100-dimensional embeddings\nof the users and items. Moreover, in each episode, we do not\nrecommend repeated items, i.e., we remove the ones already\nrecommended from the candidate set. The discount rate \u03b3is\n0.9. We utilize Adam optimizer for all the RL based methods\nwithL2-norm regularization to prevent over\ufb01tting. As for the\nreward function, we empirically normalize the ratings into\nrange [-1 ,1] and utilize the normalized ones as the feedback of\nthe corresponding recommendations. For instance, in timestep\nt, the recommender agent recommends an item jto useri,\n(denoted as action ain states), and the rating ratei,jcomes\nfrom the interaction logs if user iactually rates item j, or\nfrom a predicted value by the simulator otherwise. Therefore,\nthe reward function can be de\ufb01ned as follows:\nR(s,a) =1\n2(ratei,j\u22123)\nR(s,a) =ratei,j/10(10)\n8To get a better result of popularity based recommendation, we both test\nthe two strategies, and choose the best one to report.where the \ufb01rst setting is for MovieLens (100k), Yahoo! Music\nand MovieLens (1M), and the second one is for Jester. All the\nbaseline methods are carefully tuned for a fair comparison.\nWe model the recommendation procedure as an interaction\nepisode with length T, and the hyper-parameter Tis tuned\nfor different datasets (detailed in Section V .E).\nD. Results and Analysis\n1) Of\ufb02ine Evaluation Results and Analysis: The of\ufb02ine\nevaluation results are summarized from Table II to Table V\nrespectively, where the best results are marked in bold type.\nIn the of\ufb02ine evaluation, we compare the proposed methods\nto some representative of\ufb02ine learning methods. The results\nsuggest that the proposed methods under the DRR framework\noutperform the baselines on most of datasets, which demon-\nstrates the effectiveness of our proposed methods.\nSpeci\ufb01cally, as aforementioned that, we propose three dif-\nferent network structure in the state representation module to\nmodel the explicit interactions of the users and items under the\nDRR framework, which are the DRR-p, DRR-u and DRR-\nave. From the results in Table II to Table V, we \ufb01nd that\nthe three methods all outperform the baselines in most cases.\nMoreover, DRR-n that simply concats the item embeddings\nto represent the state s, performs worse than the proposed\nDRR-p, DRR-u and DRR-ave.", "start_char_idx": 0, "end_char_idx": 3957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1728b9a4-3f46-452a-b1fe-0016b3240982": {"__data__": {"id_": "1728b9a4-3f46-452a-b1fe-0016b3240982", "embedding": null, "metadata": {"page_label": "8", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55895482-15b6-4f3d-9c92-928d46da84f8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "e84c9aff7482e965fae1a7e9ee76ae572f56e0f0e86d48fd7af11e9e85a0ba18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5149e052-6bcf-42a1-9f91-89547425aa39", "node_type": "1", "metadata": {"page_label": "8", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "55539d7372d8678cf1acbf8ba60b78fc9959abe9bf746cddfe89251a2db1dd73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "615ed89c-9c89-4897-85d8-22b7544afeb0", "node_type": "1", "metadata": {}, "hash": "c1e917d8ee642ef86a4d6d98911cca7501cee22909b37c83aaac81d4a9cd8ca0", "class_name": "RelatedNodeInfo"}}, "text": "In the of\ufb02ine evaluation, we compare the proposed methods\nto some representative of\ufb02ine learning methods. The results\nsuggest that the proposed methods under the DRR framework\noutperform the baselines on most of datasets, which demon-\nstrates the effectiveness of our proposed methods.\nSpeci\ufb01cally, as aforementioned that, we propose three dif-\nferent network structure in the state representation module to\nmodel the explicit interactions of the users and items under the\nDRR framework, which are the DRR-p, DRR-u and DRR-\nave. From the results in Table II to Table V, we \ufb01nd that\nthe three methods all outperform the baselines in most cases.\nMoreover, DRR-n that simply concats the item embeddings\nto represent the state s, performs worse than the proposed\nDRR-p, DRR-u and DRR-ave. From the observations, we\ncan conclude in two folds: (i) the proposed methods indeed\nhave the capability of long-term scheduling and dynamic\nadaptation, which are ignored by conventional methods; (ii)\nthe proposed state representation module well captures the\ndynamic interactions between the users and items, and the\nstate should not be simply concatenate with fully connected\nlayers as DRR-n does, which may result in information loss.\nCompared with DRR-p, DRR-u and DRR-ave, we can see\nthat DRR-ave outperforms DRR-u, and DRR-u is superior than\nDRR-p on the four datasets in most cases. The reasons are as\nfollows: 1) The DRR-u method has better performance than\nDRR-p, because DRR-u only captures the interactions of user\u2019s\nhistorical items, but also seizes the personalization information\nthrough the user-item interactions. 2) DRR-ave performs the\nbest, because of two reasons: (i) DRR-ave method captures\nthe personalization information through user-item interactions;\n(ii) as noted in Section IV , by using the average pooling, it\neliminates the position effects in H.\nTABLE II\nPERFORMANCE COMPARISON OF ALL METHODS ON ML (100 K)DATASET .\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\nPopularity 0.6933 0.6012 0.9104 0.9008\nPMF 0.6988 0.6194 0.9095 0.8968\nSVD++ 0.7034 0.6255 0.9125 0.8991\nDRR-n 0.7185 0.6387 0.9147 0.9004\nDRR-p 0.7263 0.6448 0.9076 0.9015\nDRR-u 0.7417 0.6536 0.9183 0.9062\nDRR-ave 0.7887 0.6935 0.9255 0.9046", "start_char_idx": 3173, "end_char_idx": 5393, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "615ed89c-9c89-4897-85d8-22b7544afeb0": {"__data__": {"id_": "615ed89c-9c89-4897-85d8-22b7544afeb0", "embedding": null, "metadata": {"page_label": "9", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a21267d-a1df-454a-9db7-2aeb8817caf9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "df10b9302e36206c8c95faa8210f5edfc58c7e51d1a4647fe0587da8ac86e462", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1728b9a4-3f46-452a-b1fe-0016b3240982", "node_type": "1", "metadata": {"page_label": "8", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "bccbf10e0d162ef6d50645cd7d4652f3ac0f260e923526b9950d485740ac1326", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44e1133d-f199-4015-96ff-d8aaefba53a9", "node_type": "1", "metadata": {}, "hash": "c592841dc2002630cfcb08e69827df3c1bf15a30453adf3b44983ff61c676ff9", "class_name": "RelatedNodeInfo"}}, "text": "TABLE III\nPERFORMANCE COMPARISON OF ALL METHODS ON YAHOO ! M USIC\nDATASET .\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\nPopularity 0.3826 0.3805 0.8870 0.8811\nPMF 0.3835 0.3817 0.8837 0.8802\nSVD++ 0.3857 0.3821 0.8887 0.8813\nDRR-n 0.3844 0.3819 0.8876 0.8810\nDRR-p 0.3850 0.3822 0.8883 0.8815\nDRR-u 0.3864 0.3827 0.8889 0.8819\nDRR-ave 0.3917 0.3839 0.9004 0.8949\nTABLE IV\nPERFORMANCE COMPARISON OF ALL METHODS ON ML (1M) DATASET .\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\nPopularity 0.7141 0.6181 0.8906 0.8738\nPMF 0.7072 0.6193 0.8901 0.8746\nSVD++ 0.7142 0.6258 0.9009 0.8776\nDRR-n 0.7151 0.6221 0.8902 0.8751\nDRR-p 0.7346 0.6366 0.8909 0.8753\nDRR-u 0.7375 0.6385 0.8912 0.8763\nDRR-ave 0.7693 0.6594 0.9112 0.8980\n2) Simulated online evaluation results and analysis: The\nresults of the simulated online evaluation are summarized in\nTable VI, where the best results are marked in bold type. In the\nexperiment, we only compare with the baseline methods that\ncan perform online learning, which are LinUCB, HLinUCB\nand DRR-n. Again, we \ufb01nd that the proposed methods deliver\nhigher rewards than all the baselines.\nOn the one hand, the fact suggests that the proposed\nRL-based methods model dynamic adaptation and long-term\nrewards better than the multi-armed bandits based methods\nLinUCB and HLinUCB. On the other hand, the observation\nindicates that the proposed state representation structures are\nsuperior to the naive full-connected network in DRR-n. Again,\nwe observe that DRR-ave performs the best among all the three\nproposed interaction modeling structures.\nE. Parameter Study\nIn this subsection, we investigate how the episode length\nTaffect the performance of proposed methods. Figure 7\nshows the results9. From the left part of Figure 7, we observe\nthat the performance on MovieLens \ufb01rst increases and then\ndecreases as the length of the episode is gradually increased,\nand the summit appears at T= 10 . A similar tend can be\nfound for the Yahoo! Music from the right part of Figure 7,\nwhere the performance peaks at T= 20 . The reason may\ndue to the trade-off between the exploitation and exploration.\nWhen the episode length is small, the user can not fully\ninteract with the recommender agent, i.e., the exploration is\ninsuf\ufb01cient. As we enlarge the episodes, the recommender\n9Due to the space limit, We only present the performance of DRR-ave,\nwhile DRR-p and DRR-u have similar observationsTABLE V\nPERFORMANCE COMPARISON OF ALL METHODS ON JESTER DATASET .", "start_char_idx": 0, "end_char_idx": 2474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44e1133d-f199-4015-96ff-d8aaefba53a9": {"__data__": {"id_": "44e1133d-f199-4015-96ff-d8aaefba53a9", "embedding": null, "metadata": {"page_label": "9", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a21267d-a1df-454a-9db7-2aeb8817caf9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "df10b9302e36206c8c95faa8210f5edfc58c7e51d1a4647fe0587da8ac86e462", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "615ed89c-9c89-4897-85d8-22b7544afeb0", "node_type": "1", "metadata": {"page_label": "9", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "1c86156d74e6f1db22eb413dd91d5a252d2b4fe8a27c01972a35a5fa991ffb02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1014bc8-6517-483e-a40d-d8db716344dd", "node_type": "1", "metadata": {}, "hash": "fdc8ef9eef2c34292d372eee214632ada00a1dddb1b5e58f7353e97f73a3cc39", "class_name": "RelatedNodeInfo"}}, "text": "Figure 7\nshows the results9. From the left part of Figure 7, we observe\nthat the performance on MovieLens \ufb01rst increases and then\ndecreases as the length of the episode is gradually increased,\nand the summit appears at T= 10 . A similar tend can be\nfound for the Yahoo! Music from the right part of Figure 7,\nwhere the performance peaks at T= 20 . The reason may\ndue to the trade-off between the exploitation and exploration.\nWhen the episode length is small, the user can not fully\ninteract with the recommender agent, i.e., the exploration is\ninsuf\ufb01cient. As we enlarge the episodes, the recommender\n9Due to the space limit, We only present the performance of DRR-ave,\nwhile DRR-p and DRR-u have similar observationsTABLE V\nPERFORMANCE COMPARISON OF ALL METHODS ON JESTER DATASET .\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\nPopularity 0.6167 0.6012 0.8932 0.8703\nPMF 0.6171 0.6015 0.8740 0.8676\nSVD++ 0.6184 0.6027 0.8819 0.8614\nDRR-n 0.6178 0.6021 0.8915 0.8724\nDRR-p 0.6181 0.6029 0.8934 0.8753\nDRR-u 0.6217 0.6043 0.8974 0.8805\nDRR-ave 0.6278 0.6076 0.9124 0.9079\nTABLE VI\nTHE REWARDS OF ALL METHODS ON THE FOUR DATASETS .\nModel ML (100k) Yahoo! Music ML (1M) Jester\nLinUCB 1,958 30,462.5 30,174 141,358.4\nHLinUCB 1,475 32,725 32,785.5 147,105.5\nDRR-n 2,654.5 35,382.5 35,860 165,844.5\nDRR-p 2,832 37,328.5 36,653 177,414.2\nDRR-u 2,869 42,174.5 37,615 183,517.6\nDRR-ave 3,251.5 49,095 40,588 194,860.7\nagent can explore (interact with users) adequately, i.e., the\nrecommender agent captures the user\u2019s preference, so that the\nperformance improves. However, if the episodes are too large,\nthe recommender focuses on exploiting locally, but the user\npreferred items is limited, therefore the performance declines\nas we do not recommend repeated items to user. Hence, we\nshould nicely trade off the exploration and exploitation by\nsetting a suitable value for T.\nFig. 7. Parameter study on episode length Tin MovieLens and Yahoo!Music\ndatasets\nF . Case Study\nIn this subsection, we present an example to show the\ndifferent recommendation manner between LinUCB and DRR-\nave on MovieLens dataset. Speci\ufb01cally, we randomly pick up\na user with ID 11, and conduct the recommendation procedure\nwith LinUCB and DRR-ave respectively. To verify the reaction\nto the same recommendation scenario, we \ufb01x the \ufb01rst three\nrecommended items and to see what will happen next. The\nresults of recommended item and the reward are reported in\nTable VII.\nFrom Table VII, we can see that LinUCB and DRR-ave\nreact differently when given two consecutive negative recom-\nmendations (Eraser and First Knight). Speci\ufb01cally, LinUCB", "start_char_idx": 1691, "end_char_idx": 4299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1014bc8-6517-483e-a40d-d8db716344dd": {"__data__": {"id_": "b1014bc8-6517-483e-a40d-d8db716344dd", "embedding": null, "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b2ecffe-f05f-44d6-b925-5d5859e8c5c5", "node_type": "4", "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "e6b723190cd415c1227db50b89a25f4cde4f1f133b0a87a591abdf779d16e7fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44e1133d-f199-4015-96ff-d8aaefba53a9", "node_type": "1", "metadata": {"page_label": "9", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "78c6a24a87f124164b245baef55dd5823acf9df89935972519aaeb3ad6bca183", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25cd5565-e41b-4650-8319-f2994a9f3da6", "node_type": "1", "metadata": {}, "hash": "c2a7c80010401a7f7f644ab0cc3938cf20933592641a2d47b3590c22cd9f9190", "class_name": "RelatedNodeInfo"}}, "text": "keeps exploring without considering to recommend a \u201csafe\u201d\nitem to please the user. However, DRR-ave stops exploration\nand recommends a risk-free movie Dead Man Walking, which\nbelongs to the same genre as Chasing Amy that has gained a\npositive feedback from the user at timestep 1. The observation\ndemonstrates the superiority of the proposed DRR-ave against\nLinUCB.\nTABLE VII\nDIFFERENT RECOMMENDATION MANNER BETWEEN LINUCB AND\nDRR- AVE ON MOVIE LENS. (T HE VALUE IN (\u00b7)DENOTES THE\nCORRESPONDING REWARD .)\ntimestep LinUCB DRR-ave\n1 Chasing Amy (1) Chasing Amy (1)\n2 Eraser (-0.5) Eraser (-0.5)\n3 First Knight (-1) First Knight (-1)\n4 The Deer Hunter (-0.5) Dead Man Walking (1)\n5 Event Horizon (-1) Braveheart (0.5)\n6 The Net (0) The Usual Suspect (-0.5)\n7 Striptease (-0.5) Psycho (0.5)\nVI. C ONCLUSION\nIn this paper, we propose a deep reinforcement learning\nbased framework DRR to perform the recommendation task.\nUnlike the conventional studies, DRR treats the recommen-\ndation as a sequential decision making process and adopts\nan \u201cActor-Critic\u201d learning scheme, which can take both the\nimmediate and long-term rewards into account. In DRR, a\nstate representation module is incorporated and three instanti-\nation structures are designed, which can explicitly model the\ninteractions between users and items. Extensive experiments\non four real-world datasets demonstrate the superiority of the\nproposed DRR method over state-of-the-art competitors.\nREFERENCES\n[1] R. J. Mooney and L. Roy, \u201cContent-based book recommending using\nlearning for text categorization,\u201d in ACM DL , 2000, pp. 195\u2013204.\n[2] M. Deshpande and G. Karypis, \u201cItem-based top- Nrecommendation\nalgorithms,\u201d ACM Trans. Inf. Syst. , vol. 22, no. 1, pp. 143\u2013177, 2004.\n[3] Y . Koren, R. M. Bell, and C. V olinsky, \u201cMatrix factorization techniques\nfor recommender systems,\u201d IEEE Computer , vol. 42, no. 8, pp. 30\u201337,\n2009.\n[4] G. Linden, B. Smith, and J. York, \u201cAmazon.com recommendations:\nItem-to-item collaborative \ufb01ltering,\u201d IEEE Internet Computing , vol. 7,\nno. 1, pp. 76\u201380, 2003.\n[5] J. Wang, A. P. De Vries, and M. J. Reinders, \u201cUnifying user-based and\nitem-based collaborative \ufb01ltering approaches by similarity fusion,\u201d in\nSIGIR . ACM, 2006, pp. 501\u2013508.\n[6] H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady,\nL. Nie, T. Phillips, E. Davydov, D. Golovin, S. Chikkerur, D. Liu,\nM. Wattenberg, A. M. Hrafnkelsson, T. Boulos, and J. Kubica, \u201cAd\nclick prediction: a view from the trenches,\u201d in KDD 2013, Chicago, IL,\nUSA, August 11-14, 2013 , 2013, pp. 1222\u20131230.\n[7] S. Rendle, \u201cFactorization machines,\u201d in ICDM, Sydney, Australia, 14-17\nDecember 2010 , 2010, pp. 995\u20131000.\n[8] Y . Juan, Y . Zhuang, W. Chin, and C. Lin, \u201cField-aware factorization\nmachines for CTR prediction,\u201d in RecSys, Boston, MA, USA, September\n15-19, 2016 , 2016, pp. 43\u201350.\n[9] W. Zhang, T. Du, and J. Wang, \u201cDeep learning over multi-\ufb01eld categor-\nical data - - A case study on user response prediction,\u201d in ECIR 2016,\nPadua, Italy, March 20-23, 2016. Proceedings , 2016, pp. 45\u201357.[10] Y . Qu, H. Cai, K. Ren, W. Zhang, Y .", "start_char_idx": 0, "end_char_idx": 3080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25cd5565-e41b-4650-8319-f2994a9f3da6": {"__data__": {"id_": "25cd5565-e41b-4650-8319-f2994a9f3da6", "embedding": null, "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b2ecffe-f05f-44d6-b925-5d5859e8c5c5", "node_type": "4", "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "e6b723190cd415c1227db50b89a25f4cde4f1f133b0a87a591abdf779d16e7fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1014bc8-6517-483e-a40d-d8db716344dd", "node_type": "1", "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "4953f4162d419dab335ecb0b36eba2cdc328a53ab42617b62fb811b15f282d2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35f5d646-2d01-4b52-a693-0a75fb7e7447", "node_type": "1", "metadata": {}, "hash": "95a105f26b958164676e18744c55bf531454703aa40abc7ed9e72624ae8e8dcc", "class_name": "RelatedNodeInfo"}}, "text": "995\u20131000.\n[8] Y . Juan, Y . Zhuang, W. Chin, and C. Lin, \u201cField-aware factorization\nmachines for CTR prediction,\u201d in RecSys, Boston, MA, USA, September\n15-19, 2016 , 2016, pp. 43\u201350.\n[9] W. Zhang, T. Du, and J. Wang, \u201cDeep learning over multi-\ufb01eld categor-\nical data - - A case study on user response prediction,\u201d in ECIR 2016,\nPadua, Italy, March 20-23, 2016. Proceedings , 2016, pp. 45\u201357.[10] Y . Qu, H. Cai, K. Ren, W. Zhang, Y . Yu, Y . Wen, and J. Wang, \u201cProduct-\nbased neural networks for user response prediction,\u201d in ICDM 2016,\nDecember 12-15, 2016, Barcelona, Spain , 2016, pp. 1149\u20131154.\n[11] H. Guo, R. Tang, Y . Ye, Z. Li, and X. He, \u201cDeepfm: A factorization-\nmachine based neural network for CTR prediction,\u201d in IJCAI 2017,\nMelbourne, Australia, August 19-25, 2017 , 2017, pp. 1725\u20131731.\n[12] H. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye,\nG. Anderson, G. Corrado, W. Chai, M. Ispir, R. Anil, Z. Haque, L. Hong,\nV . Jain, X. Liu, and H. Shah, \u201cWide & deep learning for recommender\nsystems,\u201d CoRR , vol. abs/1606.07792, 2016.\n[13] O. Chapelle and L. Li, \u201cAn empirical evaluation of thompson sampling,\u201d\ninNIPS, Granada, Spain. , 2011, pp. 2249\u20132257.\n[14] L. Li, W. Chu, J. Langford, and R. E. Schapire, \u201cA contextual-bandit\napproach to personalized news article recommendation,\u201d in WWW 2010,\nRaleigh, North Carolina, USA, April 26-30, 2010 , 2010, pp. 661\u2013670.\n[15] H. Wang, Q. Wu, and H. Wang, \u201cFactorization bandits for interactive\nrecommendation,\u201d in AAAI, February 4-9, 2017, San Francisco, Cali-\nfornia, USA. , 2017, pp. 2695\u20132702.\n[16] C. Zeng, Q. Wang, S. Mokhtari, and T. Li, \u201cOnline context-aware\nrecommendation with time varying multi-armed bandit,\u201d in SIGKDD\n, San Francisco, CA, USA, August 13-17, 2016 , 2016, pp. 2025\u20132034.\n[17] X. Zhao, W. Zhang, and J. Wang, \u201cInteractive collaborative \ufb01ltering,\u201d in\nCIKM\u201913, San Francisco, CA, USA, October 27 - November 1, 2013 ,\n2013, pp. 1411\u20131420.\n[18] G. Shani, D. Heckerman, and R. I. Brafman, \u201cAn mdp-based recom-\nmender system,\u201d Journal of Machine Learning Research , vol. 6, pp.\n1265\u20131295, 2005.\n[19] G. Zheng, F. Zhang, Z. Zheng, Y . Xiang, N. J. Yuan, X. Xie, and\nZ. Li, \u201cDRN: A deep reinforcement learning framework for news\nrecommendation,\u201d in WWW 2018, Lyon, France, April 23-27, 2018 ,\n2018, pp. 167\u2013176.\n[20] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction .\nMIT press Cambridge, 1998, vol. 1, no. 1.\n[21] V .", "start_char_idx": 2647, "end_char_idx": 5074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35f5d646-2d01-4b52-a693-0a75fb7e7447": {"__data__": {"id_": "35f5d646-2d01-4b52-a693-0a75fb7e7447", "embedding": null, "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b2ecffe-f05f-44d6-b925-5d5859e8c5c5", "node_type": "4", "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "e6b723190cd415c1227db50b89a25f4cde4f1f133b0a87a591abdf779d16e7fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25cd5565-e41b-4650-8319-f2994a9f3da6", "node_type": "1", "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "fec062726bf7f5ec089a467e4440fd5c3c94f41a0b279a51aa9db6c8d78edb29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01832bf9-6952-4ee2-8678-cc17d0361b0f", "node_type": "1", "metadata": {}, "hash": "0400141bfccdc6162c808a687ec60bcf31cbd6a1445f53d100a0bb1c91216b4c", "class_name": "RelatedNodeInfo"}}, "text": "1411\u20131420.\n[18] G. Shani, D. Heckerman, and R. I. Brafman, \u201cAn mdp-based recom-\nmender system,\u201d Journal of Machine Learning Research , vol. 6, pp.\n1265\u20131295, 2005.\n[19] G. Zheng, F. Zhang, Z. Zheng, Y . Xiang, N. J. Yuan, X. Xie, and\nZ. Li, \u201cDRN: A deep reinforcement learning framework for news\nrecommendation,\u201d in WWW 2018, Lyon, France, April 23-27, 2018 ,\n2018, pp. 167\u2013176.\n[20] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction .\nMIT press Cambridge, 1998, vol. 1, no. 1.\n[21] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. A. Riedmiller, A. Fidjeland, G. Ostrovski,\nS. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\nD. Wierstra, S. Legg, and D. Hassabis, \u201cHuman-level control through\ndeep reinforcement learning,\u201d Nature , vol. 518, no. 7540, pp. 529\u2013533,\n2015.\n[22] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den\nDriessche, J. Schrittwieser, I. Antonoglou, V . Panneershelvam, M. Lanc-\ntot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever,\nT. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis,\n\u201cMastering the game of go with deep neural networks and tree search,\u201d\nNature , vol. 529, no. 7587, pp. 484\u2013489, 2016.\n[23] H. Cai, K. Ren, W. Zhang, K. Malialis, J. Wang, Y . Yu, and D. Guo,\n\u201cReal-time bidding by reinforcement learning in display advertising,\u201d in\nWSDM 2017, Cambridge, United Kingdom, February 6-10, 2017 , 2017,\npp. 661\u2013670.\n[24] J. Jin, C. Song, H. Li, K. Gai, J. Wang, and W. Zhang, \u201cReal-time\nbidding with multi-agent reinforcement learning in display advertising,\u201d\nCoRR , vol. abs/1802.09756, 2018.\n[25] H. Cai, T. Chen, W. Zhang, Y . Yu, and J. Wang, \u201cEf\ufb01cient architecture\nsearch by network transformation,\u201d in AAAI , New Orleans, Louisiana,\nUSA, February 2-7, 2018 , 2018.\n[26] B. Zoph and Q. V . Le, \u201cNeural architecture search with reinforcement\nlearning,\u201d CoRR , vol. abs/1611.01578, 2016.\n[27] N. Taghipour and A. A. Kardan, \u201cA hybrid web recommender system\nbased on q-learning,\u201d in Proceedings of the 2008 ACM Symposium on\nApplied Computing (SAC), Fortaleza, Ceara, Brazil, March 16-20, 2008 ,\n2008, pp. 1164\u20131168.\n[28] X. Zhao, L. Zhang, Z. Ding, D. Yin, Y . Zhao, and J. Tang, \u201cDeep\nreinforcement learning for list-wise recommendations,\u201d CoRR , vol.\nabs/1801.00209, 2018.\n[29] X. Zhao, L. Zhang, Z. Ding, L. Xia, J. Tang, and D. Yin, \u201cRecommenda-\ntions with negative feedback via pairwise deep reinforcement learning,\u201d\nCoRR , vol. abs/1802.06501, 2018.\n[30] L. Xia, J. Xu, Y .", "start_char_idx": 4568, "end_char_idx": 7115, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01832bf9-6952-4ee2-8678-cc17d0361b0f": {"__data__": {"id_": "01832bf9-6952-4ee2-8678-cc17d0361b0f", "embedding": null, "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b2ecffe-f05f-44d6-b925-5d5859e8c5c5", "node_type": "4", "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "e6b723190cd415c1227db50b89a25f4cde4f1f133b0a87a591abdf779d16e7fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35f5d646-2d01-4b52-a693-0a75fb7e7447", "node_type": "1", "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "413ae17bf5cddce0cad74f9f175c528e338a7a4431caa08174be9339b163f13e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d772b57-c0ed-4383-a790-2c58504364ca", "node_type": "1", "metadata": {}, "hash": "d54f5cf36218bbadf08a2c0de4b3f4018472a65e1ae9e15f8016e5b255e1808a", "class_name": "RelatedNodeInfo"}}, "text": "1164\u20131168.\n[28] X. Zhao, L. Zhang, Z. Ding, D. Yin, Y . Zhao, and J. Tang, \u201cDeep\nreinforcement learning for list-wise recommendations,\u201d CoRR , vol.\nabs/1801.00209, 2018.\n[29] X. Zhao, L. Zhang, Z. Ding, L. Xia, J. Tang, and D. Yin, \u201cRecommenda-\ntions with negative feedback via pairwise deep reinforcement learning,\u201d\nCoRR , vol. abs/1802.06501, 2018.\n[30] L. Xia, J. Xu, Y . Lan, J. Guo, W. Zeng, and X. Cheng, \u201cAdapting markov\ndecision process for search result diversi\ufb01cation,\u201d in SIGIR , Shinjuku,\nTokyo, Japan, August 7-11, 2017 , 2017, pp. 535\u2013544.\n[31] Z. Wei, J. Xu, Y . Lan, J. Guo, and X. Cheng, \u201cReinforcement learning to\nrank with markov decision process,\u201d in SIGIR , Shinjuku, Tokyo, Japan,\nAugust 7-11, 2017 , 2017, pp. 945\u2013948.", "start_char_idx": 6741, "end_char_idx": 7482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d772b57-c0ed-4383-a790-2c58504364ca": {"__data__": {"id_": "5d772b57-c0ed-4383-a790-2c58504364ca", "embedding": null, "metadata": {"page_label": "11", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61ce1169-9e3b-4471-81f2-2ff328b548ac", "node_type": "4", "metadata": {"page_label": "11", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "f19a816d573bc89f741189359bb31b57eaceadca308df586b2f0c3e1e622662d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01832bf9-6952-4ee2-8678-cc17d0361b0f", "node_type": "1", "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}, "hash": "efb43e03cbbcf2e5490c6812045d85700fcf5438efb55c054ea120897b05e9c1", "class_name": "RelatedNodeInfo"}}, "text": "[32] Y . Hu, Q. Da, A. Zeng, Y . Yu, and Y . Xu, \u201cReinforcement learning\nto rank in e-commerce search engine: Formalization, analysis, and\napplication,\u201d CoRR , vol. abs/1803.00710, 2018.\n[33] G. Dulac-Arnold, R. Evans, P. Sunehag, and B. Coppin, \u201cReinforcement\nlearning in large discrete action spaces,\u201d CoRR , vol. abs/1512.07679,\n2015.\n[34] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. A.\nRiedmiller, \u201cDeterministic policy gradient algorithms,\u201d in ICML 2014,\nBeijing, China, 21-26 June 2014 , 2014, pp. 387\u2013395.\n[35] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa,\nD. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement\nlearning,\u201d CoRR , vol. abs/1509.02971, 2015.\n[36] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, \u201cPrioritized experience\nreplay,\u201d arXiv preprint arXiv:1511.05952 , 2015.\n[37] A. Mnih and R. R. Salakhutdinov, \u201cProbabilistic matrix factorization,\u201d\ninNIPS , 2008, pp. 1257\u20131264.\n[38] Y . Koren, \u201cFactorization meets the neighborhood: a multifaceted collab-\norative \ufb01ltering model,\u201d in KDD . ACM, 2008, pp. 426\u2013434.\n[39] L. Li, W. Chu, J. Langford, and R. E. Schapire, \u201cA contextual-bandit\napproach to personalized news article recommendation,\u201d in WWW .\nACM, 2010, pp. 661\u2013670.\n[40] H. Wang, Q. Wu, and H. Wang, \u201cLearning hidden features for contextual\nbandits,\u201d in CIKM . ACM, 2016, pp. 1633\u20131642.", "start_char_idx": 0, "end_char_idx": 1376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"7db136dc-ee57-4a77-9846-8065af8ba6a6": {"doc_hash": "c8f1c4c1930094a370b441a463cb674a4213eadc5833d6149a253f7ddff1e5ff", "ref_doc_id": "0e841420-0ab3-4331-8b62-49fd7d387b45"}, "00391c6e-7a4e-4478-a9d1-b282ad7ee62c": {"doc_hash": "714e21722fe4ee24f29be5d24bf63cb47377e397382a24ca7518b2d23c2a24df", "ref_doc_id": "0e841420-0ab3-4331-8b62-49fd7d387b45"}, "16abbe40-c1b8-4a71-a5c8-275ec5ea1bf7": {"doc_hash": "e30d01d56e39cc698e7d86cca0c7d1b0f8748f6ac8a4e47dd2b99917d81d35d6", "ref_doc_id": "bd7e3a64-b63b-43d1-8db0-a8b2ab7cdb12"}, "d9a454c6-c2fb-4659-bc49-66b6c3c39676": {"doc_hash": "2265db8607ef113144dca3779962de10b3be4958db2e4aa590a45822791db644", "ref_doc_id": "bd7e3a64-b63b-43d1-8db0-a8b2ab7cdb12"}, "b00d6fa4-a8db-46fb-bfdc-50dd0cb2cc06": {"doc_hash": "8166aaf6db86d098916292ca68ede3df9bd0b5b74e827694c2657d77d9b0e2dd", "ref_doc_id": "7cdc9ccc-7d69-4451-b304-6c9c58e7c82f"}, "8e5c0f3e-0971-47d5-8992-763233432c7a": {"doc_hash": "4c547c0198c891e1c8a21bcb29b9be646e760e6cf5a5959cb379aaa5c2b89a97", "ref_doc_id": "7cdc9ccc-7d69-4451-b304-6c9c58e7c82f"}, "2acf71c7-adad-4ee8-bce3-6f291a083a97": {"doc_hash": "cd3b1b3fce52d138d53f1498d0073e69c509090d293c2bf7f66667af7cd55c9f", "ref_doc_id": "6b95f758-0a02-4385-9648-ed9e5000ab71"}, "a2ef63db-7d27-4212-8388-d42ae717fff9": {"doc_hash": "dd35a8a840e07c4555eef625d9f85b49bb7d7e790f7f73df4c469554a2345f7c", "ref_doc_id": "6b95f758-0a02-4385-9648-ed9e5000ab71"}, "be2a97f7-8c32-4d37-9383-eb2ae00559ea": {"doc_hash": "380953ecc31e8e332419aefcdf5ba62ddc38b23b08d1822443617558faff3ed1", "ref_doc_id": "d4db6341-4067-472a-b3f4-3e28cda74469"}, "ba9c9a35-5657-4d59-bfc3-33559d06b4bb": {"doc_hash": "456bf675a4e25311b8fa02c196c7b60de5e33afc2f6e23e5510684ea0197beb1", "ref_doc_id": "d4db6341-4067-472a-b3f4-3e28cda74469"}, "60a1a0df-5e79-4431-875d-19e9013e0af2": {"doc_hash": "6844d372a5f9a02f236aafd1969b1983d585221ce5a4634500cbd8b6c7620de6", "ref_doc_id": "309b599c-d2a9-4b9f-84f5-d0a2f2b5d590"}, "97862008-8561-43b2-8aff-5ae8e8157387": {"doc_hash": "3f18143361742d3b62b52838797bcf494ed9132ca1eff6c979607a41bdba3f70", "ref_doc_id": "309b599c-d2a9-4b9f-84f5-d0a2f2b5d590"}, "16110f8e-4c4b-40f8-88e9-805e312cee13": {"doc_hash": "8c45e097a0c80aa1488c7c0558d5909827049c7dd835aecccf3f9bbed47f39a0", "ref_doc_id": "5e808915-a91a-401f-9426-85e142367b6d"}, "3d59eff3-58a2-44e1-9bd5-a7d079a903f2": {"doc_hash": "71cef1a97a94c79c4be26654b3667025aa3fc61fa86bed28f29d2581f3cb8419", "ref_doc_id": "5e808915-a91a-401f-9426-85e142367b6d"}, "5149e052-6bcf-42a1-9f91-89547425aa39": {"doc_hash": "55539d7372d8678cf1acbf8ba60b78fc9959abe9bf746cddfe89251a2db1dd73", "ref_doc_id": "55895482-15b6-4f3d-9c92-928d46da84f8"}, "1728b9a4-3f46-452a-b1fe-0016b3240982": {"doc_hash": "bccbf10e0d162ef6d50645cd7d4652f3ac0f260e923526b9950d485740ac1326", "ref_doc_id": "55895482-15b6-4f3d-9c92-928d46da84f8"}, "615ed89c-9c89-4897-85d8-22b7544afeb0": {"doc_hash": "1c86156d74e6f1db22eb413dd91d5a252d2b4fe8a27c01972a35a5fa991ffb02", "ref_doc_id": "0a21267d-a1df-454a-9db7-2aeb8817caf9"}, "44e1133d-f199-4015-96ff-d8aaefba53a9": {"doc_hash": "78c6a24a87f124164b245baef55dd5823acf9df89935972519aaeb3ad6bca183", "ref_doc_id": "0a21267d-a1df-454a-9db7-2aeb8817caf9"}, "b1014bc8-6517-483e-a40d-d8db716344dd": {"doc_hash": "4953f4162d419dab335ecb0b36eba2cdc328a53ab42617b62fb811b15f282d2d", "ref_doc_id": "9b2ecffe-f05f-44d6-b925-5d5859e8c5c5"}, "25cd5565-e41b-4650-8319-f2994a9f3da6": {"doc_hash": "fec062726bf7f5ec089a467e4440fd5c3c94f41a0b279a51aa9db6c8d78edb29", "ref_doc_id": "9b2ecffe-f05f-44d6-b925-5d5859e8c5c5"}, "35f5d646-2d01-4b52-a693-0a75fb7e7447": {"doc_hash": "413ae17bf5cddce0cad74f9f175c528e338a7a4431caa08174be9339b163f13e", "ref_doc_id": "9b2ecffe-f05f-44d6-b925-5d5859e8c5c5"}, "01832bf9-6952-4ee2-8678-cc17d0361b0f": {"doc_hash": "efb43e03cbbcf2e5490c6812045d85700fcf5438efb55c054ea120897b05e9c1", "ref_doc_id": "9b2ecffe-f05f-44d6-b925-5d5859e8c5c5"}, "5d772b57-c0ed-4383-a790-2c58504364ca": {"doc_hash": "f19a816d573bc89f741189359bb31b57eaceadca308df586b2f0c3e1e622662d", "ref_doc_id": "61ce1169-9e3b-4471-81f2-2ff328b548ac"}}, "docstore/ref_doc_info": {"0e841420-0ab3-4331-8b62-49fd7d387b45": {"node_ids": ["7db136dc-ee57-4a77-9846-8065af8ba6a6", "00391c6e-7a4e-4478-a9d1-b282ad7ee62c"], "metadata": {"page_label": "1", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}}, "bd7e3a64-b63b-43d1-8db0-a8b2ab7cdb12": {"node_ids": ["16abbe40-c1b8-4a71-a5c8-275ec5ea1bf7", "d9a454c6-c2fb-4659-bc49-66b6c3c39676"], "metadata": {"page_label": "2", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}}, "7cdc9ccc-7d69-4451-b304-6c9c58e7c82f": {"node_ids": ["b00d6fa4-a8db-46fb-bfdc-50dd0cb2cc06", "8e5c0f3e-0971-47d5-8992-763233432c7a"], "metadata": {"page_label": "3", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}}, "6b95f758-0a02-4385-9648-ed9e5000ab71": {"node_ids": ["2acf71c7-adad-4ee8-bce3-6f291a083a97", "a2ef63db-7d27-4212-8388-d42ae717fff9"], "metadata": {"page_label": "4", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}}, "d4db6341-4067-472a-b3f4-3e28cda74469": {"node_ids": ["be2a97f7-8c32-4d37-9383-eb2ae00559ea", "ba9c9a35-5657-4d59-bfc3-33559d06b4bb"], "metadata": {"page_label": "5", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}}, "309b599c-d2a9-4b9f-84f5-d0a2f2b5d590": {"node_ids": ["60a1a0df-5e79-4431-875d-19e9013e0af2", "97862008-8561-43b2-8aff-5ae8e8157387"], "metadata": {"page_label": "6", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}}, "5e808915-a91a-401f-9426-85e142367b6d": {"node_ids": ["16110f8e-4c4b-40f8-88e9-805e312cee13", "3d59eff3-58a2-44e1-9bd5-a7d079a903f2"], "metadata": {"page_label": "7", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}}, "55895482-15b6-4f3d-9c92-928d46da84f8": {"node_ids": ["5149e052-6bcf-42a1-9f91-89547425aa39", "1728b9a4-3f46-452a-b1fe-0016b3240982"], "metadata": {"page_label": "8", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}}, "0a21267d-a1df-454a-9db7-2aeb8817caf9": {"node_ids": ["615ed89c-9c89-4897-85d8-22b7544afeb0", "44e1133d-f199-4015-96ff-d8aaefba53a9"], "metadata": {"page_label": "9", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}}, "9b2ecffe-f05f-44d6-b925-5d5859e8c5c5": {"node_ids": ["b1014bc8-6517-483e-a40d-d8db716344dd", "25cd5565-e41b-4650-8319-f2994a9f3da6", "35f5d646-2d01-4b52-a693-0a75fb7e7447", "01832bf9-6952-4ee2-8678-cc17d0361b0f"], "metadata": {"page_label": "10", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}}, "61ce1169-9e3b-4471-81f2-2ff328b548ac": {"node_ids": ["5d772b57-c0ed-4383-a790-2c58504364ca"], "metadata": {"page_label": "11", "file_name": "DRR-framework.pdf", "file_path": "DRR-framework.pdf", "file_type": "application/pdf", "file_size": 696583, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07"}}}}