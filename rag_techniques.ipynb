{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) Techniques Applications\n",
    "\n",
    "Os LLMs (Large Language Models) podem ser adaptados para realizar uma variedade de tarefas comuns, como análise de sentimentos e reconhecimento de entidades, sem a necessidade de conhecimento prévio adicional. No entanto, para tarefas mais complexas e que demandam mais conhecimento, como garantir consistência factual e evitar erros de \"alucinação\", os pesquisadores desenvolveram o método da Geração Aprimorada por Recuperação, a.k.a (`RAG`).\n",
    "\n",
    "Essa abordagem combina um componente de recuperação de informações com um modelo gerador de texto, permitindo que o sistema acesse fontes externas de conhecimento para completar suas tarefas. Dessa forma, é interessante notar este tipo de método se torna capaz de lidar com informações que evoluem ao longo do tempo, sem necessidade de um novo ajuste-fino (que é computacionalmente caro).\n",
    "\n",
    "Embora os LLMs tenham demonstrado habilidades poderosas, como impulsionar casos de uso avançados, eles ainda enfrentam desafios como inconsistências de fatos e erros de interpretação. Ao integrar **RAG** aos LLMs, é possível enriquecer suas capacidades e melhorar sua confiabilidade, permitindo a outputs (respostas) mais precisos e relevantes.\n",
    "\n",
    "**Sobre este projeto:**\n",
    "\n",
    "**RAG** tem se tornado uma técnica cada vez mais promissora, pois não é a toa que vemos na literatura artigos sobre aplicação de **RAG** para mitigação das alucinações, comparações com ajuste-fino e seus benefícios, evolução para ideias de implementação cada vez mais avançadas (e eficazes) e frameworks especializados para suportar aplicações com RAG.\n",
    "\n",
    "Por isso, neste lab, venho com o intuito de ser capaz de implementar e apresentar algumas das técnicas muito interessantes de **RAG**.\n",
    "\n",
    "<img src=\"img/RAG.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "0. [Packages and Keys;](#packages-and-keys)\n",
    "1. [Production-Ready RAG Applications;](#production-rag)\n",
    "2. [Naive RAG;](#naive-rag)\n",
    "3. [Some Advanced Techniques;](#advanced-techniques)\n",
    "4. [More other applications;](#other-apps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='packages-and-keys'></div>\n",
    "\n",
    "## 0. Packages and Keys\n",
    "\n",
    "Before we start building our applications, you (we) need to install some Python libraries. Here's a brief overview of what each library does:\n",
    "\n",
    "- **Langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
    "- **OpenAI**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
    "- **Pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone API and store our chatbot's knowledge base in a vector database.\n",
    "- **Llama Index**: is an incredibly powerful tool for enhancing the capabilities of Large Language Models with your own data. Its array of data connectors, advanced query interfaces, and flexible integration make it a vital component in the development of applications with LLMs.\n",
    "\n",
    "Após feito isso, vamos importar as dependências necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, read (yours) the keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")       # YOUR API KEY\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")   # YOUR API KEY\n",
    "PINECONE_ENV_KEY = os.getenv(\"PINECONE_API_ENV\")   # YOUR ENV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='production-rag'></div>\n",
    "\n",
    "## 1. Production-Ready RAG Applications\n",
    "_Este tópico é baseado na apresentação de Jerry Liu, Co-Founder/CEO do LlamaIndex, no AI Engineer Summit._ \n",
    "\n",
    "É surpreendente o poder da IA Generativa. Hoje, pode-se observar uma gama de casos de uso incríveis com grande relevância para os LLM tais como: \n",
    "\n",
    "- Document Processing\n",
    "- Tagging & Extraction\n",
    "- Knowledge Search & Question Answering\n",
    "- Conversational Agents\n",
    "- Workflow Automation\n",
    "\n",
    "Nesse sentido, também sabemos que grandes quantidade de dados são geradas todos os dias, o tempo todo, e tendo isso em vista, a fim de ensinar os LLMs a entender dados novos, temos em mãos duas abordagens capazes de suprir esta demanda: **retrieval augmented generation (RAG)**, que envolve adicionar contexto (por um Vector Database) ao prompt do modelo a partir de uma fonte de dados, e o **fine-tuning**, que atualiza os pesos da rede para incorporar conhecimento.\n",
    "\n",
    "No entanto, como estamos focando em `RAG` no momento, vamos tratar um pouco agora sobre a construção de um sistema de perguntas e respostas. Este processo envolve duas etapas principais: **ingestão de dados** e **consulta de dados**, que inclui recuperação e síntese. \n",
    "\n",
    "<img src=\"img/rag-stack.png\">\n",
    "\n",
    "Embora esses componentes sejam fundamentais, e seja este o sistema que mais tem surgido atualmente, os desenvolvedores talvez começam a perceber que isto não é suficiente e passam a enfrentar desafios que impedem a produção eficaz dessas aplicações, como \n",
    "1. Baixa precisão e chunks irrelevantes, que podem causar halucinações;\n",
    "2. Baixa recuperação, que não recupera todos os pedaços relevantes, implicando em contexto incompleto;\n",
    "3. Informações desatualizadas, onde dados antigos tornam o LLM impreciso e incapaz de raciocinar.\n",
    "\n",
    "Para superar esses desafios, Jerry sugere melhorias em toda a pipeline, desde armazenar informações adicionais e otimizar a pipeline de dados até explorar métodos avançados de recuperação e síntese, além da importância de ser capaz medir o desempenho de um sistema RAG (que podemos entrar em detalhes em outro momento). \n",
    "\n",
    "Nesse sentido, é isso que busco fazer aqui. Mas antes de buscarmos essas melhorias, vamos começar com um caso básico, isto é, o `Naive RAG`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='naive-rag'></div> \n",
    "\n",
    "## 2. Naive RAG\n",
    "\n",
    "### 2.1. Introdução e entendendo o Naive-RAG\n",
    "\n",
    "Atualmente, eu faço pesquisa (uma iniciação tecnológica) que trata sobre um _estudo comparativo de modelos de sistema de recomendação baseados em aprendizado por reforço_. Durante meus estudos, estive buscando entender uma certa implementação aberta acerca do artigo _Deep Reinforcement Learning based Recommendation with Explicit User-Item Interactions Modeling_. Perguntei ao ChatGPT se ele conhecia o artigo e se de alguma forma ele iria conseguir me ajudar. Ele felizmente conhecia, no entanto, apenas conhecia sua existência e do que se tratava o artigo, mas não sobre detalhes internos. Então pensei: \"por que não usar **RAG** para ele ser capaz de explicar algumas das minhas dúvidas?\". Logo, aqui mostrarei a aplicação de uma arquitetura básica de RAG\n",
    "(`Naive RAG`) para este caso de uso específico.\n",
    "\n",
    "Antes de começarmos, é interessante entender um pouco como funciona o esquema/arquitetura do Naive Rag. Segue abaixo.\n",
    "<img src=\"img/naive-rag.png\">\n",
    "\n",
    "Resumidamente, o que fazemos é o seguinte: você **divide seus textos** (de seus documentos) em pedaços, depois **incorpora** esses pedaços em vetores com algum **Transformer Encoder model** ou **Embedding model**, coloca todos esses vetores em um **Vector Database** e, finalmente, **cria um prompt** para um LLM que diz ao modelo para **responder a consulta do usuário**, considerando o **contexto** que encontramos na etapa de busca.\n",
    "\n",
    "No tempo de execução, **vetorizamos a consulta do usuário** com o mesmo Encoder model ou Embedding model e, em seguida, executamos a **pesquisa desse vetor de consulta no Vector Database**, encontramos os **k principais resultados**, recuperamos os pedaços de texto correspondentes de nosso banco de dados e os **alimentamos** no prompt do LLM **como contexto**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Prototipagem\n",
    "Como o objetivo aqui é trazer um conteúdo bem consolidado (e espero trazer, pelo menos) e apesar das discussões sobre quando usar **LangChain** ou **Llama Index** (e eu particularmente achar que talvez Llama Index seja mais apropriado), acredito que usar ambos será definitivamente divertido (embora mais caro rs). \n",
    "\n",
    "#### 2.2.1. Com LangChain\n",
    "\n",
    "#### Fast overview\n",
    "Inicialmente, vamos criar um chatbot simples sem qualquer tipo de contexto inserido, inicializando uma instância de `ChatOpenAI` com nossa chave da API da OpenAI e utilizando o modelo `gpt-3.5-turbo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    api_key = OPENAI_API_KEY,\n",
    "    model = \"gpt-3.5-turbo-0125\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chats com modelos de chat da OpenAI como o `gpt-3.5-turbo` e o `gpt-4-0125-preview` geralmente são estruturados da seguinte forma: \n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi Chat, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand about reinforcement learning theory.\n",
    "\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "O final `\"Assistant:\"` sem uma resposta é o que levaria o modelo a continuar a conversa. No caso do endpoint oficial do OpenAI `ChatCompletion`, eles seriam passados como:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi Chat, how are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand about reinforcement learning theory.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "No entanto, quando utilizamos o LangChain, há uma leve diferença que se dá por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi Chat, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand about reinforcement learning theory.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No fim, esse formato vai se assemelhar bastante com o contexto oficial. Note que em termos de analogia, quando a nossa `role` era **user**, para o LangChain temos um objeto de **HumanMessage**; quando a nossa `role` era **assistant**, para o LangChain temos um objeto de **AIMessage** e, por fim, quando a nossa `role` era **system**, temos para o LangChain um objeto de **SystemMessage**.\n",
    "\n",
    "Agora, para sermos capazes de gerar uma resposta, vamos passar estas mensagens para a instância do `ChatOpenAI` que criamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Reinforcement learning is a type of machine learning that involves an agent learning how to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions, and uses this feedback to learn the best strategies for achieving its goals.\\n\\nIn reinforcement learning, the agent's goal is typically to maximize its cumulative reward over time. Through trial and error, the agent learns which actions lead to the most favorable outcomes and adjusts its behavior accordingly.\\n\\nSome key components of reinforcement learning include:\\n\\n1. Agent: The entity that learns and makes decisions within the environment.\\n2. Environment: The external system with which the agent interacts.\\n3. State: A representation of the current situation of the agent within the environment.\\n4. Action: The choices available to the agent at any given state.\\n5. Reward: The feedback signal that the agent receives after taking an action, indicating how favorable or unfavorable that action was.\\n\\nReinforcement learning algorithms, such as Q-learning and deep Q-networks (DQN), are used to train agents to make optimal decisions in various tasks, such as playing games, controlling robots, and optimizing business processes.\\n\\nI hope this overview helps you understand the basics of reinforcement learning theory. Let me know if you have any specific questions or need further clarification.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que obtemos uma mensagem do tipo AI Message. Se quisermos dar uma olhada mais clara na resposta podemos fazer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement learning is a type of machine learning that involves an agent learning how to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions, and uses this feedback to learn the best strategies for achieving its goals.\n",
      "\n",
      "In reinforcement learning, the agent's goal is typically to maximize its cumulative reward over time. Through trial and error, the agent learns which actions lead to the most favorable outcomes and adjusts its behavior accordingly.\n",
      "\n",
      "Some key components of reinforcement learning include:\n",
      "\n",
      "1. Agent: The entity that learns and makes decisions within the environment.\n",
      "2. Environment: The external system with which the agent interacts.\n",
      "3. State: A representation of the current situation of the agent within the environment.\n",
      "4. Action: The choices available to the agent at any given state.\n",
      "5. Reward: The feedback signal that the agent receives after taking an action, indicating how favorable or unfavorable that action was.\n",
      "\n",
      "Reinforcement learning algorithms, such as Q-learning and deep Q-networks (DQN), are used to train agents to make optimal decisions in various tasks, such as playing games, controlling robots, and optimizing business processes.\n",
      "\n",
      "I hope this overview helps you understand the basics of reinforcement learning theory. Let me know if you have any specific questions or need further clarification.\n"
     ]
    }
   ],
   "source": [
    "print(response.content) # tomando o conteúdo da resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que nossa `response` é um outro objeto do tipo `AIMessage`, podemos adicionar ela à lista `messages` e outra `HumanMessage` para gerar uma conversação, tomando em vista a memória do que já foi conversado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are some examples of agents in the context of reinforcement learning:\n",
      "\n",
      "1. **Autonomous Driving Agent**: An agent that learns to navigate a car through traffic by receiving rewards for safe driving behaviors and penalties for accidents or traffic violations.\n",
      "\n",
      "2. **Game-Playing Agent**: An agent that learns to play video games by receiving rewards for achieving high scores or completing levels efficiently.\n",
      "\n",
      "3. **Robotic Arm Agent**: An agent that learns to control a robotic arm to perform specific tasks, such as picking and placing objects, based on feedback from sensors and actuators.\n",
      "\n",
      "4. **Recommendation System Agent**: An agent that learns user preferences and makes personalized recommendations for products, movies, or content based on feedback and user interactions.\n",
      "\n",
      "5. **Financial Trading Agent**: An agent that learns to make profitable investment decisions in the stock market by receiving rewards for successful trades and penalties for losses.\n",
      "\n",
      "These are just a few examples of how agents can be applied in different domains to learn and optimize decision-making processes through reinforcement learning. Let me know if you'd like more information on any specific example or have any other questions.\n"
     ]
    }
   ],
   "source": [
    "# Adicionando a resposta da IA à messages\n",
    "messages.append(response)\n",
    "\n",
    "# Criando um novo user prompt\n",
    "prompt = HumanMessage(content=\"Give me some Agent examples.\")\n",
    "\n",
    "# Adicionando o user prompt à messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# Requisitando uma resposta do ChatGPT\n",
    "response = chat(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legal, temos o nosso bot, porém nada novo por agora. Então, o que podemos ver em seguida é a questão do conhecimento dos LLMs. Apesar de grande parte dos LLMs hoje existentes conseguirem responder uma gama enorme de perguntas diversas com bastante eficácia, nós devemos ter em mente que seu conhecimento não é infinito. O seu conhecimento é limitado pelo seu treinamento, com os dados de treino comprimidos em seus parâmetros internos. Assim, essencialmente, eles aprendem tudo o que sabem durante o treino, e isto implica que eles não podem ter acesso a dados externos por si só.\n",
    "\n",
    "Podemos ver isso de forma clara quando perguntamos algo recente ao LLM (neste caso, o gpt-3.5-turbo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not familiar with the specific term \"RL4RS framework.\" It's possible that it refers to a specific framework or approach related to reinforcement learning, but without more context or information, I can't provide specific details about it.\n",
      "\n",
      "If you can provide more details or context about the RL4RS framework, I'd be happy to try to help you understand it better or provide information on a related topic. Feel free to share more information or ask any other questions you may have.\n"
     ]
    }
   ],
   "source": [
    "# Adicionando a resposta da IA à messages\n",
    "messages.append(response)\n",
    "\n",
    "# Criando um novo user prompt\n",
    "prompt = HumanMessage(content=\"Do you know de RL4RS framework?\")\n",
    "\n",
    "# Adicionando o user prompt à messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# Requisitando uma resposta do ChatGPT\n",
    "response = chat(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse sentido, como podemos então obter respostas dado um conteúdo atual? E se o LLM me respondesse algo que não é verdade? Ou fazemos um novo ajuste-fino com novos dados (no entanto, muitas vezes isso pode ser inviável dependendo do tipo de problema ou caso de uso que você quer solucionar) ou implementamos justamente o RAG, que insere contexto externo para o LLM ter conteúdo suficiente para dar uma resposta. \n",
    "\n",
    "Antes de implementarmos o conceito de RAG a fim de inserir este tipo de conhecimento externo, vamos primeiramente construir a base deste conhecimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Building the Knowledge Base\n",
    "\n",
    "Como apresentado no _tópico 1_, aqui estaremos basicamente implementando a seção de `data ingestation`. Neste caso espeífico, estarei utilizando o Facebook AI Similarity Search (`FAISS`) como nosso Vector Database, que basicamente é uma biblioteca para pesquisa eficiente de similaridade e agrupamento de vetores densos e possui uma usabilidade fácil. Eu irei alimentá-lo com o paper _Deep Reinforcement Learning based Recommendation with Explicit User-Item Interactions Modeling_.\n",
    "\n",
    "Para isso, vamos primeiro utilizar um objeto do tipo `Document Loader` disponibilizado pelo LangChain. Um objeto `Document Loader` basicamente é usado para carregar dados de uma fonte como documentos, em que um documento é um pedaço de texto com metadados associados. Eles podem carregar desde dados de arquivos .txt simples a carregar transcrições de vídeos do Youtube. \n",
    "\n",
    "Neste caso, como um paper é um PDF, estaremos utilizando um objeto `Document Loader` que lê PDF's, sendo mais específico, vamos utilizar o `PyPDFLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"DRR-framework.pdf\")\n",
    "drr_framework = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Deep Reinforcement Learning based\\nRecommendation with Explicit User-Item\\nInteractions Modeling\\nFeng Liu∗, Ruiming Tang†, Xutao Li∗, Weinan Zhang‡Yunming Ye∗, Haokun Chen‡, Huifeng Guo†and Yuzhou Zhang†\\n∗Shenzhen Key Laboratory of Internet Information Collaboration\\nShenzhen Graduate School, Harbin Institute of Technology, Shenzhen, 518055, China\\nEmail: fengliu@stu.hit.edu.cn, lixutao@hit.edu.cn, yeyunming@hit.edu.cn\\n†Noah’s Ark Lab, Huawei, China\\nEmail: tangruiming, huifeng.guo, zhangyuzhou3@huawei.com\\n‡Shanghai Jiao Tong University, Shanghai, China\\nEmail: wnzhang@sjtu.edu.cn, chenhaokun@sjtu.edu.cn\\nAbstract —Recommendation is crucial in both academia and\\nindustry, and various techniques are proposed such as content-\\nbased collaborative ﬁltering, matrix factorization, logistic re-\\ngression, factorization machines, neural networks and multi-\\narmed bandits. However, most of the previous studies suffer\\nfrom two limitations: (1) considering the recommendation as\\na static procedure and ignoring the dynamic interactive nature\\nbetween users and the recommender systems; (2) focusing on the\\nimmediate feedback of recommended items and neglecting the\\nlong-term rewards. To address the two limitations, in this paper\\nwe propose a novel recommendation framework based on deep\\nreinforcement learning, called DRR. The DRR framework treats\\nrecommendation as a sequential decision making procedure and\\nadopts an “Actor-Critic” reinforcement learning scheme to model\\nthe interactions between the users and recommender systems,\\nwhich can consider both the dynamic adaptation and long-\\nterm rewards. Further more, a state representation module is\\nincorporated into DRR, which can explicitly capture the interac-\\ntions between items and users. Three instantiation structures are\\ndeveloped. Extensive experiments on four real-world datasets are\\nconducted under both the ofﬂine and online evaluation settings.\\nThe experimental results demonstrate the proposed DRR method\\nindeed outperforms the state-of-the-art competitors.\\nIndex Terms —Recommendation, Deep Reinforcement Learn-\\ning, User-Item Interactions\\nI. I NTRODUCTION\\nThanks to the increasing online services, such as online\\nshopping, online news and online social networks, it becomes\\nquite convenient to acquire items (goods, books, videos,\\nnews, etc.) via Internet or mobile devices. Albeit the great\\nconvenience, the overwhelming number of items in the sys-\\ntems also pose a signiﬁcant challenge for users, to ﬁnd the\\nitems that match their interests. Recommendation is a widely\\nused solution and various families of techniques have been\\nproposed, such as content-based collaborative ﬁltering [1],\\nmatrix factorization based methods [2]–[5], logistic regression,\\nfactorization machines and its variants [6]–[8], deep learning\\nmodels [9]–[12] and multi-armed bandits [13]–[17]. However,\\nsuch mentioned studies suffer from two serious limitations.Firstly , most of them consider the recommendation proce-\\ndure as a static process, i.e., they assume the user’s underlying\\npreference keeps unchanged. However, it is very common\\nthat a user’s preference is dynamic w.r.t. time, i.e., a user’s\\npreference on previous items will affect her choice on the next\\nitems. Hence, it would be more reasonable to model the recom-\\nmendation as a sequential decision making process. We will\\nshow some evidence observed in publicly available datasets\\n(MovieLens and Yahoo! Music) to support our opinion. In the\\ntwo datasets, the sequential behaviors of users are recorded and\\nwe are interested in what would happen if a user consecutively\\nreceives satisﬁed or unsatisﬁed recommendations. Though the\\ndatasets do not record any recommendation procedure, we\\ncan simulate this according to the users’ ratings, namely,\\nconsecutive rating “positive” (“negative”) simulates that a user\\nconsecutively receives satisﬁed (unsatisﬁed) recommendations.\\nAs presented in Figure 1, we observe that a user tends\\nto gives a higher (lower) rating if she has consecutively\\nreceived more satisﬁed (unsatisﬁed) items, as shown by the\\ngreen (red) line, where the blue dot line denotes the average\\nrating for reference. This suggests that a user will be more\\npleasant (unpleasant) if she consecutively receives more sat-\\nisﬁed (unsatisﬁed) recommendations and therefore she tends\\nto give a higher (lower) rating to the current recommendation.\\nHence, the user’s dynamic preference suggests that a good\\nrecommendation should be modeled as a sequential decision\\nmaking process.\\nSecondly , the aforementioned studies are trained by max-\\nimizing the immediate rewards of recommendations, which\\nmerely concentrates on whether the recommended items are\\nclicked or consumed, but ignores the long-term contributions\\nthat the items can make. However, the items with small imme-\\ndiate rewards but large long-term beneﬁts are also crucial [18].\\nWe take an example in News recommendation [19] to explain\\nthis. As a user requests for news to read, two possible pieces of\\nnews may lead to the same immediate reward, i.e., the user willarXiv:1810.12027v3  [cs.IR]  29 Oct 2019', metadata={'source': 'DRR-framework.pdf', 'page': 0}),\n",
       " Document(page_content='Fig. 1. Analysis on sequential patterns on user’s behavior in MovieLens and\\nYahoo!Music datasets\\nclick and read the two pieces of news with equal probability,\\nwhere one is about a thunderstorm alert and the other is about\\na basketball player Kobe Bryant. In this example, after reading\\nthe news about thunderstorm, the user probably is not willing\\nto read news about this issue anymore; while on the other hand,\\nthe user will possibly read more about NBA or basketball\\nafter reading the news about Kobe. The fact suggests that\\nrecommending the news about Kobe will introduce more long-\\nterm rewards. Hence, when recommending items to users, both\\nthe immediate and long-term rewards should be taken into\\nconsideration.\\nRecently, Reinforcement Learning (RL) [20], which has\\nshown great potential in various challenging scenarios that\\nrequire both dynamic modeling and long term planning, such\\nas game playing [21], [22], real-time ads bidding [23], [24],\\nneural network structure searching [25], [26], is introduced in\\nrecommender systems [18], [19], [27]–[33].\\nIn the early stage, model-based RL techniques are proposed\\nto model recommendation procedure, such as POMDP [18]\\nand Q-learning [27]. However, these methods are inapplicable\\nto complicated recommendation scenarios when the number\\nof candidate items is large, because a time-consuming dy-\\nnamic programming step is required to update the model.\\nLater, model-free RL techniques are utilized in recommender\\nsystems, from both academia and industry. Such techniques\\ncan be divided into two categories: value-based [19], [29] and\\npolicy-based [28], [32], [33]. Value-based approaches compute\\nQ-values of all available actions for a given state and the\\none with the maximum Q-value is selected as the best action.\\nDue to the evaluation on overall actions, the approaches may\\nbecome very inefﬁcient if the action space is too large. As\\nfor the policy-based approaches, this type of studies generate\\na continuous parameter vector as the representation of an\\naction [28], [32], [33], which can be utilized in generating the\\nrecommendation and updating the Q-value evaluator. Thanks\\nto the continuous representations, the inefﬁciency drawbacks\\ncan be overcome. However, these studies [28], [32], [33] still\\nhave one common limitation: the user state is learnt via a\\nconventional fully connected neural network, which does not\\nexplicitly and carefully model the interactions between users\\nand items.\\nIn this paper, to break the limitations stated above, we\\npropose a d eep r einforcement learning based r ecommendation\\nframework with explicit user-item interactions modeling(DRR). The “Actor-Critic” type framework DRR is incor-\\nporated with a state representation module, which explicitly\\nmodels the complex dynamic user-item interactions to pursuit\\nbetter recommendation performance. Speciﬁcally, the embed-\\ndings of users and items from the historical interactions are fed\\ninto a carefully designed multi-layer network, which explicitly\\nmodels the interactions between users and items, to produce\\na continuous state representation of the user in terms of her\\nunderlying sequential behaviors. This network is named as\\nthe state representation module, which plays two important\\nroles in our framework. On the one hand, it is utilized to\\ngenerate an ranking action to calculate the recommendation\\nscores for ranking. On the other hand, the state representation\\ntogether with the generated action is the input of the Critic\\nnetwork, which aims to estimate the Q-value, i.e., the quality\\nof the action in the current state. Based on the evaluation, the\\nActor (policy) network can be updated. We note that both the\\nActor and Critic networks are carefully designed by modeling\\nthe interactions between users and items explicitly. Extensive\\nexperiments on four real-world datasets demonstrate that the\\nproposed method yields superior performance than the state-\\nof-the-art methods. The main contributions of this paper can\\nbe summarized as follows:\\n•We propose a deep reinforcement learning based rec-\\nommendation framework DRR. Unlike the conventional\\nstudies, DRR adopts an “Actor-Critic” structure and treats\\nthe recommendation as a sequential decision making\\nprocess, which takes both the immediate and long-term\\nrewards into consideration.\\n•Under the DRR framework, three different network struc-\\ntures are proposed, which can explicitly model the inter-\\nactions between users and items.\\n•Extensive experiments are carried out on four real-world\\ndatasets, and the results demonstrate the proposed meth-\\nods indeed outperforms the state-of-the-art competitors.\\nThe rest of this paper is organized as follows. Related work\\nand background are presented in Section II. The preliminary\\nknowledge is presented in Section III. The proposed methods\\nare introduced in Section IV . Experimental details and results\\nare discussed in Section V . Finally, we conclude this paper\\nand discuss some future work in Section VI.\\nII. R ELATED WORK\\nA. Non-RL based Recommendation Techniques\\nVarious kinds of recommendation techniques are proposed\\nin the past a few decades to improve the performance of\\nrecommender systems, including content-based ﬁltering [1],\\nmatrix factorization based methods [2]–[5], logistic regression,\\nfactorization machines and its variants [6]–[8], and until\\nrecently deep learning models [9]–[12].\\nAt the beginning of this century, content-based ﬁltering [1]\\nis proposed to recommend items by considering the content\\nsimilarity between items. Later, collaborative ﬁltering (CF) is\\nput forward and extensively studied. The rationale behind CF\\nis that the users with similar behaviors tend to prefer the same', metadata={'source': 'DRR-framework.pdf', 'page': 1}),\n",
       " Document(page_content='items, and the items consumed by similar users tend to have\\nthe same rating. However, conventional CF based methods\\ntend to suffer from the data scarcity, because the similarity\\ncalculated from sparse data can be very unreliable. Matrix\\nfactorization (MF), as an advanced CF technique, plays an\\nimportant role in recommender systems. MF models [2]–[5]\\ncharacterize both items and users by vectors in the same space,\\nwhich are inferred from the observed user-item interactions.\\nRegarding the recommendation as a binary classiﬁcation prob-\\nlem, logistic regression and its variants [6] are also utilized\\nin recommender systems. However, logistic regression based\\nmodels are hard to generalize to the feature interactions that\\nnever or rarely appear in the training data. Factorization\\nmachines [7] model pairwise feature interactions as inner\\nproduct of latent vectors between features and show promising\\nresults. As an extension to FM, Field-aware FM (FFM [8])\\nenables each feature to have multiple latent vectors to interact\\nwith different ﬁelds. Recently, deep learning models [9]–[12]\\nare applied to model the complicated feature interactions for\\nrecommendation.\\nAs a distinguished direction, contextual multi-armed bandits\\nare also utilized to model the interactive nature of recom-\\nmender systems [13]–[17]. Li et al. apply Thompson Sampling\\n(TS) and Upper Conﬁdent Bound (UCB) to balance the trade-\\noff between exploration and exploitation in [13] and [14],\\nrespectively. The authors of [16] propose a dynamic context\\ndrift model to address the time varying problem. To integrate\\nthe latent vectors of items and users with some exploration,\\nthe authors of [15], [17] combine matrix factorization with\\nmulti-armed bandits.\\nHowever, all these methods suffer from two limitations.\\nFirst, they consider the recommendation procedure as a static\\nprocess, i.e., they assume the underlying user’s preference\\nkeeps static and they aim to learn the user’s preference as\\nprecise as possible. Second, they are learned to maximize the\\nimmediate rewards of recommendations, but ignore the long-\\nterm beneﬁts that the recommendations can make.\\nB. RL based Recommendation Techniques\\nAs model-based RL techniques [18], [27] are inapplicable\\nin recommendation scenario due to their high time complex-\\nity, most researchers turn to model-free RL techniques. The\\nmodel-free RL techniques can be divide into two categories:\\npolicy-based and value-based.\\nPolicy-based approaches [28], [32], [33] aim to generate\\na policy, of which the input is a state, and the output is\\nan action. These works apply deterministic policies, which\\ngenerates an action directly. Dulac-Arnold et al. [33] resolves\\nthe large action space problem by modeling the state in a\\ncontinuous item embedding space and selecting the items via\\na neighborhood method. However, as the underlying algorithm\\nis essentially a continuous-action algorithm, its performance\\nmay be cursed by the gap between the continuous and discrete\\naction spaces. In [28], [32], the policy network outputs a\\ncontinuous action representation, and the recommendation is\\ngenerated by ranking the items with their scores, which arecomputed by a pre-deﬁned function with the action representa-\\ntion and the item embeddings as input. However, one common\\nlimitation of the studies is that they do not carefully learn the\\nstate representation.\\nFor value-based approaches [19], [29], the action with max-\\nimum Q-value over all the possible actions is selected as the\\nbest action. Zhao et al. [29] take both user’s positive feedback\\nand negative feedback into consideration when modeling user\\nstate. Dueling Q-network is utilized in [19], to model Q-\\nvalue of a state-action pair. Moreover, a minor update with\\nexploration by dueling bandit gradient descent is proposed.\\nHowever, such value-based approaches need to evaluate the\\nQ-values of all the actions under a speciﬁc state, which is\\nvery inefﬁcient when the number of actions is large.\\nTo make RL based recommendation techniques suitable for\\nlarge-scale scenario, in this paper, we propose the DRR frame-\\nwork which carefully and explicitly model the interactions\\nbetween users and items to learn the state representation.\\nIII. P RELIMINARIES\\nThe essential underlying model of reinforcement learning\\nis Markov Decision Process (MDP). An MDP is deﬁned as\\n(S,A,P,R,γ).Sis the state space and Ais the action\\nspace.P:S×A×S ↦→ [0,1]is the state transition\\nfunction.R:S×A×S ↦→ Ris the reward function. γ\\nis the discount rate. The objective of an agent in an MDP\\nis to ﬁnd an optimal policy ( πθ:S×A ↦→ [0,1]) which\\nmaximizes the expected cumulative rewards from any state\\ns∈ S , i.e.,V∗(s) = max πθEπθ{∑∞\\nk=0γkrt+k|st=s},\\nor maximizes equivalently the expected cumulative rewards\\nfrom any state-action pair s∈ S,a∈ A , i.e.,Q∗(s,a) =\\nmaxπθEπθ{∑∞\\nk=0γkrt+k|st=s,at=a}. Here Eπθis the\\nexpectation under policy πθ,tis the current timestep and rt+k\\nis the immediate reward at a future timestep t+k.\\nWe model the recommendation procedure as a sequential\\ndecision making problem, in which the recommender (i.e.,\\nagent) interacts with users (i.e., environment) to suggest a list\\nof items sequentially over the timesteps, by maximizing the\\ncumulative rewards of the whole recommendation procedure.\\nMore speciﬁcally, the recommendation procedure is modeled\\nby an MDP, as follows.\\n•StatesS.A statesis the representation of user’s positive\\ninteraction history with recommender, as well as her\\ndemographic information (if it exists in the datasets).\\n•ActionsA.An actionais a continuous parameter vector\\ndenoted as a∈R1×k. Each item it∈R1×k1has a\\nranking score, which is deﬁned as the inner product of\\nthe action and the item embedding, i.e., ita⊤. Then the\\ntop ranked ones will be recommended.\\n•TransitionsP.The state is modeled as the representa-\\ntion of user’s positive interaction history. Hence, once\\nthe user’s feedback is collected, the state transition is\\ndetermined.\\n1itis the embedding of item i, which can be generated by MF or V AE.', metadata={'source': 'DRR-framework.pdf', 'page': 2}),\n",
       " Document(page_content='•RewardR.Given the recommendation based on the\\nactionaand the user state s, the user will provide her\\nfeedback, i.e., click, not click, or rating, etc. The recom-\\nmender receives immediate reward R(s,a)according to\\nthe user’s feedback.\\n•Discount rate γ.γ∈[0,1]is a factor measuring the\\npresent value of long-term rewards. In the case of γ= 0,\\nthe recommender considers only immediate rewards but\\nlong-term rewards are ignored. On the other hand, when\\nγ= 1, the recommender treats immediate rewards and\\nlong-term rewards as equally important.\\nFigure 2 illustrates the recommender-user interactions in\\nMDP formulation. Considering the current user state and\\nimmediate reward to the previous action, the recommender\\ntakes an action. Note that in our model, an action corre-\\nsponds to neither recommending an item nor recommending\\na list of items. Instead, an action is a continuous parameter\\nvector. Taking such an action, the parameter vector is used\\nto determine the ranking scores of all the candidate items,\\nby performing inner product with item embeddings. All the\\ncandidate items are ranked according to the computed scores\\nand Top-N items are recommended to the user. Taking the\\nrecommendation from the recommender, the user provides her\\nfeedback to the recommender and the user state is updated\\naccordingly. The recommender receives rewards according\\nto the user’s feedback. Without loss of generalization, a\\nrecommendation procedure is a Ttimestep2trajectory as\\n(s0,a0,r0,s1,a1,r1,...,sT−1,aT−1,rT−1,sT).\\nRecommender (Agent)\\nUsers (Environment)state!\"!\"=$(&\")reward(\"(\"=)(!\",+\")!\",-(\",-action+\"+\"=./(!\")\\nFig. 2. Recommender-User interactions in MDP\\nIV. T HEPROPOSED DRR F RAMEWORK\\nAs aforementioned in Section 1, conventional recommenda-\\ntion techniques suffer from either a lack of sequential model-\\ning or ignoring the long-term rewards, or both. To address\\nthe drawbacks, we propose a deep reinforcement learning\\nbased recommendation framework (DRR) based on the Actor-\\nCritic learning scheme. Also, different from some recent RL\\nstudies, we carefully and explicitly build a state representation\\nmodule to model the interactions between the users and items.\\nNext, we will ﬁrst elaborate the Actor network, Critic network\\nand the state representation module respectively, which are\\n2If a recommendation episode terminates in less than T timesteps, then the\\nlength of the episode is the actual value.essentially the three key ingredients in our framework; then\\nthe training and evaluation procedures will be presented to\\nshow how to learn and use the DRR framework.\\nSReLUReLUaReLUconcatQ(s,a)State RepresentationModuleItemsSReLUReLUaTanhItem Space.RankingRecommendationActorCriticLegend.Item or userScalar productFC layerXElement-wise productWeight 1Item weightAverage poolinglayerScalar \\nFig. 3. DRR Framework\\nA. Three Key Ingredients in DRR\\n1) The Actor network: The Actor network, also called the\\npolicy network, is depicted on the left part of Figure 3. For\\na given user, the network accounts for generating an action\\nabased on her state s. Let us explain the network from the\\ninput to the output part. In DRR, the user state, denoted by\\nthe embeddings of her nlatest positively interacted items,\\nis regarded as the input. Then the embeddings are fed into\\na state representation module (which will be introduced in\\ndetails later) to produce a summarized representation sfor the\\nuser. For instance, at timestep t, the state can be deﬁned in\\nEq. (1):\\nst=f(Ht) (1)\\nwheref(·)stands for the state representation module, Ht=\\n{i1,...,in}denotes the embeddings of the latest positive inter-\\naction history, and it∈R1×kis ak-dimensional vector. When\\nthe recommender agent recommends an item it, if the user\\nprovides positive feedback, then in the next timestep, the state\\nis updated to st+1=f(Ht+1), whereHt+1={i2,...,in,it};\\notherwise,Ht+1=Ht. The reasons to deﬁne the state in such\\na manner are two folds: (i) a superior recommender system\\nshould cater to the users’ taste, i.e., what items the users like;\\n(ii) the latest records represent the users’ recent interests more\\nprecisely.\\nFinally, by two ReLU layers and one Tanh layer, the state\\nrepresentation sis transformed into an action a=πθ(s)as\\nthe output of the Actor network. Particularly, the action a\\nis deﬁned as a ranking function represented by a continuous\\nparameter vector a∈R1×k. By using the action, the ranking\\nscore of the item itis deﬁned as:\\nscoret=ita⊤(2)\\nThen, the top ranked item (w.r.t. the ranking scores) is rec-\\nommended to the user. Note that, the widely used ε-greedy\\nexploration technique is adopted here.', metadata={'source': 'DRR-framework.pdf', 'page': 3}),\n",
       " Document(page_content='2) The Critic network: The Critic part in DRR, shown as\\nthe middle part of Figure 3, is a Deep Q-Network [21], which\\nleverages a deep neural network parameterized as Qω(s,a)\\nto approximate the true state-action value function Qπ(s,a),\\nnamely, the Q-value function. The Q-value function reﬂects the\\nmerits of the action policy generated by the Actor network.\\nSpeciﬁcally, the input of the Critic network is the user state\\nsgenerated by the user state representation module and the\\nactionagenerated by the policy network, and the output is\\nthe Q-value, which is a scalar. According to the Q-value, the\\nparameters of the Actor network are updated in the direction of\\nimproving the performance of action a, i.e., boosting Qω(s,a).\\nBased on the deterministic policy gradient theorem [34], we\\ncan update the Actor by the sampled policy gradient shown\\nin Eq.(3):\\n∇θJ(πθ)≈1\\nN∑\\nt∇aQω(s,a)|s=st,a=πθ(st)∇θπθ(s)|s=st\\n(3)\\nwhereJ(πθ)is the expectation of all possible Q-values that\\nfollow the policy πθ. Here the mini-batch strategy is utilized\\nandNdenotes the batch size. Moreover, the Critic network\\nis updated accordingly by the temporal-difference learning\\napproach [20], i.e., minimizing the mean squared error shown\\nin Eq.(4):\\nL=1\\nN∑\\ni(yi−Qω(si,ai))2(4)\\nwhereyi=ri+γQω′(si+1,πθ′(si+1)). The target net-\\nwork [35] technique is also adopted in DRR framework, where\\nω′andθ′is the parameters of the target Critic and Actor\\nnetwork.\\n3) The State Representation Module: As noted above, the\\nstate representation module plays an important role in both\\nthe Actor network and Critic network. Hence, it is very\\ncrucial to design a good structure to model the state. In [10],\\n[11], it has been shown that modeling the feature interactions\\nexplicitly can boost the performance of a recommendation\\nsystem. Inspired by the studies, we propose to design the state\\nrepresentation module by explicitly modeling the interactions\\nbetween the users and items. Speciﬁcally, we develop three\\nstructures, which will be elaborated next.\\nItemsS…Concat& flattenXXX\\nFig. 4. DRR-p Structure\\n•DRR-p . Inspired by [10], [11], we propose a product\\nbased neural network for the state representation module,which is depicted in Figure 43. The structure is named\\nas DRR-p, which utilizes a product operator to capture\\nthe pairwise local dependency between items. We can\\nsee that the structure clones the representations of the n\\nitems fromH={i1,...,in}. In addition, it computes the\\npairwise interactions between the nitems, by using the\\nelement-wise product operator. As a result, n(n−1)/2\\nnew features vectors are yielded, which will be concate-\\nnated with the cloned vectors as the state representation.\\nWe note that in the element-wise product part, a weight is\\nalso learned for each item to show its importance. Hence,\\nin DRR-p the state representation module can be formally\\nstated as follows:\\ns= [H,{pa,b|a,b= 1,...,n}] (5)\\npa,b=waia⊗wbib (6)\\nwhere⊗denotes the element-wise product, wais a\\nscalar indicating the importance of item ia, andpa,bis\\nak-dimensional vector which models the interactions\\nbetween item iaandib. The dimensionality of sis\\nk(n+n(n−1)/2).\\n•DRR-u . Though DRR-p can model the pairwise local\\ndependency between items, the user-item interactions are\\nneglected. To remedy this, we design another structure in\\nFigure 5, which is referred as DRR-u. In DRR-u, we\\ncan see that the user embedding is also incorporated.\\nIn addition to the local dependency between items, the\\npairwise interactions of user-item are also taken into\\naccount. Formally, the state representation module can\\nbe expressed as:\\ns= [{u⊗waia|a= 1,...,n},{pa,b|a,b= 1,...,n}](7)\\nThe dimensionality of sis alsok(n+n(n−1)/2).\\nItemsS…Concat& flattenXXXuserXXXXX\\nFig. 5. DRR-u Structure\\n•DRR-ave . In DRR-p and DRR-u structures, the inter-\\nactions between users and items can be exploited and\\nmodeled. For the two structures, it is not difﬁcult to ﬁnd\\nthat the positions of items in Hmatters, e.g., the state\\nrepresentations of H1={ia,ib,ic}andH2={ic,ib,ia}\\n3The legend in Figure 4, 5 and 6 is the same to Figure 3', metadata={'source': 'DRR-framework.pdf', 'page': 4}),\n",
       " Document(page_content='are different. When His large, we expect the positions\\nof items really matter, because Hdenotes a long-term\\nsequence; whereas memorizing the positions of items\\nmay lead to overﬁtting if the sequence His a short-term\\none. Hence, we design another structure by eliminating\\nthe position effects, which is depicted in Figure 6. As an\\naverage pooling layer is adopted, we call the structure\\nDRR-ave. We can see from Figure 6 that the embeddings\\nof items inHare ﬁrst transformed by a weighted average\\npooling layer. Then, the resulting vector is leveraged to\\nmodel the interactions with the input user. Finally, the\\nembedding of the user, the interaction vector, and the\\naverage pooling result of items are concatenate into a\\nvector to denote the state representation. Formally, the\\nDRR-ave structure can be expressed as:\\ns= [u,u⊗{g(ia)|a= 1,...,n},{g(ia)|a= 1,...,n}]\\n(8)\\ng(ia) =ave(waia)|a= 1,...,n (9)\\nHereg(·)indicates the weighted average pooling layer.\\nThe dimensionality of sin DRR-ave is 3k.\\nItemsSConcat& flattenXuser\\nFig. 6. DRR-ave Structure\\nB. Training Procedure of the DRR Framework\\nNext, we introduce how to train the DRR framework. We\\nﬁrst present the overall idea and then discuss the detailed\\nalgorithm. As aforementioned, DRR utilizes the users’ inter-\\naction history with the recommender agent as training data.\\nDuring the procedure, the recommender takes an action at\\nfollowing the current recommendation policy πθ(st)after\\nobserving the user (environment) state st, then it obtains the\\nfeedback (reward) rtfrom the user, and the user state is\\nupdated tost+1. According to the feedback, the recommender\\nupdates its recommendation policy. In this work, we utilize\\ndeep deterministic policy gradient (DDPG) [35] algorithm to\\ntrain the proposed DRR framework, as detailed in Algorithm\\n1.\\nSpeciﬁcally, in timestep t, the training procedure mainly\\nincludes two phases, i.e., transition generation (lines 7-12)\\nand model updating (lines 13-17). For the ﬁrst stage, the\\nrecommender observes the current state stthat is calculated\\nby the proposed state representation module, then generates\\nan actionat=πθ(st)according to the current policy πθwithε-greedy exploration, and recommends an item itaccording to\\nthe actionatby Eq. (2) (lines 8-9). Subsequently, the reward\\nrtcan be calculated based on the feedback of the user to\\nthe recommended item it, and the user state is updated (lines\\n10-11). Finally, the recommender agent stores the transition\\n(st,at,rt,st+1)into the replay buffer D(line 12).\\nIn the second stage, the model updating, the recommender\\nsamples a minibatch of Ntransitions with widely used pri-\\noritized experience replay [36] sampling technique (line 13),\\nwhich is essentially an importance sampling strategy. Then, the\\nrecommender updates the parameters of the Actor network and\\nCritic network according to Eq. (3) and Eq. (4) respectively\\n(line 14-16). Finally, the recommender updates the target\\nnetworks’ parameters with the soft replace strategy.\\nAlgorithm 1: Training Algorithm of DRR Framework\\ninput : Actor learning rate ηa, Critic learning rate ηc,\\ndiscount factor γ, batch size N, state window\\nsizenand reward function R\\n1Randomly initialize the Actor πθand the Critic Qωwith\\nparametersθandω\\n2Initialize the target network π′andQ′with weights\\nθ′←θandω′←ω\\n3Initialize replay buffer D\\n4forsession = 1, M do\\n5 Observe the initial state s0according to the ofﬂine\\nlog\\n6 for t = 1, T do\\n7 Observe current state st=f(Ht), where\\nHt={i1,...,in}\\n8 Find action at=πθ(st)according to the current\\npolicy with ε-greedy exploration\\n9 Recommended item itaccording to action atby\\nEq. (2)\\n10 Calculate reward rt=R(st,at)based on the\\nfeedback of the user\\n11 Observe new state st+1=f(Ht+1), where\\nHt+1={i2,...,in,it}ifrtis positive,\\notherwise,Ht+1=Ht\\n12 Store transition (st,at,rt,st+1)inD\\n13 Sample a minibatch of Ntransitions\\n(si,ai,ri,si+1)inDwith prioritized experience\\nreplay sampling technique\\n14 Setyi=ri+γQω′(si+1,πθ′(si+1))\\n15 Update the Critic network by minimizing the\\nloss:L=1\\nN∑\\ni(yi−Qω(si,ai))2\\n16 Update the Actor network using the sampled\\npolicy gradient:\\n∇θJ(πθ)≈\\n1\\nN∑\\nt∇aQω(s,a)|s=st,a=πθ(st)∇θπθ(s)|s=st\\n17 Update the target networks:\\nθ′←τθ+ (1−τ)θ′\\nω′←τω+ (1−τ)ω′\\n18returnθandω', metadata={'source': 'DRR-framework.pdf', 'page': 5}),\n",
       " Document(page_content='C. Evaluation\\nIn this subsection, we discuss how to evaluate the models\\nwith a environment simulator. The most straightforward way to\\nevaluate the RL based models is to conduct online experiments\\non recommender systems where the recommender directly\\ninteracts with users. However, the underlying commercial risk\\nand the costly deployment on the platform make it impracti-\\ncal. Therefore, throughout the testing phase, we conduct the\\nevaluation of the proposed models on public ofﬂine datasets\\nand propose two ways to evaluate the models, which are the\\nofﬂine evaluation and the online evaluation.\\n1) Ofﬂine evaluation: Intuitively, the ofﬂine evaluation of\\nthe trained models is to test the recommendation performance\\nwith the learned policy, which is described in Algorithm 2.\\nSpeciﬁcally, for a given session Sj, the recommender only\\nrecommends the items that appear in this session, denoted as\\nI(Sj), rather than the ones in the whole item space. The reason\\nis that we only have the ground truth feedback for the items in\\nthe session in the recoreded ofﬂine log. For each timestep, the\\nrecommender agent takes an action ataccording to the learned\\npolicyπθ, and recommends an item it∈I(Sj)based on the\\nactionatby Eq. (2) (lines 4-5). After that, the recommender\\nobserves the reward rt=R(st,at)according to the feedback\\nof the recommended item itby Eq. (10) (lines 5-6). Then the\\nuser state is updated to st+1and the recommended item itis\\nremoved from the candidate set I(Sj)(lines 7-8). The ofﬂine\\nevaluation procedure can be treated as a rerank procedure of\\nthe candidate set by iteratively selecting an item w.r.t. the\\naction generated by the Actor network in DRR framework.\\nMoreover, the model parameters are not updated in the ofﬂine\\nevaluation.\\nAlgorithm 2: Ofﬂine Evaluation Algorithm of DRR\\nFramework\\ninput : state window size nand reward function R\\n1Observe the initial state s0and item setIaccording to\\nthe ofﬂine log\\n2for t = 1, T do\\n3 Observe current state st={i1,...,in}\\n4 Execute action at=πθ(st)according to the current\\npolicy\\n5 Observe the recommended item itaccording to\\nactionatby Eq. (2)\\n6 Get rewardrt=R(st,at)from the feedback located\\nin the users’ log by Eq. (10)\\n7 Update to a new state st+1=f(Ht+1), where\\nHt+1={i2,...,in,it}ifrtis positive, otherwise,\\nHt+1=Ht\\n8 removeitfromI\\n2) Online evaluation with environment simulator: As afore-\\nmentioned that it is risky and costly to directly deploy the\\nRL based models on recommender systems. Therefore, we\\nconduct online evaluation with an environment simulator. In\\nthis paper, we pretrain a PMF [37] model as the environmentsimulator, i.e., to predict an item’s feedback that the user\\nnever rates before. The online evaluation procedure follows\\nAlgorithm 1, i.e., the parameters continuously update dur-\\ning the online evaluation stage. Its major difference from\\nAlgorithm 1 is that the feedback of a recommended item\\nis observed by the environment simulator. Moreover, before\\neach recommendation session starting in the simulated online\\nevaluation, we reset the parameters back to θandωwhich is\\nthe policy learned in the training stage for a fair comparison.\\nV. E XPERIMENT\\nA. Datasets and Evaluation Metrics\\nWe adopt the following publicly available datasets from the\\nreal world to conduct the experiments:\\n•MovieLens (100k)4. A benchmark dataset comprises\\nof 0.1 million ratings from users to the recommended\\nmovies on MovieLens website.\\n•Yahoo! Music (R3)5. This dataset contains over 0.36\\nmillion ratings of songs collected from two different\\nsources. The ﬁrst source consists of ratings provided\\nby users during normal interactions with Yahoo! Music\\nservices. The second source consists of ratings of ran-\\ndomly selected songs collected during an online survey\\nby Yahoo! Research. We normalize the ratings to discrete\\nvalues from 1 to 5.\\n•MovieLens (1M)6. A benchmark dataset includes of 1\\nmillion ratings from the MovieLens website.\\n•Jester (2)7. This dataset contains over 1.7 million real-\\nvalue ratings (-10.0 to +10.0) over jokes in an online joke\\nrecommender system.\\nNote that except for Jester, the ratings in the other datasets\\nare discrete values from 1 to 5, and the statistic information\\nof the datasets is given in Table I. The MovieLens (100k) and\\nMovieLens (1M) are abbreviated as ML (100k) and ML (1M)\\nrespectively.\\nTABLE I\\nSTATISTIC INFORMATION OF THE DATASETS\\nML (100k) Yahoo! Music ML (1M) Jester\\n# user 943 15,400 6,040 63,978\\n# item 1,682 1,000 3,952 150\\n# ratings 100,000 365,740 1,000,209 1,761,439\\nWe conduct both ofﬂine and simulated online evaluation\\non these four datasets. For the ofﬂine evaluation, we utilize\\nPrecision@k and NDCG@k as the metrics to measure the\\nperformance of the proposed models. For the simulated online\\nevaluation, we leverage the total accumulated rewards as the\\nmetric.\\n4https://grouplens.org/datasets/movielens/100k/\\n5https://webscope.sandbox.yahoo.com/\\n6https://grouplens.org/datasets/movielens/1m/\\n7http://eigentaste.berkeley.edu/dataset/', metadata={'source': 'DRR-framework.pdf', 'page': 6}),\n",
       " Document(page_content='B. Compared Methods\\nWe compare the proposed methods with some representative\\nbaseline methods. For the ofﬂine evaluation, we compare to\\nconventional methods including Popularity, PMF [37] and\\nSVD++ [38], and a RL based method DRR-n. Moreover, the\\nonline evaluation baselines contain the state-of-the-art multi-\\narmed bandits methods LinUCB [39] and HLinUCB [40] and\\nthe DRR-n as well.\\n•Popularity recommends the most popular item, i.e., the\\nitem with the highest average rating or the items with\\nlargest number of positive ratings8from current available\\nitems to the users at each timestep.\\n•PMF makes a matrix decomposition as SVD, while it\\nonly takes into account the non zero elements.\\n•SVD++ mixes strengths of the latent model as well as\\nthe neighborhood model.\\n•LinUCB selects an arm (item) according to the estimated\\nupper conﬁdence bound of the potential reward.\\n•HLinUCB further learns hidden features for each arm to\\nmodel the potential reward.\\n•DRR-n simply utilizes the concatenation of the item\\nembeddings to represent user state, which is widely\\nused in previous studies. Although it is under the DRR\\nframework, we treat this method as a baseline to assess\\nthe effectiveness of our proposed state representation\\nmodule.\\nC. Experimental Settings\\nFor each dataset, we choose 80% of the interactions in\\neach user session as the training set, and leave the rest as the\\ntesting set. Moreover, for MovieLens (100k), Yahoo! Music\\nand MovieLens (1M), the positive ratings are 4and5, while for\\nJester, the positive ones are those higher than 0. The number\\nof latest positively rated items n, which is empirically set to 5.\\nWe perform PMF to pretrain the 100-dimensional embeddings\\nof the users and items. Moreover, in each episode, we do not\\nrecommend repeated items, i.e., we remove the ones already\\nrecommended from the candidate set. The discount rate γis\\n0.9. We utilize Adam optimizer for all the RL based methods\\nwithL2-norm regularization to prevent overﬁtting. As for the\\nreward function, we empirically normalize the ratings into\\nrange [-1 ,1] and utilize the normalized ones as the feedback of\\nthe corresponding recommendations. For instance, in timestep\\nt, the recommender agent recommends an item jto useri,\\n(denoted as action ain states), and the rating ratei,jcomes\\nfrom the interaction logs if user iactually rates item j, or\\nfrom a predicted value by the simulator otherwise. Therefore,\\nthe reward function can be deﬁned as follows:\\nR(s,a) =1\\n2(ratei,j−3)\\nR(s,a) =ratei,j/10(10)\\n8To get a better result of popularity based recommendation, we both test\\nthe two strategies, and choose the best one to report.where the ﬁrst setting is for MovieLens (100k), Yahoo! Music\\nand MovieLens (1M), and the second one is for Jester. All the\\nbaseline methods are carefully tuned for a fair comparison.\\nWe model the recommendation procedure as an interaction\\nepisode with length T, and the hyper-parameter Tis tuned\\nfor different datasets (detailed in Section V .E).\\nD. Results and Analysis\\n1) Ofﬂine Evaluation Results and Analysis: The ofﬂine\\nevaluation results are summarized from Table II to Table V\\nrespectively, where the best results are marked in bold type.\\nIn the ofﬂine evaluation, we compare the proposed methods\\nto some representative ofﬂine learning methods. The results\\nsuggest that the proposed methods under the DRR framework\\noutperform the baselines on most of datasets, which demon-\\nstrates the effectiveness of our proposed methods.\\nSpeciﬁcally, as aforementioned that, we propose three dif-\\nferent network structure in the state representation module to\\nmodel the explicit interactions of the users and items under the\\nDRR framework, which are the DRR-p, DRR-u and DRR-\\nave. From the results in Table II to Table V, we ﬁnd that\\nthe three methods all outperform the baselines in most cases.\\nMoreover, DRR-n that simply concats the item embeddings\\nto represent the state s, performs worse than the proposed\\nDRR-p, DRR-u and DRR-ave. From the observations, we\\ncan conclude in two folds: (i) the proposed methods indeed\\nhave the capability of long-term scheduling and dynamic\\nadaptation, which are ignored by conventional methods; (ii)\\nthe proposed state representation module well captures the\\ndynamic interactions between the users and items, and the\\nstate should not be simply concatenate with fully connected\\nlayers as DRR-n does, which may result in information loss.\\nCompared with DRR-p, DRR-u and DRR-ave, we can see\\nthat DRR-ave outperforms DRR-u, and DRR-u is superior than\\nDRR-p on the four datasets in most cases. The reasons are as\\nfollows: 1) The DRR-u method has better performance than\\nDRR-p, because DRR-u only captures the interactions of user’s\\nhistorical items, but also seizes the personalization information\\nthrough the user-item interactions. 2) DRR-ave performs the\\nbest, because of two reasons: (i) DRR-ave method captures\\nthe personalization information through user-item interactions;\\n(ii) as noted in Section IV , by using the average pooling, it\\neliminates the position effects in H.\\nTABLE II\\nPERFORMANCE COMPARISON OF ALL METHODS ON ML (100 K)DATASET .\\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\\nPopularity 0.6933 0.6012 0.9104 0.9008\\nPMF 0.6988 0.6194 0.9095 0.8968\\nSVD++ 0.7034 0.6255 0.9125 0.8991\\nDRR-n 0.7185 0.6387 0.9147 0.9004\\nDRR-p 0.7263 0.6448 0.9076 0.9015\\nDRR-u 0.7417 0.6536 0.9183 0.9062\\nDRR-ave 0.7887 0.6935 0.9255 0.9046', metadata={'source': 'DRR-framework.pdf', 'page': 7}),\n",
       " Document(page_content='TABLE III\\nPERFORMANCE COMPARISON OF ALL METHODS ON YAHOO ! M USIC\\nDATASET .\\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\\nPopularity 0.3826 0.3805 0.8870 0.8811\\nPMF 0.3835 0.3817 0.8837 0.8802\\nSVD++ 0.3857 0.3821 0.8887 0.8813\\nDRR-n 0.3844 0.3819 0.8876 0.8810\\nDRR-p 0.3850 0.3822 0.8883 0.8815\\nDRR-u 0.3864 0.3827 0.8889 0.8819\\nDRR-ave 0.3917 0.3839 0.9004 0.8949\\nTABLE IV\\nPERFORMANCE COMPARISON OF ALL METHODS ON ML (1M) DATASET .\\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\\nPopularity 0.7141 0.6181 0.8906 0.8738\\nPMF 0.7072 0.6193 0.8901 0.8746\\nSVD++ 0.7142 0.6258 0.9009 0.8776\\nDRR-n 0.7151 0.6221 0.8902 0.8751\\nDRR-p 0.7346 0.6366 0.8909 0.8753\\nDRR-u 0.7375 0.6385 0.8912 0.8763\\nDRR-ave 0.7693 0.6594 0.9112 0.8980\\n2) Simulated online evaluation results and analysis: The\\nresults of the simulated online evaluation are summarized in\\nTable VI, where the best results are marked in bold type. In the\\nexperiment, we only compare with the baseline methods that\\ncan perform online learning, which are LinUCB, HLinUCB\\nand DRR-n. Again, we ﬁnd that the proposed methods deliver\\nhigher rewards than all the baselines.\\nOn the one hand, the fact suggests that the proposed\\nRL-based methods model dynamic adaptation and long-term\\nrewards better than the multi-armed bandits based methods\\nLinUCB and HLinUCB. On the other hand, the observation\\nindicates that the proposed state representation structures are\\nsuperior to the naive full-connected network in DRR-n. Again,\\nwe observe that DRR-ave performs the best among all the three\\nproposed interaction modeling structures.\\nE. Parameter Study\\nIn this subsection, we investigate how the episode length\\nTaffect the performance of proposed methods. Figure 7\\nshows the results9. From the left part of Figure 7, we observe\\nthat the performance on MovieLens ﬁrst increases and then\\ndecreases as the length of the episode is gradually increased,\\nand the summit appears at T= 10 . A similar tend can be\\nfound for the Yahoo! Music from the right part of Figure 7,\\nwhere the performance peaks at T= 20 . The reason may\\ndue to the trade-off between the exploitation and exploration.\\nWhen the episode length is small, the user can not fully\\ninteract with the recommender agent, i.e., the exploration is\\ninsufﬁcient. As we enlarge the episodes, the recommender\\n9Due to the space limit, We only present the performance of DRR-ave,\\nwhile DRR-p and DRR-u have similar observationsTABLE V\\nPERFORMANCE COMPARISON OF ALL METHODS ON JESTER DATASET .\\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\\nPopularity 0.6167 0.6012 0.8932 0.8703\\nPMF 0.6171 0.6015 0.8740 0.8676\\nSVD++ 0.6184 0.6027 0.8819 0.8614\\nDRR-n 0.6178 0.6021 0.8915 0.8724\\nDRR-p 0.6181 0.6029 0.8934 0.8753\\nDRR-u 0.6217 0.6043 0.8974 0.8805\\nDRR-ave 0.6278 0.6076 0.9124 0.9079\\nTABLE VI\\nTHE REWARDS OF ALL METHODS ON THE FOUR DATASETS .\\nModel ML (100k) Yahoo! Music ML (1M) Jester\\nLinUCB 1,958 30,462.5 30,174 141,358.4\\nHLinUCB 1,475 32,725 32,785.5 147,105.5\\nDRR-n 2,654.5 35,382.5 35,860 165,844.5\\nDRR-p 2,832 37,328.5 36,653 177,414.2\\nDRR-u 2,869 42,174.5 37,615 183,517.6\\nDRR-ave 3,251.5 49,095 40,588 194,860.7\\nagent can explore (interact with users) adequately, i.e., the\\nrecommender agent captures the user’s preference, so that the\\nperformance improves. However, if the episodes are too large,\\nthe recommender focuses on exploiting locally, but the user\\npreferred items is limited, therefore the performance declines\\nas we do not recommend repeated items to user. Hence, we\\nshould nicely trade off the exploration and exploitation by\\nsetting a suitable value for T.\\nFig. 7. Parameter study on episode length Tin MovieLens and Yahoo!Music\\ndatasets\\nF . Case Study\\nIn this subsection, we present an example to show the\\ndifferent recommendation manner between LinUCB and DRR-\\nave on MovieLens dataset. Speciﬁcally, we randomly pick up\\na user with ID 11, and conduct the recommendation procedure\\nwith LinUCB and DRR-ave respectively. To verify the reaction\\nto the same recommendation scenario, we ﬁx the ﬁrst three\\nrecommended items and to see what will happen next. The\\nresults of recommended item and the reward are reported in\\nTable VII.\\nFrom Table VII, we can see that LinUCB and DRR-ave\\nreact differently when given two consecutive negative recom-\\nmendations (Eraser and First Knight). Speciﬁcally, LinUCB', metadata={'source': 'DRR-framework.pdf', 'page': 8}),\n",
       " Document(page_content='keeps exploring without considering to recommend a “safe”\\nitem to please the user. However, DRR-ave stops exploration\\nand recommends a risk-free movie Dead Man Walking, which\\nbelongs to the same genre as Chasing Amy that has gained a\\npositive feedback from the user at timestep 1. The observation\\ndemonstrates the superiority of the proposed DRR-ave against\\nLinUCB.\\nTABLE VII\\nDIFFERENT RECOMMENDATION MANNER BETWEEN LINUCB AND\\nDRR- AVE ON MOVIE LENS. (T HE VALUE IN (·)DENOTES THE\\nCORRESPONDING REWARD .)\\ntimestep LinUCB DRR-ave\\n1 Chasing Amy (1) Chasing Amy (1)\\n2 Eraser (-0.5) Eraser (-0.5)\\n3 First Knight (-1) First Knight (-1)\\n4 The Deer Hunter (-0.5) Dead Man Walking (1)\\n5 Event Horizon (-1) Braveheart (0.5)\\n6 The Net (0) The Usual Suspect (-0.5)\\n7 Striptease (-0.5) Psycho (0.5)\\nVI. C ONCLUSION\\nIn this paper, we propose a deep reinforcement learning\\nbased framework DRR to perform the recommendation task.\\nUnlike the conventional studies, DRR treats the recommen-\\ndation as a sequential decision making process and adopts\\nan “Actor-Critic” learning scheme, which can take both the\\nimmediate and long-term rewards into account. In DRR, a\\nstate representation module is incorporated and three instanti-\\nation structures are designed, which can explicitly model the\\ninteractions between users and items. Extensive experiments\\non four real-world datasets demonstrate the superiority of the\\nproposed DRR method over state-of-the-art competitors.\\nREFERENCES\\n[1] R. J. Mooney and L. Roy, “Content-based book recommending using\\nlearning for text categorization,” in ACM DL , 2000, pp. 195–204.\\n[2] M. Deshpande and G. Karypis, “Item-based top- Nrecommendation\\nalgorithms,” ACM Trans. Inf. Syst. , vol. 22, no. 1, pp. 143–177, 2004.\\n[3] Y . Koren, R. M. Bell, and C. V olinsky, “Matrix factorization techniques\\nfor recommender systems,” IEEE Computer , vol. 42, no. 8, pp. 30–37,\\n2009.\\n[4] G. Linden, B. Smith, and J. York, “Amazon.com recommendations:\\nItem-to-item collaborative ﬁltering,” IEEE Internet Computing , vol. 7,\\nno. 1, pp. 76–80, 2003.\\n[5] J. Wang, A. P. De Vries, and M. J. Reinders, “Unifying user-based and\\nitem-based collaborative ﬁltering approaches by similarity fusion,” in\\nSIGIR . ACM, 2006, pp. 501–508.\\n[6] H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady,\\nL. Nie, T. Phillips, E. Davydov, D. Golovin, S. Chikkerur, D. Liu,\\nM. Wattenberg, A. M. Hrafnkelsson, T. Boulos, and J. Kubica, “Ad\\nclick prediction: a view from the trenches,” in KDD 2013, Chicago, IL,\\nUSA, August 11-14, 2013 , 2013, pp. 1222–1230.\\n[7] S. Rendle, “Factorization machines,” in ICDM, Sydney, Australia, 14-17\\nDecember 2010 , 2010, pp. 995–1000.\\n[8] Y . Juan, Y . Zhuang, W. Chin, and C. Lin, “Field-aware factorization\\nmachines for CTR prediction,” in RecSys, Boston, MA, USA, September\\n15-19, 2016 , 2016, pp. 43–50.\\n[9] W. Zhang, T. Du, and J. Wang, “Deep learning over multi-ﬁeld categor-\\nical data - - A case study on user response prediction,” in ECIR 2016,\\nPadua, Italy, March 20-23, 2016. Proceedings , 2016, pp. 45–57.[10] Y . Qu, H. Cai, K. Ren, W. Zhang, Y . Yu, Y . Wen, and J. Wang, “Product-\\nbased neural networks for user response prediction,” in ICDM 2016,\\nDecember 12-15, 2016, Barcelona, Spain , 2016, pp. 1149–1154.\\n[11] H. Guo, R. Tang, Y . Ye, Z. Li, and X. He, “Deepfm: A factorization-\\nmachine based neural network for CTR prediction,” in IJCAI 2017,\\nMelbourne, Australia, August 19-25, 2017 , 2017, pp. 1725–1731.\\n[12] H. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye,\\nG. Anderson, G. Corrado, W. Chai, M. Ispir, R. Anil, Z. Haque, L. Hong,\\nV . Jain, X. Liu, and H. Shah, “Wide & deep learning for recommender\\nsystems,” CoRR , vol. abs/1606.07792, 2016.\\n[13] O. Chapelle and L. Li, “An empirical evaluation of thompson sampling,”\\ninNIPS, Granada, Spain. , 2011, pp. 2249–2257.\\n[14] L. Li, W. Chu, J. Langford, and R. E. Schapire, “A contextual-bandit\\napproach to personalized news article recommendation,” in WWW 2010,\\nRaleigh, North Carolina, USA, April 26-30, 2010 , 2010, pp. 661–670.\\n[15] H. Wang, Q. Wu, and H. Wang, “Factorization bandits for interactive\\nrecommendation,” in AAAI, February 4-9, 2017, San Francisco, Cali-\\nfornia, USA. , 2017, pp. 2695–2702.\\n[16] C. Zeng, Q. Wang, S. Mokhtari, and T. Li, “Online context-aware\\nrecommendation with time varying multi-armed bandit,” in SIGKDD\\n, San Francisco, CA, USA, August 13-17, 2016 , 2016, pp. 2025–2034.\\n[17] X. Zhao, W. Zhang, and J. Wang, “Interactive collaborative ﬁltering,” in\\nCIKM’13, San Francisco, CA, USA, October 27 - November 1, 2013 ,\\n2013, pp. 1411–1420.\\n[18] G. Shani, D. Heckerman, and R. I. Brafman, “An mdp-based recom-\\nmender system,” Journal of Machine Learning Research , vol. 6, pp.\\n1265–1295, 2005.\\n[19] G. Zheng, F. Zhang, Z. Zheng, Y . Xiang, N. J. Yuan, X. Xie, and\\nZ. Li, “DRN: A deep reinforcement learning framework for news\\nrecommendation,” in WWW 2018, Lyon, France, April 23-27, 2018 ,\\n2018, pp. 167–176.\\n[20] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction .\\nMIT press Cambridge, 1998, vol. 1, no. 1.\\n[21] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\\nBellemare, A. Graves, M. A. Riedmiller, A. Fidjeland, G. Ostrovski,\\nS. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\\nD. Wierstra, S. Legg, and D. Hassabis, “Human-level control through\\ndeep reinforcement learning,” Nature , vol. 518, no. 7540, pp. 529–533,\\n2015.\\n[22] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den\\nDriessche, J. Schrittwieser, I. Antonoglou, V . Panneershelvam, M. Lanc-\\ntot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever,\\nT. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis,\\n“Mastering the game of go with deep neural networks and tree search,”\\nNature , vol. 529, no. 7587, pp. 484–489, 2016.\\n[23] H. Cai, K. Ren, W. Zhang, K. Malialis, J. Wang, Y . Yu, and D. Guo,\\n“Real-time bidding by reinforcement learning in display advertising,” in\\nWSDM 2017, Cambridge, United Kingdom, February 6-10, 2017 , 2017,\\npp. 661–670.\\n[24] J. Jin, C. Song, H. Li, K. Gai, J. Wang, and W. Zhang, “Real-time\\nbidding with multi-agent reinforcement learning in display advertising,”\\nCoRR , vol. abs/1802.09756, 2018.\\n[25] H. Cai, T. Chen, W. Zhang, Y . Yu, and J. Wang, “Efﬁcient architecture\\nsearch by network transformation,” in AAAI , New Orleans, Louisiana,\\nUSA, February 2-7, 2018 , 2018.\\n[26] B. Zoph and Q. V . Le, “Neural architecture search with reinforcement\\nlearning,” CoRR , vol. abs/1611.01578, 2016.\\n[27] N. Taghipour and A. A. Kardan, “A hybrid web recommender system\\nbased on q-learning,” in Proceedings of the 2008 ACM Symposium on\\nApplied Computing (SAC), Fortaleza, Ceara, Brazil, March 16-20, 2008 ,\\n2008, pp. 1164–1168.\\n[28] X. Zhao, L. Zhang, Z. Ding, D. Yin, Y . Zhao, and J. Tang, “Deep\\nreinforcement learning for list-wise recommendations,” CoRR , vol.\\nabs/1801.00209, 2018.\\n[29] X. Zhao, L. Zhang, Z. Ding, L. Xia, J. Tang, and D. Yin, “Recommenda-\\ntions with negative feedback via pairwise deep reinforcement learning,”\\nCoRR , vol. abs/1802.06501, 2018.\\n[30] L. Xia, J. Xu, Y . Lan, J. Guo, W. Zeng, and X. Cheng, “Adapting markov\\ndecision process for search result diversiﬁcation,” in SIGIR , Shinjuku,\\nTokyo, Japan, August 7-11, 2017 , 2017, pp. 535–544.\\n[31] Z. Wei, J. Xu, Y . Lan, J. Guo, and X. Cheng, “Reinforcement learning to\\nrank with markov decision process,” in SIGIR , Shinjuku, Tokyo, Japan,\\nAugust 7-11, 2017 , 2017, pp. 945–948.', metadata={'source': 'DRR-framework.pdf', 'page': 9}),\n",
       " Document(page_content='[32] Y . Hu, Q. Da, A. Zeng, Y . Yu, and Y . Xu, “Reinforcement learning\\nto rank in e-commerce search engine: Formalization, analysis, and\\napplication,” CoRR , vol. abs/1803.00710, 2018.\\n[33] G. Dulac-Arnold, R. Evans, P. Sunehag, and B. Coppin, “Reinforcement\\nlearning in large discrete action spaces,” CoRR , vol. abs/1512.07679,\\n2015.\\n[34] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. A.\\nRiedmiller, “Deterministic policy gradient algorithms,” in ICML 2014,\\nBeijing, China, 21-26 June 2014 , 2014, pp. 387–395.\\n[35] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa,\\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\\nlearning,” CoRR , vol. abs/1509.02971, 2015.\\n[36] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience\\nreplay,” arXiv preprint arXiv:1511.05952 , 2015.\\n[37] A. Mnih and R. R. Salakhutdinov, “Probabilistic matrix factorization,”\\ninNIPS , 2008, pp. 1257–1264.\\n[38] Y . Koren, “Factorization meets the neighborhood: a multifaceted collab-\\norative ﬁltering model,” in KDD . ACM, 2008, pp. 426–434.\\n[39] L. Li, W. Chu, J. Langford, and R. E. Schapire, “A contextual-bandit\\napproach to personalized news article recommendation,” in WWW .\\nACM, 2010, pp. 661–670.\\n[40] H. Wang, Q. Wu, and H. Wang, “Learning hidden features for contextual\\nbandits,” in CIKM . ACM, 2016, pp. 1633–1642.', metadata={'source': 'DRR-framework.pdf', 'page': 10})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drr_framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que basicamente aconteceu foi: nós carregamos o artigo, lemos cada página e criamos objetos `Document` com as informações e metadados de cada página do artigo.\n",
    "\n",
    "Se você dar uma olhada no artigo, poderá ver que ele tem exatamente 11 páginas totais. Podemos ver isso também pelo tamanho da lista de documentos `drr_framework`, uma vez que cada objeto foi criado a partir de cada página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(drr_framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para analisarmos o conteúdo existente em cada página, basta extrair o atributo `page_content` do objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep Reinforcement Learning based\\nRecommendation with Explicit User-Item\\nInteractions Modeling\\nFeng Liu∗, Ruiming Tang†, Xutao Li∗, Weinan Zhang‡Yunming Ye∗, Haokun Chen‡, Huifeng Guo†and Yuzhou Zhang†\\n∗Shenzhen Key Laboratory of Internet Information Collaboration\\nShenzhen Graduate School, Harbin Institute of Technology, Shenzhen, 518055, China\\nEmail: fengliu@stu.hit.edu.cn, lixutao@hit.edu.cn, yeyunming@hit.edu.cn\\n†Noah’s Ark Lab, Huawei, China\\nEmail: tangruiming, huifeng.guo, zhangyuzhou3@huawei.com\\n‡Shanghai Jiao Tong University, Shanghai, China\\nEmail: wnzhang@sjtu.edu.cn, chenhaokun@sjtu.edu.cn\\nAbstract —Recommendation is crucial in both academia and\\nindustry, and various techniques are proposed such as content-\\nbased collaborative ﬁltering, matrix factorization, logistic re-\\ngression, factorization machines, neural networks and multi-\\narmed bandits. However, most of the previous studies suffer\\nfrom two limitations: (1) considering the recommendation as\\na static procedure and ignoring the dynamic interactive nature\\nbetween users and the recommender systems; (2) focusing on the\\nimmediate feedback of recommended items and neglecting the\\nlong-term rewards. To address the two limitations, in this paper\\nwe propose a novel recommendation framework based on deep\\nreinforcement learning, called DRR. The DRR framework treats\\nrecommendation as a sequential decision making procedure and\\nadopts an “Actor-Critic” reinforcement learning scheme to model\\nthe interactions between the users and recommender systems,\\nwhich can consider both the dynamic adaptation and long-\\nterm rewards. Further more, a state representation module is\\nincorporated into DRR, which can explicitly capture the interac-\\ntions between items and users. Three instantiation structures are\\ndeveloped. Extensive experiments on four real-world datasets are\\nconducted under both the ofﬂine and online evaluation settings.\\nThe experimental results demonstrate the proposed DRR method\\nindeed outperforms the state-of-the-art competitors.\\nIndex Terms —Recommendation, Deep Reinforcement Learn-\\ning, User-Item Interactions\\nI. I NTRODUCTION\\nThanks to the increasing online services, such as online\\nshopping, online news and online social networks, it becomes\\nquite convenient to acquire items (goods, books, videos,\\nnews, etc.) via Internet or mobile devices. Albeit the great\\nconvenience, the overwhelming number of items in the sys-\\ntems also pose a signiﬁcant challenge for users, to ﬁnd the\\nitems that match their interests. Recommendation is a widely\\nused solution and various families of techniques have been\\nproposed, such as content-based collaborative ﬁltering [1],\\nmatrix factorization based methods [2]–[5], logistic regression,\\nfactorization machines and its variants [6]–[8], deep learning\\nmodels [9]–[12] and multi-armed bandits [13]–[17]. However,\\nsuch mentioned studies suffer from two serious limitations.Firstly , most of them consider the recommendation proce-\\ndure as a static process, i.e., they assume the user’s underlying\\npreference keeps unchanged. However, it is very common\\nthat a user’s preference is dynamic w.r.t. time, i.e., a user’s\\npreference on previous items will affect her choice on the next\\nitems. Hence, it would be more reasonable to model the recom-\\nmendation as a sequential decision making process. We will\\nshow some evidence observed in publicly available datasets\\n(MovieLens and Yahoo! Music) to support our opinion. In the\\ntwo datasets, the sequential behaviors of users are recorded and\\nwe are interested in what would happen if a user consecutively\\nreceives satisﬁed or unsatisﬁed recommendations. Though the\\ndatasets do not record any recommendation procedure, we\\ncan simulate this according to the users’ ratings, namely,\\nconsecutive rating “positive” (“negative”) simulates that a user\\nconsecutively receives satisﬁed (unsatisﬁed) recommendations.\\nAs presented in Figure 1, we observe that a user tends\\nto gives a higher (lower) rating if she has consecutively\\nreceived more satisﬁed (unsatisﬁed) items, as shown by the\\ngreen (red) line, where the blue dot line denotes the average\\nrating for reference. This suggests that a user will be more\\npleasant (unpleasant) if she consecutively receives more sat-\\nisﬁed (unsatisﬁed) recommendations and therefore she tends\\nto give a higher (lower) rating to the current recommendation.\\nHence, the user’s dynamic preference suggests that a good\\nrecommendation should be modeled as a sequential decision\\nmaking process.\\nSecondly , the aforementioned studies are trained by max-\\nimizing the immediate rewards of recommendations, which\\nmerely concentrates on whether the recommended items are\\nclicked or consumed, but ignores the long-term contributions\\nthat the items can make. However, the items with small imme-\\ndiate rewards but large long-term beneﬁts are also crucial [18].\\nWe take an example in News recommendation [19] to explain\\nthis. As a user requests for news to read, two possible pieces of\\nnews may lead to the same immediate reward, i.e., the user willarXiv:1810.12027v3  [cs.IR]  29 Oct 2019'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drr_framework[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meu objetivo agora é dividir todo o conteúdo do artigo em pedaços, isto é, `chunks`. Para isso, vamos utilizar um dos `Text Splliters` do LangChain, sendo mais específico o `RecursiveCharacterTextSplitter`, que é um divisor de textos por caractere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro tomamos todo o conteúdo presente no artigo\n",
    "content_drr_framework = ''.join([doc.page_content for doc in drr_framework])\n",
    "\n",
    "# Configuramos o text_splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Criamos os chunks\n",
    "texts = text_splitter.create_documents([content_drr_framework])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Deep Reinforcement Learning based\\nRecommendation with Explicit User-Item\\nInteractions Modeling\\nFeng Liu∗, Ruiming Tang†, Xutao Li∗, Weinan Zhang‡Yunming Ye∗, Haokun Chen‡, Huifeng Guo†and Yuzhou Zhang†\\n∗Shenzhen Key Laboratory of Internet Information Collaboration\\nShenzhen Graduate School, Harbin Institute of Technology, Shenzhen, 518055, China\\nEmail: fengliu@stu.hit.edu.cn, lixutao@hit.edu.cn, yeyunming@hit.edu.cn\\n†Noah’s Ark Lab, Huawei, China\\nEmail: tangruiming, huifeng.guo, zhangyuzhou3@huawei.com\\n‡Shanghai Jiao Tong University, Shanghai, China\\nEmail: wnzhang@sjtu.edu.cn, chenhaokun@sjtu.edu.cn\\nAbstract —Recommendation is crucial in both academia and\\nindustry, and various techniques are proposed such as content-\\nbased collaborative ﬁltering, matrix factorization, logistic re-\\ngression, factorization machines, neural networks and multi-\\narmed bandits. However, most of the previous studies suffer\\nfrom two limitations: (1) considering the recommendation as\\na static procedure and ignoring the dynamic interactive nature\\nbetween users and the recommender systems; (2) focusing on the\\nimmediate feedback of recommended items and neglecting the\\nlong-term rewards. To address the two limitations, in this paper\\nwe propose a novel recommendation framework based on deep\\nreinforcement learning, called DRR. The DRR framework treats\\nrecommendation as a sequential decision making procedure and\\nadopts an “Actor-Critic” reinforcement learning scheme to model\\nthe interactions between the users and recommender systems,\\nwhich can consider both the dynamic adaptation and long-\\nterm rewards. Further more, a state representation module is\\nincorporated into DRR, which can explicitly capture the interac-\\ntions between items and users. Three instantiation structures are\\ndeveloped. Extensive experiments on four real-world datasets are\\nconducted under both the ofﬂine and online evaluation settings.\\nThe experimental results demonstrate the proposed DRR method\\nindeed outperforms the state-of-the-art competitors.\\nIndex Terms —Recommendation, Deep Reinforcement Learn-\\ning, User-Item Interactions\\nI. I NTRODUCTION\\nThanks to the increasing online services, such as online\\nshopping, online news and online social networks, it becomes\\nquite convenient to acquire items (goods, books, videos,\\nnews, etc.) via Internet or mobile devices. Albeit the great\\nconvenience, the overwhelming number of items in the sys-\\ntems also pose a signiﬁcant challenge for users, to ﬁnd the\\nitems that match their interests. Recommendation is a widely\\nused solution and various families of techniques have been\\nproposed, such as content-based collaborative ﬁltering [1],\\nmatrix factorization based methods [2]–[5], logistic regression,\\nfactorization machines and its variants [6]–[8], deep learning\\nmodels [9]–[12] and multi-armed bandits [13]–[17]. However,\\nsuch mentioned studies suffer from two serious limitations.Firstly , most of them consider the recommendation proce-'),\n",
       " Document(page_content='dure as a static process, i.e., they assume the user’s underlying\\npreference keeps unchanged. However, it is very common\\nthat a user’s preference is dynamic w.r.t. time, i.e., a user’s\\npreference on previous items will affect her choice on the next\\nitems. Hence, it would be more reasonable to model the recom-\\nmendation as a sequential decision making process. We will\\nshow some evidence observed in publicly available datasets\\n(MovieLens and Yahoo! Music) to support our opinion. In the\\ntwo datasets, the sequential behaviors of users are recorded and\\nwe are interested in what would happen if a user consecutively\\nreceives satisﬁed or unsatisﬁed recommendations. Though the\\ndatasets do not record any recommendation procedure, we\\ncan simulate this according to the users’ ratings, namely,\\nconsecutive rating “positive” (“negative”) simulates that a user\\nconsecutively receives satisﬁed (unsatisﬁed) recommendations.\\nAs presented in Figure 1, we observe that a user tends\\nto gives a higher (lower) rating if she has consecutively\\nreceived more satisﬁed (unsatisﬁed) items, as shown by the\\ngreen (red) line, where the blue dot line denotes the average\\nrating for reference. This suggests that a user will be more\\npleasant (unpleasant) if she consecutively receives more sat-\\nisﬁed (unsatisﬁed) recommendations and therefore she tends\\nto give a higher (lower) rating to the current recommendation.\\nHence, the user’s dynamic preference suggests that a good\\nrecommendation should be modeled as a sequential decision\\nmaking process.\\nSecondly , the aforementioned studies are trained by max-\\nimizing the immediate rewards of recommendations, which\\nmerely concentrates on whether the recommended items are\\nclicked or consumed, but ignores the long-term contributions\\nthat the items can make. However, the items with small imme-\\ndiate rewards but large long-term beneﬁts are also crucial [18].\\nWe take an example in News recommendation [19] to explain\\nthis. As a user requests for news to read, two possible pieces of\\nnews may lead to the same immediate reward, i.e., the user willarXiv:1810.12027v3  [cs.IR]  29 Oct 2019Fig. 1. Analysis on sequential patterns on user’s behavior in MovieLens and\\nYahoo!Music datasets\\nclick and read the two pieces of news with equal probability,\\nwhere one is about a thunderstorm alert and the other is about\\na basketball player Kobe Bryant. In this example, after reading\\nthe news about thunderstorm, the user probably is not willing\\nto read news about this issue anymore; while on the other hand,\\nthe user will possibly read more about NBA or basketball\\nafter reading the news about Kobe. The fact suggests that\\nrecommending the news about Kobe will introduce more long-\\nterm rewards. Hence, when recommending items to users, both\\nthe immediate and long-term rewards should be taken into\\nconsideration.\\nRecently, Reinforcement Learning (RL) [20], which has\\nshown great potential in various challenging scenarios that'),\n",
       " Document(page_content='shown great potential in various challenging scenarios that\\nrequire both dynamic modeling and long term planning, such\\nas game playing [21], [22], real-time ads bidding [23], [24],\\nneural network structure searching [25], [26], is introduced in\\nrecommender systems [18], [19], [27]–[33].\\nIn the early stage, model-based RL techniques are proposed\\nto model recommendation procedure, such as POMDP [18]\\nand Q-learning [27]. However, these methods are inapplicable\\nto complicated recommendation scenarios when the number\\nof candidate items is large, because a time-consuming dy-\\nnamic programming step is required to update the model.\\nLater, model-free RL techniques are utilized in recommender\\nsystems, from both academia and industry. Such techniques\\ncan be divided into two categories: value-based [19], [29] and\\npolicy-based [28], [32], [33]. Value-based approaches compute\\nQ-values of all available actions for a given state and the\\none with the maximum Q-value is selected as the best action.\\nDue to the evaluation on overall actions, the approaches may\\nbecome very inefﬁcient if the action space is too large. As\\nfor the policy-based approaches, this type of studies generate\\na continuous parameter vector as the representation of an\\naction [28], [32], [33], which can be utilized in generating the\\nrecommendation and updating the Q-value evaluator. Thanks\\nto the continuous representations, the inefﬁciency drawbacks\\ncan be overcome. However, these studies [28], [32], [33] still\\nhave one common limitation: the user state is learnt via a\\nconventional fully connected neural network, which does not\\nexplicitly and carefully model the interactions between users\\nand items.\\nIn this paper, to break the limitations stated above, we\\npropose a d eep r einforcement learning based r ecommendation\\nframework with explicit user-item interactions modeling(DRR). The “Actor-Critic” type framework DRR is incor-\\nporated with a state representation module, which explicitly\\nmodels the complex dynamic user-item interactions to pursuit\\nbetter recommendation performance. Speciﬁcally, the embed-\\ndings of users and items from the historical interactions are fed\\ninto a carefully designed multi-layer network, which explicitly\\nmodels the interactions between users and items, to produce\\na continuous state representation of the user in terms of her\\nunderlying sequential behaviors. This network is named as\\nthe state representation module, which plays two important\\nroles in our framework. On the one hand, it is utilized to\\ngenerate an ranking action to calculate the recommendation\\nscores for ranking. On the other hand, the state representation\\ntogether with the generated action is the input of the Critic\\nnetwork, which aims to estimate the Q-value, i.e., the quality\\nof the action in the current state. Based on the evaluation, the\\nActor (policy) network can be updated. We note that both the\\nActor and Critic networks are carefully designed by modeling'),\n",
       " Document(page_content='Actor and Critic networks are carefully designed by modeling\\nthe interactions between users and items explicitly. Extensive\\nexperiments on four real-world datasets demonstrate that the\\nproposed method yields superior performance than the state-\\nof-the-art methods. The main contributions of this paper can\\nbe summarized as follows:\\n•We propose a deep reinforcement learning based rec-\\nommendation framework DRR. Unlike the conventional\\nstudies, DRR adopts an “Actor-Critic” structure and treats\\nthe recommendation as a sequential decision making\\nprocess, which takes both the immediate and long-term\\nrewards into consideration.\\n•Under the DRR framework, three different network struc-\\ntures are proposed, which can explicitly model the inter-\\nactions between users and items.\\n•Extensive experiments are carried out on four real-world\\ndatasets, and the results demonstrate the proposed meth-\\nods indeed outperforms the state-of-the-art competitors.\\nThe rest of this paper is organized as follows. Related work\\nand background are presented in Section II. The preliminary\\nknowledge is presented in Section III. The proposed methods\\nare introduced in Section IV . Experimental details and results\\nare discussed in Section V . Finally, we conclude this paper\\nand discuss some future work in Section VI.\\nII. R ELATED WORK\\nA. Non-RL based Recommendation Techniques\\nVarious kinds of recommendation techniques are proposed\\nin the past a few decades to improve the performance of\\nrecommender systems, including content-based ﬁltering [1],\\nmatrix factorization based methods [2]–[5], logistic regression,\\nfactorization machines and its variants [6]–[8], and until\\nrecently deep learning models [9]–[12].\\nAt the beginning of this century, content-based ﬁltering [1]\\nis proposed to recommend items by considering the content\\nsimilarity between items. Later, collaborative ﬁltering (CF) is\\nput forward and extensively studied. The rationale behind CF\\nis that the users with similar behaviors tend to prefer the sameitems, and the items consumed by similar users tend to have\\nthe same rating. However, conventional CF based methods\\ntend to suffer from the data scarcity, because the similarity\\ncalculated from sparse data can be very unreliable. Matrix\\nfactorization (MF), as an advanced CF technique, plays an\\nimportant role in recommender systems. MF models [2]–[5]\\ncharacterize both items and users by vectors in the same space,\\nwhich are inferred from the observed user-item interactions.\\nRegarding the recommendation as a binary classiﬁcation prob-\\nlem, logistic regression and its variants [6] are also utilized\\nin recommender systems. However, logistic regression based\\nmodels are hard to generalize to the feature interactions that\\nnever or rarely appear in the training data. Factorization\\nmachines [7] model pairwise feature interactions as inner\\nproduct of latent vectors between features and show promising\\nresults. As an extension to FM, Field-aware FM (FFM [8])'),\n",
       " Document(page_content='results. As an extension to FM, Field-aware FM (FFM [8])\\nenables each feature to have multiple latent vectors to interact\\nwith different ﬁelds. Recently, deep learning models [9]–[12]\\nare applied to model the complicated feature interactions for\\nrecommendation.\\nAs a distinguished direction, contextual multi-armed bandits\\nare also utilized to model the interactive nature of recom-\\nmender systems [13]–[17]. Li et al. apply Thompson Sampling\\n(TS) and Upper Conﬁdent Bound (UCB) to balance the trade-\\noff between exploration and exploitation in [13] and [14],\\nrespectively. The authors of [16] propose a dynamic context\\ndrift model to address the time varying problem. To integrate\\nthe latent vectors of items and users with some exploration,\\nthe authors of [15], [17] combine matrix factorization with\\nmulti-armed bandits.\\nHowever, all these methods suffer from two limitations.\\nFirst, they consider the recommendation procedure as a static\\nprocess, i.e., they assume the underlying user’s preference\\nkeeps static and they aim to learn the user’s preference as\\nprecise as possible. Second, they are learned to maximize the\\nimmediate rewards of recommendations, but ignore the long-\\nterm beneﬁts that the recommendations can make.\\nB. RL based Recommendation Techniques\\nAs model-based RL techniques [18], [27] are inapplicable\\nin recommendation scenario due to their high time complex-\\nity, most researchers turn to model-free RL techniques. The\\nmodel-free RL techniques can be divide into two categories:\\npolicy-based and value-based.\\nPolicy-based approaches [28], [32], [33] aim to generate\\na policy, of which the input is a state, and the output is\\nan action. These works apply deterministic policies, which\\ngenerates an action directly. Dulac-Arnold et al. [33] resolves\\nthe large action space problem by modeling the state in a\\ncontinuous item embedding space and selecting the items via\\na neighborhood method. However, as the underlying algorithm\\nis essentially a continuous-action algorithm, its performance\\nmay be cursed by the gap between the continuous and discrete\\naction spaces. In [28], [32], the policy network outputs a\\ncontinuous action representation, and the recommendation is\\ngenerated by ranking the items with their scores, which arecomputed by a pre-deﬁned function with the action representa-\\ntion and the item embeddings as input. However, one common\\nlimitation of the studies is that they do not carefully learn the\\nstate representation.\\nFor value-based approaches [19], [29], the action with max-\\nimum Q-value over all the possible actions is selected as the\\nbest action. Zhao et al. [29] take both user’s positive feedback\\nand negative feedback into consideration when modeling user\\nstate. Dueling Q-network is utilized in [19], to model Q-\\nvalue of a state-action pair. Moreover, a minor update with\\nexploration by dueling bandit gradient descent is proposed.\\nHowever, such value-based approaches need to evaluate the'),\n",
       " Document(page_content='However, such value-based approaches need to evaluate the\\nQ-values of all the actions under a speciﬁc state, which is\\nvery inefﬁcient when the number of actions is large.\\nTo make RL based recommendation techniques suitable for\\nlarge-scale scenario, in this paper, we propose the DRR frame-\\nwork which carefully and explicitly model the interactions\\nbetween users and items to learn the state representation.\\nIII. P RELIMINARIES\\nThe essential underlying model of reinforcement learning\\nis Markov Decision Process (MDP). An MDP is deﬁned as\\n(S,A,P,R,γ).Sis the state space and Ais the action\\nspace.P:S×A×S ↦→ [0,1]is the state transition\\nfunction.R:S×A×S ↦→ Ris the reward function. γ\\nis the discount rate. The objective of an agent in an MDP\\nis to ﬁnd an optimal policy ( πθ:S×A ↦→ [0,1]) which\\nmaximizes the expected cumulative rewards from any state\\ns∈ S , i.e.,V∗(s) = max πθEπθ{∑∞\\nk=0γkrt+k|st=s},\\nor maximizes equivalently the expected cumulative rewards\\nfrom any state-action pair s∈ S,a∈ A , i.e.,Q∗(s,a) =\\nmaxπθEπθ{∑∞\\nk=0γkrt+k|st=s,at=a}. Here Eπθis the\\nexpectation under policy πθ,tis the current timestep and rt+k\\nis the immediate reward at a future timestep t+k.\\nWe model the recommendation procedure as a sequential\\ndecision making problem, in which the recommender (i.e.,\\nagent) interacts with users (i.e., environment) to suggest a list\\nof items sequentially over the timesteps, by maximizing the\\ncumulative rewards of the whole recommendation procedure.\\nMore speciﬁcally, the recommendation procedure is modeled\\nby an MDP, as follows.\\n•StatesS.A statesis the representation of user’s positive\\ninteraction history with recommender, as well as her\\ndemographic information (if it exists in the datasets).\\n•ActionsA.An actionais a continuous parameter vector\\ndenoted as a∈R1×k. Each item it∈R1×k1has a\\nranking score, which is deﬁned as the inner product of\\nthe action and the item embedding, i.e., ita⊤. Then the\\ntop ranked ones will be recommended.\\n•TransitionsP.The state is modeled as the representa-\\ntion of user’s positive interaction history. Hence, once\\nthe user’s feedback is collected, the state transition is\\ndetermined.\\n1itis the embedding of item i, which can be generated by MF or V AE.•RewardR.Given the recommendation based on the\\nactionaand the user state s, the user will provide her\\nfeedback, i.e., click, not click, or rating, etc. The recom-\\nmender receives immediate reward R(s,a)according to\\nthe user’s feedback.\\n•Discount rate γ.γ∈[0,1]is a factor measuring the\\npresent value of long-term rewards. In the case of γ= 0,\\nthe recommender considers only immediate rewards but\\nlong-term rewards are ignored. On the other hand, when\\nγ= 1, the recommender treats immediate rewards and\\nlong-term rewards as equally important.\\nFigure 2 illustrates the recommender-user interactions in\\nMDP formulation. Considering the current user state and\\nimmediate reward to the previous action, the recommender\\ntakes an action. Note that in our model, an action corre-'),\n",
       " Document(page_content='takes an action. Note that in our model, an action corre-\\nsponds to neither recommending an item nor recommending\\na list of items. Instead, an action is a continuous parameter\\nvector. Taking such an action, the parameter vector is used\\nto determine the ranking scores of all the candidate items,\\nby performing inner product with item embeddings. All the\\ncandidate items are ranked according to the computed scores\\nand Top-N items are recommended to the user. Taking the\\nrecommendation from the recommender, the user provides her\\nfeedback to the recommender and the user state is updated\\naccordingly. The recommender receives rewards according\\nto the user’s feedback. Without loss of generalization, a\\nrecommendation procedure is a Ttimestep2trajectory as\\n(s0,a0,r0,s1,a1,r1,...,sT−1,aT−1,rT−1,sT).\\nRecommender (Agent)\\nUsers (Environment)state!\"!\"=$(&\")reward(\"(\"=)(!\",+\")!\",-(\",-action+\"+\"=./(!\")\\nFig. 2. Recommender-User interactions in MDP\\nIV. T HEPROPOSED DRR F RAMEWORK\\nAs aforementioned in Section 1, conventional recommenda-\\ntion techniques suffer from either a lack of sequential model-\\ning or ignoring the long-term rewards, or both. To address\\nthe drawbacks, we propose a deep reinforcement learning\\nbased recommendation framework (DRR) based on the Actor-\\nCritic learning scheme. Also, different from some recent RL\\nstudies, we carefully and explicitly build a state representation\\nmodule to model the interactions between the users and items.\\nNext, we will ﬁrst elaborate the Actor network, Critic network\\nand the state representation module respectively, which are\\n2If a recommendation episode terminates in less than T timesteps, then the\\nlength of the episode is the actual value.essentially the three key ingredients in our framework; then\\nthe training and evaluation procedures will be presented to\\nshow how to learn and use the DRR framework.\\nSReLUReLUaReLUconcatQ(s,a)State RepresentationModuleItemsSReLUReLUaTanhItem Space.RankingRecommendationActorCriticLegend.Item or userScalar productFC layerXElement-wise productWeight 1Item weightAverage poolinglayerScalar \\nFig. 3. DRR Framework\\nA. Three Key Ingredients in DRR\\n1) The Actor network: The Actor network, also called the\\npolicy network, is depicted on the left part of Figure 3. For\\na given user, the network accounts for generating an action\\nabased on her state s. Let us explain the network from the\\ninput to the output part. In DRR, the user state, denoted by\\nthe embeddings of her nlatest positively interacted items,\\nis regarded as the input. Then the embeddings are fed into\\na state representation module (which will be introduced in\\ndetails later) to produce a summarized representation sfor the\\nuser. For instance, at timestep t, the state can be deﬁned in\\nEq. (1):\\nst=f(Ht) (1)\\nwheref(·)stands for the state representation module, Ht=\\n{i1,...,in}denotes the embeddings of the latest positive inter-\\naction history, and it∈R1×kis ak-dimensional vector. When\\nthe recommender agent recommends an item it, if the user'),\n",
       " Document(page_content='the recommender agent recommends an item it, if the user\\nprovides positive feedback, then in the next timestep, the state\\nis updated to st+1=f(Ht+1), whereHt+1={i2,...,in,it};\\notherwise,Ht+1=Ht. The reasons to deﬁne the state in such\\na manner are two folds: (i) a superior recommender system\\nshould cater to the users’ taste, i.e., what items the users like;\\n(ii) the latest records represent the users’ recent interests more\\nprecisely.\\nFinally, by two ReLU layers and one Tanh layer, the state\\nrepresentation sis transformed into an action a=πθ(s)as\\nthe output of the Actor network. Particularly, the action a\\nis deﬁned as a ranking function represented by a continuous\\nparameter vector a∈R1×k. By using the action, the ranking\\nscore of the item itis deﬁned as:\\nscoret=ita⊤(2)\\nThen, the top ranked item (w.r.t. the ranking scores) is rec-\\nommended to the user. Note that, the widely used ε-greedy\\nexploration technique is adopted here.2) The Critic network: The Critic part in DRR, shown as\\nthe middle part of Figure 3, is a Deep Q-Network [21], which\\nleverages a deep neural network parameterized as Qω(s,a)\\nto approximate the true state-action value function Qπ(s,a),\\nnamely, the Q-value function. The Q-value function reﬂects the\\nmerits of the action policy generated by the Actor network.\\nSpeciﬁcally, the input of the Critic network is the user state\\nsgenerated by the user state representation module and the\\nactionagenerated by the policy network, and the output is\\nthe Q-value, which is a scalar. According to the Q-value, the\\nparameters of the Actor network are updated in the direction of\\nimproving the performance of action a, i.e., boosting Qω(s,a).\\nBased on the deterministic policy gradient theorem [34], we\\ncan update the Actor by the sampled policy gradient shown\\nin Eq.(3):\\n∇θJ(πθ)≈1\\nN∑\\nt∇aQω(s,a)|s=st,a=πθ(st)∇θπθ(s)|s=st\\n(3)\\nwhereJ(πθ)is the expectation of all possible Q-values that\\nfollow the policy πθ. Here the mini-batch strategy is utilized\\nandNdenotes the batch size. Moreover, the Critic network\\nis updated accordingly by the temporal-difference learning\\napproach [20], i.e., minimizing the mean squared error shown\\nin Eq.(4):\\nL=1\\nN∑\\ni(yi−Qω(si,ai))2(4)\\nwhereyi=ri+γQω′(si+1,πθ′(si+1)). The target net-\\nwork [35] technique is also adopted in DRR framework, where\\nω′andθ′is the parameters of the target Critic and Actor\\nnetwork.\\n3) The State Representation Module: As noted above, the\\nstate representation module plays an important role in both\\nthe Actor network and Critic network. Hence, it is very\\ncrucial to design a good structure to model the state. In [10],\\n[11], it has been shown that modeling the feature interactions\\nexplicitly can boost the performance of a recommendation\\nsystem. Inspired by the studies, we propose to design the state\\nrepresentation module by explicitly modeling the interactions\\nbetween the users and items. Speciﬁcally, we develop three\\nstructures, which will be elaborated next.\\nItemsS…Concat& flattenXXX\\nFig. 4. DRR-p Structure'),\n",
       " Document(page_content='structures, which will be elaborated next.\\nItemsS…Concat& flattenXXX\\nFig. 4. DRR-p Structure\\n•DRR-p . Inspired by [10], [11], we propose a product\\nbased neural network for the state representation module,which is depicted in Figure 43. The structure is named\\nas DRR-p, which utilizes a product operator to capture\\nthe pairwise local dependency between items. We can\\nsee that the structure clones the representations of the n\\nitems fromH={i1,...,in}. In addition, it computes the\\npairwise interactions between the nitems, by using the\\nelement-wise product operator. As a result, n(n−1)/2\\nnew features vectors are yielded, which will be concate-\\nnated with the cloned vectors as the state representation.\\nWe note that in the element-wise product part, a weight is\\nalso learned for each item to show its importance. Hence,\\nin DRR-p the state representation module can be formally\\nstated as follows:\\ns= [H,{pa,b|a,b= 1,...,n}] (5)\\npa,b=waia⊗wbib (6)\\nwhere⊗denotes the element-wise product, wais a\\nscalar indicating the importance of item ia, andpa,bis\\nak-dimensional vector which models the interactions\\nbetween item iaandib. The dimensionality of sis\\nk(n+n(n−1)/2).\\n•DRR-u . Though DRR-p can model the pairwise local\\ndependency between items, the user-item interactions are\\nneglected. To remedy this, we design another structure in\\nFigure 5, which is referred as DRR-u. In DRR-u, we\\ncan see that the user embedding is also incorporated.\\nIn addition to the local dependency between items, the\\npairwise interactions of user-item are also taken into\\naccount. Formally, the state representation module can\\nbe expressed as:\\ns= [{u⊗waia|a= 1,...,n},{pa,b|a,b= 1,...,n}](7)\\nThe dimensionality of sis alsok(n+n(n−1)/2).\\nItemsS…Concat& flattenXXXuserXXXXX\\nFig. 5. DRR-u Structure\\n•DRR-ave . In DRR-p and DRR-u structures, the inter-\\nactions between users and items can be exploited and\\nmodeled. For the two structures, it is not difﬁcult to ﬁnd\\nthat the positions of items in Hmatters, e.g., the state\\nrepresentations of H1={ia,ib,ic}andH2={ic,ib,ia}\\n3The legend in Figure 4, 5 and 6 is the same to Figure 3are different. When His large, we expect the positions\\nof items really matter, because Hdenotes a long-term\\nsequence; whereas memorizing the positions of items\\nmay lead to overﬁtting if the sequence His a short-term\\none. Hence, we design another structure by eliminating\\nthe position effects, which is depicted in Figure 6. As an\\naverage pooling layer is adopted, we call the structure\\nDRR-ave. We can see from Figure 6 that the embeddings\\nof items inHare ﬁrst transformed by a weighted average\\npooling layer. Then, the resulting vector is leveraged to\\nmodel the interactions with the input user. Finally, the\\nembedding of the user, the interaction vector, and the\\naverage pooling result of items are concatenate into a\\nvector to denote the state representation. Formally, the\\nDRR-ave structure can be expressed as:\\ns= [u,u⊗{g(ia)|a= 1,...,n},{g(ia)|a= 1,...,n}]\\n(8)\\ng(ia) =ave(waia)|a= 1,...,n (9)'),\n",
       " Document(page_content='s= [u,u⊗{g(ia)|a= 1,...,n},{g(ia)|a= 1,...,n}]\\n(8)\\ng(ia) =ave(waia)|a= 1,...,n (9)\\nHereg(·)indicates the weighted average pooling layer.\\nThe dimensionality of sin DRR-ave is 3k.\\nItemsSConcat& flattenXuser\\nFig. 6. DRR-ave Structure\\nB. Training Procedure of the DRR Framework\\nNext, we introduce how to train the DRR framework. We\\nﬁrst present the overall idea and then discuss the detailed\\nalgorithm. As aforementioned, DRR utilizes the users’ inter-\\naction history with the recommender agent as training data.\\nDuring the procedure, the recommender takes an action at\\nfollowing the current recommendation policy πθ(st)after\\nobserving the user (environment) state st, then it obtains the\\nfeedback (reward) rtfrom the user, and the user state is\\nupdated tost+1. According to the feedback, the recommender\\nupdates its recommendation policy. In this work, we utilize\\ndeep deterministic policy gradient (DDPG) [35] algorithm to\\ntrain the proposed DRR framework, as detailed in Algorithm\\n1.\\nSpeciﬁcally, in timestep t, the training procedure mainly\\nincludes two phases, i.e., transition generation (lines 7-12)\\nand model updating (lines 13-17). For the ﬁrst stage, the\\nrecommender observes the current state stthat is calculated\\nby the proposed state representation module, then generates\\nan actionat=πθ(st)according to the current policy πθwithε-greedy exploration, and recommends an item itaccording to\\nthe actionatby Eq. (2) (lines 8-9). Subsequently, the reward\\nrtcan be calculated based on the feedback of the user to\\nthe recommended item it, and the user state is updated (lines\\n10-11). Finally, the recommender agent stores the transition\\n(st,at,rt,st+1)into the replay buffer D(line 12).\\nIn the second stage, the model updating, the recommender\\nsamples a minibatch of Ntransitions with widely used pri-\\noritized experience replay [36] sampling technique (line 13),\\nwhich is essentially an importance sampling strategy. Then, the\\nrecommender updates the parameters of the Actor network and\\nCritic network according to Eq. (3) and Eq. (4) respectively\\n(line 14-16). Finally, the recommender updates the target\\nnetworks’ parameters with the soft replace strategy.\\nAlgorithm 1: Training Algorithm of DRR Framework\\ninput : Actor learning rate ηa, Critic learning rate ηc,\\ndiscount factor γ, batch size N, state window\\nsizenand reward function R\\n1Randomly initialize the Actor πθand the Critic Qωwith\\nparametersθandω\\n2Initialize the target network π′andQ′with weights\\nθ′←θandω′←ω\\n3Initialize replay buffer D\\n4forsession = 1, M do\\n5 Observe the initial state s0according to the ofﬂine\\nlog\\n6 for t = 1, T do\\n7 Observe current state st=f(Ht), where\\nHt={i1,...,in}\\n8 Find action at=πθ(st)according to the current\\npolicy with ε-greedy exploration\\n9 Recommended item itaccording to action atby\\nEq. (2)\\n10 Calculate reward rt=R(st,at)based on the\\nfeedback of the user\\n11 Observe new state st+1=f(Ht+1), where\\nHt+1={i2,...,in,it}ifrtis positive,\\notherwise,Ht+1=Ht\\n12 Store transition (st,at,rt,st+1)inD'),\n",
       " Document(page_content='Ht+1={i2,...,in,it}ifrtis positive,\\notherwise,Ht+1=Ht\\n12 Store transition (st,at,rt,st+1)inD\\n13 Sample a minibatch of Ntransitions\\n(si,ai,ri,si+1)inDwith prioritized experience\\nreplay sampling technique\\n14 Setyi=ri+γQω′(si+1,πθ′(si+1))\\n15 Update the Critic network by minimizing the\\nloss:L=1\\nN∑\\ni(yi−Qω(si,ai))2\\n16 Update the Actor network using the sampled\\npolicy gradient:\\n∇θJ(πθ)≈\\n1\\nN∑\\nt∇aQω(s,a)|s=st,a=πθ(st)∇θπθ(s)|s=st\\n17 Update the target networks:\\nθ′←τθ+ (1−τ)θ′\\nω′←τω+ (1−τ)ω′\\n18returnθandωC. Evaluation\\nIn this subsection, we discuss how to evaluate the models\\nwith a environment simulator. The most straightforward way to\\nevaluate the RL based models is to conduct online experiments\\non recommender systems where the recommender directly\\ninteracts with users. However, the underlying commercial risk\\nand the costly deployment on the platform make it impracti-\\ncal. Therefore, throughout the testing phase, we conduct the\\nevaluation of the proposed models on public ofﬂine datasets\\nand propose two ways to evaluate the models, which are the\\nofﬂine evaluation and the online evaluation.\\n1) Ofﬂine evaluation: Intuitively, the ofﬂine evaluation of\\nthe trained models is to test the recommendation performance\\nwith the learned policy, which is described in Algorithm 2.\\nSpeciﬁcally, for a given session Sj, the recommender only\\nrecommends the items that appear in this session, denoted as\\nI(Sj), rather than the ones in the whole item space. The reason\\nis that we only have the ground truth feedback for the items in\\nthe session in the recoreded ofﬂine log. For each timestep, the\\nrecommender agent takes an action ataccording to the learned\\npolicyπθ, and recommends an item it∈I(Sj)based on the\\nactionatby Eq. (2) (lines 4-5). After that, the recommender\\nobserves the reward rt=R(st,at)according to the feedback\\nof the recommended item itby Eq. (10) (lines 5-6). Then the\\nuser state is updated to st+1and the recommended item itis\\nremoved from the candidate set I(Sj)(lines 7-8). The ofﬂine\\nevaluation procedure can be treated as a rerank procedure of\\nthe candidate set by iteratively selecting an item w.r.t. the\\naction generated by the Actor network in DRR framework.\\nMoreover, the model parameters are not updated in the ofﬂine\\nevaluation.\\nAlgorithm 2: Ofﬂine Evaluation Algorithm of DRR\\nFramework\\ninput : state window size nand reward function R\\n1Observe the initial state s0and item setIaccording to\\nthe ofﬂine log\\n2for t = 1, T do\\n3 Observe current state st={i1,...,in}\\n4 Execute action at=πθ(st)according to the current\\npolicy\\n5 Observe the recommended item itaccording to\\nactionatby Eq. (2)\\n6 Get rewardrt=R(st,at)from the feedback located\\nin the users’ log by Eq. (10)\\n7 Update to a new state st+1=f(Ht+1), where\\nHt+1={i2,...,in,it}ifrtis positive, otherwise,\\nHt+1=Ht\\n8 removeitfromI\\n2) Online evaluation with environment simulator: As afore-\\nmentioned that it is risky and costly to directly deploy the\\nRL based models on recommender systems. Therefore, we'),\n",
       " Document(page_content='RL based models on recommender systems. Therefore, we\\nconduct online evaluation with an environment simulator. In\\nthis paper, we pretrain a PMF [37] model as the environmentsimulator, i.e., to predict an item’s feedback that the user\\nnever rates before. The online evaluation procedure follows\\nAlgorithm 1, i.e., the parameters continuously update dur-\\ning the online evaluation stage. Its major difference from\\nAlgorithm 1 is that the feedback of a recommended item\\nis observed by the environment simulator. Moreover, before\\neach recommendation session starting in the simulated online\\nevaluation, we reset the parameters back to θandωwhich is\\nthe policy learned in the training stage for a fair comparison.\\nV. E XPERIMENT\\nA. Datasets and Evaluation Metrics\\nWe adopt the following publicly available datasets from the\\nreal world to conduct the experiments:\\n•MovieLens (100k)4. A benchmark dataset comprises\\nof 0.1 million ratings from users to the recommended\\nmovies on MovieLens website.\\n•Yahoo! Music (R3)5. This dataset contains over 0.36\\nmillion ratings of songs collected from two different\\nsources. The ﬁrst source consists of ratings provided\\nby users during normal interactions with Yahoo! Music\\nservices. The second source consists of ratings of ran-\\ndomly selected songs collected during an online survey\\nby Yahoo! Research. We normalize the ratings to discrete\\nvalues from 1 to 5.\\n•MovieLens (1M)6. A benchmark dataset includes of 1\\nmillion ratings from the MovieLens website.\\n•Jester (2)7. This dataset contains over 1.7 million real-\\nvalue ratings (-10.0 to +10.0) over jokes in an online joke\\nrecommender system.\\nNote that except for Jester, the ratings in the other datasets\\nare discrete values from 1 to 5, and the statistic information\\nof the datasets is given in Table I. The MovieLens (100k) and\\nMovieLens (1M) are abbreviated as ML (100k) and ML (1M)\\nrespectively.\\nTABLE I\\nSTATISTIC INFORMATION OF THE DATASETS\\nML (100k) Yahoo! Music ML (1M) Jester\\n# user 943 15,400 6,040 63,978\\n# item 1,682 1,000 3,952 150\\n# ratings 100,000 365,740 1,000,209 1,761,439\\nWe conduct both ofﬂine and simulated online evaluation\\non these four datasets. For the ofﬂine evaluation, we utilize\\nPrecision@k and NDCG@k as the metrics to measure the\\nperformance of the proposed models. For the simulated online\\nevaluation, we leverage the total accumulated rewards as the\\nmetric.\\n4https://grouplens.org/datasets/movielens/100k/\\n5https://webscope.sandbox.yahoo.com/\\n6https://grouplens.org/datasets/movielens/1m/\\n7http://eigentaste.berkeley.edu/dataset/B. Compared Methods\\nWe compare the proposed methods with some representative\\nbaseline methods. For the ofﬂine evaluation, we compare to\\nconventional methods including Popularity, PMF [37] and\\nSVD++ [38], and a RL based method DRR-n. Moreover, the\\nonline evaluation baselines contain the state-of-the-art multi-\\narmed bandits methods LinUCB [39] and HLinUCB [40] and\\nthe DRR-n as well.\\n•Popularity recommends the most popular item, i.e., the'),\n",
       " Document(page_content='the DRR-n as well.\\n•Popularity recommends the most popular item, i.e., the\\nitem with the highest average rating or the items with\\nlargest number of positive ratings8from current available\\nitems to the users at each timestep.\\n•PMF makes a matrix decomposition as SVD, while it\\nonly takes into account the non zero elements.\\n•SVD++ mixes strengths of the latent model as well as\\nthe neighborhood model.\\n•LinUCB selects an arm (item) according to the estimated\\nupper conﬁdence bound of the potential reward.\\n•HLinUCB further learns hidden features for each arm to\\nmodel the potential reward.\\n•DRR-n simply utilizes the concatenation of the item\\nembeddings to represent user state, which is widely\\nused in previous studies. Although it is under the DRR\\nframework, we treat this method as a baseline to assess\\nthe effectiveness of our proposed state representation\\nmodule.\\nC. Experimental Settings\\nFor each dataset, we choose 80% of the interactions in\\neach user session as the training set, and leave the rest as the\\ntesting set. Moreover, for MovieLens (100k), Yahoo! Music\\nand MovieLens (1M), the positive ratings are 4and5, while for\\nJester, the positive ones are those higher than 0. The number\\nof latest positively rated items n, which is empirically set to 5.\\nWe perform PMF to pretrain the 100-dimensional embeddings\\nof the users and items. Moreover, in each episode, we do not\\nrecommend repeated items, i.e., we remove the ones already\\nrecommended from the candidate set. The discount rate γis\\n0.9. We utilize Adam optimizer for all the RL based methods\\nwithL2-norm regularization to prevent overﬁtting. As for the\\nreward function, we empirically normalize the ratings into\\nrange [-1 ,1] and utilize the normalized ones as the feedback of\\nthe corresponding recommendations. For instance, in timestep\\nt, the recommender agent recommends an item jto useri,\\n(denoted as action ain states), and the rating ratei,jcomes\\nfrom the interaction logs if user iactually rates item j, or\\nfrom a predicted value by the simulator otherwise. Therefore,\\nthe reward function can be deﬁned as follows:\\nR(s,a) =1\\n2(ratei,j−3)\\nR(s,a) =ratei,j/10(10)\\n8To get a better result of popularity based recommendation, we both test\\nthe two strategies, and choose the best one to report.where the ﬁrst setting is for MovieLens (100k), Yahoo! Music\\nand MovieLens (1M), and the second one is for Jester. All the\\nbaseline methods are carefully tuned for a fair comparison.\\nWe model the recommendation procedure as an interaction\\nepisode with length T, and the hyper-parameter Tis tuned\\nfor different datasets (detailed in Section V .E).\\nD. Results and Analysis\\n1) Ofﬂine Evaluation Results and Analysis: The ofﬂine\\nevaluation results are summarized from Table II to Table V\\nrespectively, where the best results are marked in bold type.\\nIn the ofﬂine evaluation, we compare the proposed methods\\nto some representative ofﬂine learning methods. The results\\nsuggest that the proposed methods under the DRR framework'),\n",
       " Document(page_content='suggest that the proposed methods under the DRR framework\\noutperform the baselines on most of datasets, which demon-\\nstrates the effectiveness of our proposed methods.\\nSpeciﬁcally, as aforementioned that, we propose three dif-\\nferent network structure in the state representation module to\\nmodel the explicit interactions of the users and items under the\\nDRR framework, which are the DRR-p, DRR-u and DRR-\\nave. From the results in Table II to Table V, we ﬁnd that\\nthe three methods all outperform the baselines in most cases.\\nMoreover, DRR-n that simply concats the item embeddings\\nto represent the state s, performs worse than the proposed\\nDRR-p, DRR-u and DRR-ave. From the observations, we\\ncan conclude in two folds: (i) the proposed methods indeed\\nhave the capability of long-term scheduling and dynamic\\nadaptation, which are ignored by conventional methods; (ii)\\nthe proposed state representation module well captures the\\ndynamic interactions between the users and items, and the\\nstate should not be simply concatenate with fully connected\\nlayers as DRR-n does, which may result in information loss.\\nCompared with DRR-p, DRR-u and DRR-ave, we can see\\nthat DRR-ave outperforms DRR-u, and DRR-u is superior than\\nDRR-p on the four datasets in most cases. The reasons are as\\nfollows: 1) The DRR-u method has better performance than\\nDRR-p, because DRR-u only captures the interactions of user’s\\nhistorical items, but also seizes the personalization information\\nthrough the user-item interactions. 2) DRR-ave performs the\\nbest, because of two reasons: (i) DRR-ave method captures\\nthe personalization information through user-item interactions;\\n(ii) as noted in Section IV , by using the average pooling, it\\neliminates the position effects in H.\\nTABLE II\\nPERFORMANCE COMPARISON OF ALL METHODS ON ML (100 K)DATASET .\\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\\nPopularity 0.6933 0.6012 0.9104 0.9008\\nPMF 0.6988 0.6194 0.9095 0.8968\\nSVD++ 0.7034 0.6255 0.9125 0.8991\\nDRR-n 0.7185 0.6387 0.9147 0.9004\\nDRR-p 0.7263 0.6448 0.9076 0.9015\\nDRR-u 0.7417 0.6536 0.9183 0.9062\\nDRR-ave 0.7887 0.6935 0.9255 0.9046TABLE III\\nPERFORMANCE COMPARISON OF ALL METHODS ON YAHOO ! M USIC\\nDATASET .\\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\\nPopularity 0.3826 0.3805 0.8870 0.8811\\nPMF 0.3835 0.3817 0.8837 0.8802\\nSVD++ 0.3857 0.3821 0.8887 0.8813\\nDRR-n 0.3844 0.3819 0.8876 0.8810\\nDRR-p 0.3850 0.3822 0.8883 0.8815\\nDRR-u 0.3864 0.3827 0.8889 0.8819\\nDRR-ave 0.3917 0.3839 0.9004 0.8949\\nTABLE IV\\nPERFORMANCE COMPARISON OF ALL METHODS ON ML (1M) DATASET .\\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\\nPopularity 0.7141 0.6181 0.8906 0.8738\\nPMF 0.7072 0.6193 0.8901 0.8746\\nSVD++ 0.7142 0.6258 0.9009 0.8776\\nDRR-n 0.7151 0.6221 0.8902 0.8751\\nDRR-p 0.7346 0.6366 0.8909 0.8753\\nDRR-u 0.7375 0.6385 0.8912 0.8763\\nDRR-ave 0.7693 0.6594 0.9112 0.8980\\n2) Simulated online evaluation results and analysis: The\\nresults of the simulated online evaluation are summarized in'),\n",
       " Document(page_content='results of the simulated online evaluation are summarized in\\nTable VI, where the best results are marked in bold type. In the\\nexperiment, we only compare with the baseline methods that\\ncan perform online learning, which are LinUCB, HLinUCB\\nand DRR-n. Again, we ﬁnd that the proposed methods deliver\\nhigher rewards than all the baselines.\\nOn the one hand, the fact suggests that the proposed\\nRL-based methods model dynamic adaptation and long-term\\nrewards better than the multi-armed bandits based methods\\nLinUCB and HLinUCB. On the other hand, the observation\\nindicates that the proposed state representation structures are\\nsuperior to the naive full-connected network in DRR-n. Again,\\nwe observe that DRR-ave performs the best among all the three\\nproposed interaction modeling structures.\\nE. Parameter Study\\nIn this subsection, we investigate how the episode length\\nTaffect the performance of proposed methods. Figure 7\\nshows the results9. From the left part of Figure 7, we observe\\nthat the performance on MovieLens ﬁrst increases and then\\ndecreases as the length of the episode is gradually increased,\\nand the summit appears at T= 10 . A similar tend can be\\nfound for the Yahoo! Music from the right part of Figure 7,\\nwhere the performance peaks at T= 20 . The reason may\\ndue to the trade-off between the exploitation and exploration.\\nWhen the episode length is small, the user can not fully\\ninteract with the recommender agent, i.e., the exploration is\\ninsufﬁcient. As we enlarge the episodes, the recommender\\n9Due to the space limit, We only present the performance of DRR-ave,\\nwhile DRR-p and DRR-u have similar observationsTABLE V\\nPERFORMANCE COMPARISON OF ALL METHODS ON JESTER DATASET .\\nModel Precision@5 Precision@10 NDCG@5 NDCG@10\\nPopularity 0.6167 0.6012 0.8932 0.8703\\nPMF 0.6171 0.6015 0.8740 0.8676\\nSVD++ 0.6184 0.6027 0.8819 0.8614\\nDRR-n 0.6178 0.6021 0.8915 0.8724\\nDRR-p 0.6181 0.6029 0.8934 0.8753\\nDRR-u 0.6217 0.6043 0.8974 0.8805\\nDRR-ave 0.6278 0.6076 0.9124 0.9079\\nTABLE VI\\nTHE REWARDS OF ALL METHODS ON THE FOUR DATASETS .\\nModel ML (100k) Yahoo! Music ML (1M) Jester\\nLinUCB 1,958 30,462.5 30,174 141,358.4\\nHLinUCB 1,475 32,725 32,785.5 147,105.5\\nDRR-n 2,654.5 35,382.5 35,860 165,844.5\\nDRR-p 2,832 37,328.5 36,653 177,414.2\\nDRR-u 2,869 42,174.5 37,615 183,517.6\\nDRR-ave 3,251.5 49,095 40,588 194,860.7\\nagent can explore (interact with users) adequately, i.e., the\\nrecommender agent captures the user’s preference, so that the\\nperformance improves. However, if the episodes are too large,\\nthe recommender focuses on exploiting locally, but the user\\npreferred items is limited, therefore the performance declines\\nas we do not recommend repeated items to user. Hence, we\\nshould nicely trade off the exploration and exploitation by\\nsetting a suitable value for T.\\nFig. 7. Parameter study on episode length Tin MovieLens and Yahoo!Music\\ndatasets\\nF . Case Study\\nIn this subsection, we present an example to show the\\ndifferent recommendation manner between LinUCB and DRR-'),\n",
       " Document(page_content='different recommendation manner between LinUCB and DRR-\\nave on MovieLens dataset. Speciﬁcally, we randomly pick up\\na user with ID 11, and conduct the recommendation procedure\\nwith LinUCB and DRR-ave respectively. To verify the reaction\\nto the same recommendation scenario, we ﬁx the ﬁrst three\\nrecommended items and to see what will happen next. The\\nresults of recommended item and the reward are reported in\\nTable VII.\\nFrom Table VII, we can see that LinUCB and DRR-ave\\nreact differently when given two consecutive negative recom-\\nmendations (Eraser and First Knight). Speciﬁcally, LinUCBkeeps exploring without considering to recommend a “safe”\\nitem to please the user. However, DRR-ave stops exploration\\nand recommends a risk-free movie Dead Man Walking, which\\nbelongs to the same genre as Chasing Amy that has gained a\\npositive feedback from the user at timestep 1. The observation\\ndemonstrates the superiority of the proposed DRR-ave against\\nLinUCB.\\nTABLE VII\\nDIFFERENT RECOMMENDATION MANNER BETWEEN LINUCB AND\\nDRR- AVE ON MOVIE LENS. (T HE VALUE IN (·)DENOTES THE\\nCORRESPONDING REWARD .)\\ntimestep LinUCB DRR-ave\\n1 Chasing Amy (1) Chasing Amy (1)\\n2 Eraser (-0.5) Eraser (-0.5)\\n3 First Knight (-1) First Knight (-1)\\n4 The Deer Hunter (-0.5) Dead Man Walking (1)\\n5 Event Horizon (-1) Braveheart (0.5)\\n6 The Net (0) The Usual Suspect (-0.5)\\n7 Striptease (-0.5) Psycho (0.5)\\nVI. C ONCLUSION\\nIn this paper, we propose a deep reinforcement learning\\nbased framework DRR to perform the recommendation task.\\nUnlike the conventional studies, DRR treats the recommen-\\ndation as a sequential decision making process and adopts\\nan “Actor-Critic” learning scheme, which can take both the\\nimmediate and long-term rewards into account. In DRR, a\\nstate representation module is incorporated and three instanti-\\nation structures are designed, which can explicitly model the\\ninteractions between users and items. Extensive experiments\\non four real-world datasets demonstrate the superiority of the\\nproposed DRR method over state-of-the-art competitors.\\nREFERENCES\\n[1] R. J. Mooney and L. Roy, “Content-based book recommending using\\nlearning for text categorization,” in ACM DL , 2000, pp. 195–204.\\n[2] M. Deshpande and G. Karypis, “Item-based top- Nrecommendation\\nalgorithms,” ACM Trans. Inf. Syst. , vol. 22, no. 1, pp. 143–177, 2004.\\n[3] Y . Koren, R. M. Bell, and C. V olinsky, “Matrix factorization techniques\\nfor recommender systems,” IEEE Computer , vol. 42, no. 8, pp. 30–37,\\n2009.\\n[4] G. Linden, B. Smith, and J. York, “Amazon.com recommendations:\\nItem-to-item collaborative ﬁltering,” IEEE Internet Computing , vol. 7,\\nno. 1, pp. 76–80, 2003.\\n[5] J. Wang, A. P. De Vries, and M. J. Reinders, “Unifying user-based and\\nitem-based collaborative ﬁltering approaches by similarity fusion,” in\\nSIGIR . ACM, 2006, pp. 501–508.\\n[6] H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady,\\nL. Nie, T. Phillips, E. Davydov, D. Golovin, S. Chikkerur, D. Liu,'),\n",
       " Document(page_content='L. Nie, T. Phillips, E. Davydov, D. Golovin, S. Chikkerur, D. Liu,\\nM. Wattenberg, A. M. Hrafnkelsson, T. Boulos, and J. Kubica, “Ad\\nclick prediction: a view from the trenches,” in KDD 2013, Chicago, IL,\\nUSA, August 11-14, 2013 , 2013, pp. 1222–1230.\\n[7] S. Rendle, “Factorization machines,” in ICDM, Sydney, Australia, 14-17\\nDecember 2010 , 2010, pp. 995–1000.\\n[8] Y . Juan, Y . Zhuang, W. Chin, and C. Lin, “Field-aware factorization\\nmachines for CTR prediction,” in RecSys, Boston, MA, USA, September\\n15-19, 2016 , 2016, pp. 43–50.\\n[9] W. Zhang, T. Du, and J. Wang, “Deep learning over multi-ﬁeld categor-\\nical data - - A case study on user response prediction,” in ECIR 2016,\\nPadua, Italy, March 20-23, 2016. Proceedings , 2016, pp. 45–57.[10] Y . Qu, H. Cai, K. Ren, W. Zhang, Y . Yu, Y . Wen, and J. Wang, “Product-\\nbased neural networks for user response prediction,” in ICDM 2016,\\nDecember 12-15, 2016, Barcelona, Spain , 2016, pp. 1149–1154.\\n[11] H. Guo, R. Tang, Y . Ye, Z. Li, and X. He, “Deepfm: A factorization-\\nmachine based neural network for CTR prediction,” in IJCAI 2017,\\nMelbourne, Australia, August 19-25, 2017 , 2017, pp. 1725–1731.\\n[12] H. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye,\\nG. Anderson, G. Corrado, W. Chai, M. Ispir, R. Anil, Z. Haque, L. Hong,\\nV . Jain, X. Liu, and H. Shah, “Wide & deep learning for recommender\\nsystems,” CoRR , vol. abs/1606.07792, 2016.\\n[13] O. Chapelle and L. Li, “An empirical evaluation of thompson sampling,”\\ninNIPS, Granada, Spain. , 2011, pp. 2249–2257.\\n[14] L. Li, W. Chu, J. Langford, and R. E. Schapire, “A contextual-bandit\\napproach to personalized news article recommendation,” in WWW 2010,\\nRaleigh, North Carolina, USA, April 26-30, 2010 , 2010, pp. 661–670.\\n[15] H. Wang, Q. Wu, and H. Wang, “Factorization bandits for interactive\\nrecommendation,” in AAAI, February 4-9, 2017, San Francisco, Cali-\\nfornia, USA. , 2017, pp. 2695–2702.\\n[16] C. Zeng, Q. Wang, S. Mokhtari, and T. Li, “Online context-aware\\nrecommendation with time varying multi-armed bandit,” in SIGKDD\\n, San Francisco, CA, USA, August 13-17, 2016 , 2016, pp. 2025–2034.\\n[17] X. Zhao, W. Zhang, and J. Wang, “Interactive collaborative ﬁltering,” in\\nCIKM’13, San Francisco, CA, USA, October 27 - November 1, 2013 ,\\n2013, pp. 1411–1420.\\n[18] G. Shani, D. Heckerman, and R. I. Brafman, “An mdp-based recom-\\nmender system,” Journal of Machine Learning Research , vol. 6, pp.\\n1265–1295, 2005.\\n[19] G. Zheng, F. Zhang, Z. Zheng, Y . Xiang, N. J. Yuan, X. Xie, and\\nZ. Li, “DRN: A deep reinforcement learning framework for news\\nrecommendation,” in WWW 2018, Lyon, France, April 23-27, 2018 ,\\n2018, pp. 167–176.\\n[20] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction .\\nMIT press Cambridge, 1998, vol. 1, no. 1.\\n[21] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\\nBellemare, A. Graves, M. A. Riedmiller, A. Fidjeland, G. Ostrovski,\\nS. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,'),\n",
       " Document(page_content='S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\\nD. Wierstra, S. Legg, and D. Hassabis, “Human-level control through\\ndeep reinforcement learning,” Nature , vol. 518, no. 7540, pp. 529–533,\\n2015.\\n[22] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den\\nDriessche, J. Schrittwieser, I. Antonoglou, V . Panneershelvam, M. Lanc-\\ntot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever,\\nT. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis,\\n“Mastering the game of go with deep neural networks and tree search,”\\nNature , vol. 529, no. 7587, pp. 484–489, 2016.\\n[23] H. Cai, K. Ren, W. Zhang, K. Malialis, J. Wang, Y . Yu, and D. Guo,\\n“Real-time bidding by reinforcement learning in display advertising,” in\\nWSDM 2017, Cambridge, United Kingdom, February 6-10, 2017 , 2017,\\npp. 661–670.\\n[24] J. Jin, C. Song, H. Li, K. Gai, J. Wang, and W. Zhang, “Real-time\\nbidding with multi-agent reinforcement learning in display advertising,”\\nCoRR , vol. abs/1802.09756, 2018.\\n[25] H. Cai, T. Chen, W. Zhang, Y . Yu, and J. Wang, “Efﬁcient architecture\\nsearch by network transformation,” in AAAI , New Orleans, Louisiana,\\nUSA, February 2-7, 2018 , 2018.\\n[26] B. Zoph and Q. V . Le, “Neural architecture search with reinforcement\\nlearning,” CoRR , vol. abs/1611.01578, 2016.\\n[27] N. Taghipour and A. A. Kardan, “A hybrid web recommender system\\nbased on q-learning,” in Proceedings of the 2008 ACM Symposium on\\nApplied Computing (SAC), Fortaleza, Ceara, Brazil, March 16-20, 2008 ,\\n2008, pp. 1164–1168.\\n[28] X. Zhao, L. Zhang, Z. Ding, D. Yin, Y . Zhao, and J. Tang, “Deep\\nreinforcement learning for list-wise recommendations,” CoRR , vol.\\nabs/1801.00209, 2018.\\n[29] X. Zhao, L. Zhang, Z. Ding, L. Xia, J. Tang, and D. Yin, “Recommenda-\\ntions with negative feedback via pairwise deep reinforcement learning,”\\nCoRR , vol. abs/1802.06501, 2018.\\n[30] L. Xia, J. Xu, Y . Lan, J. Guo, W. Zeng, and X. Cheng, “Adapting markov\\ndecision process for search result diversiﬁcation,” in SIGIR , Shinjuku,\\nTokyo, Japan, August 7-11, 2017 , 2017, pp. 535–544.\\n[31] Z. Wei, J. Xu, Y . Lan, J. Guo, and X. Cheng, “Reinforcement learning to\\nrank with markov decision process,” in SIGIR , Shinjuku, Tokyo, Japan,\\nAugust 7-11, 2017 , 2017, pp. 945–948.[32] Y . Hu, Q. Da, A. Zeng, Y . Yu, and Y . Xu, “Reinforcement learning\\nto rank in e-commerce search engine: Formalization, analysis, and\\napplication,” CoRR , vol. abs/1803.00710, 2018.\\n[33] G. Dulac-Arnold, R. Evans, P. Sunehag, and B. Coppin, “Reinforcement\\nlearning in large discrete action spaces,” CoRR , vol. abs/1512.07679,\\n2015.\\n[34] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. A.\\nRiedmiller, “Deterministic policy gradient algorithms,” in ICML 2014,\\nBeijing, China, 21-26 June 2014 , 2014, pp. 387–395.\\n[35] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa,\\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement'),\n",
       " Document(page_content='D. Silver, and D. Wierstra, “Continuous control with deep reinforcement\\nlearning,” CoRR , vol. abs/1509.02971, 2015.\\n[36] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience\\nreplay,” arXiv preprint arXiv:1511.05952 , 2015.\\n[37] A. Mnih and R. R. Salakhutdinov, “Probabilistic matrix factorization,”\\ninNIPS , 2008, pp. 1257–1264.\\n[38] Y . Koren, “Factorization meets the neighborhood: a multifaceted collab-\\norative ﬁltering model,” in KDD . ACM, 2008, pp. 426–434.\\n[39] L. Li, W. Chu, J. Langford, and R. E. Schapire, “A contextual-bandit\\napproach to personalized news article recommendation,” in WWW .\\nACM, 2010, pp. 661–670.\\n[40] H. Wang, Q. Wu, and H. Wang, “Learning hidden features for contextual\\nbandits,” in CIKM . ACM, 2016, pp. 1633–1642.')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que agora temos os nossos `chunks`, em uma quantidade maior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado os nossos chunks, vamos incorporá-los em vetores e alimentar o FAISS (um Vector Database) com essas incorporações. Nesse sentido, estarei usando o modelo de embedding da OpenAI, o `text-embedding-3-large`. Podemos acessá-lo via LangChain da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, \n",
    "                              model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, nós alimentamos o FAISS a partir dos embeddings dos documentos dos chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "vec_database = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x16582624fd0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dar uma olhada num dos textos que ele retorna quando consultamos: `As for the reward function, what range were the ratings normalized to?`. Por padrão, utilizando o método abaixo, ele retorna os **4** textos/documentos mais similares. Irei selecionar o que ele tomou como mais similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RL based models on recommender systems. Therefore, we\\nconduct online evaluation with an environment simulator. In\\nthis paper, we pretrain a PMF [37] model as the environmentsimulator, i.e., to predict an item’s feedback that the user\\nnever rates before. The online evaluation procedure follows\\nAlgorithm 1, i.e., the parameters continuously update dur-\\ning the online evaluation stage. Its major difference from\\nAlgorithm 1 is that the feedback of a recommended item\\nis observed by the environment simulator. Moreover, before\\neach recommendation session starting in the simulated online\\nevaluation, we reset the parameters back to θandωwhich is\\nthe policy learned in the training stage for a fair comparison.\\nV. E XPERIMENT\\nA. Datasets and Evaluation Metrics\\nWe adopt the following publicly available datasets from the\\nreal world to conduct the experiments:\\n•MovieLens (100k)4. A benchmark dataset comprises\\nof 0.1 million ratings from users to the recommended\\nmovies on MovieLens website.\\n•Yahoo! Music (R3)5. This dataset contains over 0.36\\nmillion ratings of songs collected from two different\\nsources. The ﬁrst source consists of ratings provided\\nby users during normal interactions with Yahoo! Music\\nservices. The second source consists of ratings of ran-\\ndomly selected songs collected during an online survey\\nby Yahoo! Research. We normalize the ratings to discrete\\nvalues from 1 to 5.\\n•MovieLens (1M)6. A benchmark dataset includes of 1\\nmillion ratings from the MovieLens website.\\n•Jester (2)7. This dataset contains over 1.7 million real-\\nvalue ratings (-10.0 to +10.0) over jokes in an online joke\\nrecommender system.\\nNote that except for Jester, the ratings in the other datasets\\nare discrete values from 1 to 5, and the statistic information\\nof the datasets is given in Table I. The MovieLens (100k) and\\nMovieLens (1M) are abbreviated as ML (100k) and ML (1M)\\nrespectively.\\nTABLE I\\nSTATISTIC INFORMATION OF THE DATASETS\\nML (100k) Yahoo! Music ML (1M) Jester\\n# user 943 15,400 6,040 63,978\\n# item 1,682 1,000 3,952 150\\n# ratings 100,000 365,740 1,000,209 1,761,439\\nWe conduct both ofﬂine and simulated online evaluation\\non these four datasets. For the ofﬂine evaluation, we utilize\\nPrecision@k and NDCG@k as the metrics to measure the\\nperformance of the proposed models. For the simulated online\\nevaluation, we leverage the total accumulated rewards as the\\nmetric.\\n4https://grouplens.org/datasets/movielens/100k/\\n5https://webscope.sandbox.yahoo.com/\\n6https://grouplens.org/datasets/movielens/1m/\\n7http://eigentaste.berkeley.edu/dataset/B. Compared Methods\\nWe compare the proposed methods with some representative\\nbaseline methods. For the ofﬂine evaluation, we compare to\\nconventional methods including Popularity, PMF [37] and\\nSVD++ [38], and a RL based method DRR-n. Moreover, the\\nonline evaluation baselines contain the state-of-the-art multi-\\narmed bandits methods LinUCB [39] and HLinUCB [40] and\\nthe DRR-n as well.\\n•Popularity recommends the most popular item, i.e., the'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = vec_database.similarity_search(\"As for the reward function, what range were the ratings normalized to?\")\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de implementarmos de fato a arquitetura do `Naive RAG`, vamos ver como o nosso chat se sai sem contexto externo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a lista de messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi Chat, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"Do you know about Deep Reinforcement Learning Based Recommendation With Explicit User-Item Interactions Modeling paper?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I am familiar with the paper on Deep Reinforcement Learning Based Recommendation with Explicit User-Item Interactions Modeling. It proposes a method that combines deep reinforcement learning with explicit modeling of user-item interactions to improve recommendation systems. The approach aims to better capture user preferences and provide more accurate recommendations. Is there anything specific you would like to know or discuss about this paper?\n"
     ]
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRR-ave stands for Deep Reinforcement Learning Based Recommendation with Explicit User-Item Interactions Modeling (DRR-ave). It is a method proposed in a research paper that combines deep reinforcement learning with explicit modeling of user-item interactions to enhance recommendation systems. By incorporating user feedback and interactions with items, DRR-ave aims to provide more personalized and accurate recommendations to users. This approach leverages reinforcement learning techniques to learn optimal recommendation policies based on user preferences and behaviors.\n"
     ]
    }
   ],
   "source": [
    "# Adicionando a resposta da IA à messages\n",
    "messages.append(response)\n",
    "\n",
    "# Criando um novo user prompt\n",
    "prompt = HumanMessage(content=\"Tell me about DRR-ave. What is that?\")\n",
    "\n",
    "# Adicionando o user prompt à messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# Requisitando uma resposta do ChatGPT\n",
    "response = chat(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como eu já estou um pouco mais famialiarizado com este artigo, posso afirmar que essa não é uma resposta certa. Na verdade, aqui está um exemplo de **alucinação**. Vamos fazer mais uma pergunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the paper on Deep Reinforcement Learning Based Recommendation with Explicit User-Item Interactions Modeling, the state representation module was defined as a component responsible for encoding the user's historical interactions and preferences into a meaningful representation that can be used by the recommendation system. The state representation module typically takes as input the user's interaction history with items, such as clicks, purchases, ratings, etc., and processes this information to create a condensed and informative representation of the user's current state.\n",
      "\n",
      "The specific design and implementation of the state representation module may vary depending on the recommendation system and the underlying data. Common techniques used in the state representation module include neural networks, embeddings, and other machine learning models that can capture complex relationships between users and items. The goal of the state representation module is to transform raw user-item interaction data into a structured and informative representation that can be used by the recommendation algorithm to generate personalized recommendations.\n"
     ]
    }
   ],
   "source": [
    "# Adicionando a resposta da IA à messages\n",
    "messages.append(response)\n",
    "\n",
    "# Criando um novo user prompt\n",
    "prompt = HumanMessage(content=\"How was the state representation module defined in the article?\")\n",
    "\n",
    "# Adicionando o user prompt à messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# Requisitando uma resposta do ChatGPT\n",
    "response = chat(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está um exemplo de mais uma resposta sem muito sentido. No entanto, esse não é o fim. \n",
    "\n",
    "Tendo a nossa base de conhecimento pronta, podemos agora de fato implementar a arquitetura do `Naive RAG` e ver o poder da geração aumentada por recuperação. Quando fizermos uma consulta, tomaremos os **4** textos mais similares em relação a nossa consulta (esse valor pode ser redefinido setando algum outro valor no parâmetro k, e vale lembrar que 4 é o valor padrão) e os passamos como contexto ao LLM para ele ser capaz de responder com fidelidade (além de passar a própria consulta).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vec_database.similarity_search(query)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Answer the query using the contexts below.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I am familiar with the \"Deep Reinforcement Learning Based Recommendation With Explicit User-Item Interactions Modeling\" paper. It proposes a novel recommendation framework called DRR, which is based on deep reinforcement learning. The framework treats recommendation as a sequential decision-making procedure and uses an \"Actor-Critic\" reinforcement learning scheme to model interactions between users and recommender systems. It aims to address the limitations of previous studies by considering dynamic adaptation and long-term rewards in the recommendation process. The paper conducts experiments on real-world datasets and shows that the DRR method outperforms existing techniques.\n"
     ]
    }
   ],
   "source": [
    "# Setando novamente a lista de messages e gerando uma nova resposta\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi Chat, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=augment_prompt(\"Do you know about Deep Reinforcement Learning Based Recommendation With Explicit User-Item Interactions Modeling paper?\"))\n",
    "]\n",
    "\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, essa é uma resposta bem mais concisa! Vamos continuar dando uma olhada com as mesmas perguntas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRR-ave is a structure proposed in the DRR framework for modeling the state representation module. In DRR-ave, the interactions between users and items are explicitly modeled. The structure involves transforming the embeddings of items using a weighted average pooling layer, which is denoted as g(ia). These transformed vectors are then used to model the interactions with the input user. The state representation in DRR-ave is a concatenated vector that includes the user embedding, the interaction vector, and the average pooling result of items.\n",
      "\n",
      "The key idea behind DRR-ave is to capture the dynamic interactions between users and items while eliminating the position effects in the long-term sequence of items. By incorporating the user-item interactions and leveraging the average pooling technique, DRR-ave aims to provide a more effective and personalized recommendation approach compared to other structures like DRR-p and DRR-u. Experimental results have shown that DRR-ave outperforms other methods on various datasets in terms of precision and ranking metrics.\n"
     ]
    }
   ],
   "source": [
    "# Adicionando a resposta da IA à messages\n",
    "messages.append(response)\n",
    "\n",
    "# Criando um novo user prompt\n",
    "prompt = HumanMessage(content=augment_prompt(\"Tell me about DRR-ave. What is that?\"))\n",
    "\n",
    "# Adicionando o user prompt à messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# Requisitando uma resposta do ChatGPT\n",
    "response = chat(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ótimo! Você pode comparar a diferença entre as respostas e concluir que não houve mais alucinação do modelo. Vamos mais uma vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state representation module in the article was defined by explicitly modeling the interactions between users and items. Three different structures were proposed within the state representation module in the DRR framework:\n",
      "\n",
      "1. DRR-p: This structure utilizes a product-based neural network to capture the pairwise local dependency between items. It clones the representations of the n items and computes the pairwise interactions between them using an element-wise product operator. Each item's importance is indicated by a scalar weight, and the interactions between items are represented by k-dimensional vectors. The state representation in DRR-p is a concatenation of the cloned vectors and the pairwise interaction vectors.\n",
      "\n",
      "2. DRR-u: While DRR-p focuses on modeling the pairwise local dependency between items, DRR-u incorporates user-item interactions as well. This structure includes the user embedding and considers the pairwise interactions between users and items in addition to the local dependencies between items.\n",
      "\n",
      "3. DRR-ave: In DRR-ave, the interactions between users and items are exploited and modeled. This structure eliminates the position effects of items in the sequence and uses a weighted average pooling layer to transform the embeddings of items. The resulting vector is used to model the interactions with the input user. The state representation in DRR-ave is a concatenation of the user embedding, the interaction vector, and the average pooling result of items.\n",
      "\n",
      "These structures in the state representation module aim to capture the dynamic interactions between users and items, providing a more effective and personalized recommendation approach within the DRR framework.\n"
     ]
    }
   ],
   "source": [
    "# Adicionando a resposta da IA à messages\n",
    "messages.append(response)\n",
    "\n",
    "# Criando um novo user prompt\n",
    "prompt = HumanMessage(content=augment_prompt(\"How was the state representation module defined in the article?\"))\n",
    "\n",
    "# Adicionando o user prompt à messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# Requisitando uma resposta do ChatGPT\n",
    "response = chat(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incrível! Obtemos respostas corretas e mais concisas e sem a necessidade de um novo ajuste-fino. Agora, terei uma nova ferramenta auxiliar aos meus estudos acerca do artigo e sobre a implementação do mesmo. Maravilhoso, não? \n",
    "\n",
    "Agora, vamos dar uma olhada na implementação com o **Llama Index**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.2.2. Com Llama Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
